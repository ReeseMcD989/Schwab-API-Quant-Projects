{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"color:red;\">Initial Setup</h>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "import schwabdev\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import numpy as np\n",
    "import mplfinance as mpf\n",
    "from IPython.display import display, clear_output\n",
    "import warnings\n",
    "from Multi_Strat_Back_Tester_Functions import *\n",
    "from datetime import datetime, time\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load environment variables from .env file for authentification purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patches the notebook's running event loop so that it can handle multiple calls to `asyncio.run()` or other asynchronous methods without causing conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the display options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display options\n",
    "# Set the global float format to 4 decimal places\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.4f}')\n",
    "pd.options.display.float_format = '{:,.4f}'.format # Format numerical output to have certain number of decimals\n",
    "# pd.options.display.float_format = None # Reset to default numerical output formatting\n",
    "pd.set_option('display.width', 2000) # Set the display width to a large number\n",
    "pd.set_option('display.max_colwidth', 1000) # Set max column width to a large number\n",
    "\n",
    "pd.set_option('display.max_columns', None) # Displays all columns\n",
    "# pd.set_option('display.max_rows', None) # Displays all rows                             \n",
    "# pd.reset_option('display.max_columns') # Display default abbreviated columns\n",
    "pd.reset_option('display.max_rows') # Display default abbreviated rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the client object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "App Key: 2C7Jh6zt5QdlSb0N5RnhWpaiEzKFr150\n",
      "App Secret: OQGxH0GkD5bAEMA4\n",
      "Callback URL: https://127.0.0.1\n",
      "Client initialized!\n"
     ]
    }
   ],
   "source": [
    "app_key = os.getenv('app_key')\n",
    "app_secret = os.getenv('app_secret')\n",
    "callback_url = os.getenv('callback_url')\n",
    "\n",
    "# Print them to verify (avoid printing sensitive info in production)\n",
    "print(f\"App Key: {app_key}\")\n",
    "print(f\"App Secret: {app_secret}\")\n",
    "print(f\"Callback URL: {callback_url}\")\n",
    "\n",
    "# Now proceed to initialize the client\n",
    "client = schwabdev.Client(app_key, app_secret, callback_url)\n",
    "print('Client initialized!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this when in need of updating the refresh token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.update_tokens(force=True)\n",
    "# client = schwabdev.Client(app_key, app_secret, callback_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"color:red;\">Permanent Data Extraction and Storage Functions</h>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ticker list and point value definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers = \"/ES,/NQ,/CL,/GC\"\n",
    "# tickers = \"MARA,PLUG,SOFI,SWN,RKLB\"\n",
    "\n",
    "ticker_to_point_value = {\n",
    "    \"/ES\": 50,       # E-Mini S&P 500\n",
    "    \"/NQ\": 20,       # E-Mini NASDAQ 100\n",
    "    \"/CL\": 1000,     # Crude Oil\n",
    "    \"/GC\": 100,      # Gold\n",
    "}\n",
    "\n",
    "ticker_to_tick_size = {\n",
    "    \"/ES\": 0.25,        # E-Mini S&P 500\n",
    "    \"/NQ\": 0.25,        # E-Mini NASDAQ 100\n",
    "    \"/CL\": 0.01,        # Crude Oil\n",
    "    \"/GC\": 0.1,         # Gold\n",
    "}\n",
    "\n",
    "tick_size = ticker_to_tick_size.get(\"/ES\")\n",
    "tick_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tickers = \"/ES,/NQ,/CL,/GC,/MES,/MNQ,/MCL,/MGC\"\n",
    "# # tickers = \"MARA,PLUG,SOFI,SWN,RKLB\"\n",
    "\n",
    "# ticker_to_point_value = {\n",
    "#     \"/ES\": 50,       # E-Mini S&P 500\n",
    "#     \"/NQ\": 20,       # E-Mini NASDAQ 100\n",
    "#     \"/CL\": 1000,     # Crude Oil\n",
    "#     \"/GC\": 100,      # Gold\n",
    "#     \"/MES\": 5,       # Micro E-Mini S&P 500\n",
    "#     \"/MNQ\": 2,       # Micro E-Mini NASDAQ 100\n",
    "#     \"/MCL\": 100,     # Micro Crude Oil\n",
    "#     \"/MGC\": 10       # Micro Gold\n",
    "# }\n",
    "\n",
    "# ticker_to_tick_size = {\n",
    "#     \"/ES\": 0.25,        # E-Mini S&P 500\n",
    "#     \"/NQ\": 0.25,        # E-Mini NASDAQ 100\n",
    "#     \"/CL\": 0.01,        # Crude Oil\n",
    "#     \"/GC\": 0.1,         # Gold\n",
    "#     \"/MES\": 0.25,       # Micro E-Mini S&P 500\n",
    "#     \"/MNQ\": 0.25,       # Micro E-Mini NASDAQ 100\n",
    "#     \"/MCL\": 0.01,       # Micro Crude Oil\n",
    "#     \"/MGC\": 0.1         # Micro Gold\n",
    "# }\n",
    "\n",
    "# tick_size = ticker_to_tick_size.get(\"/ES\")\n",
    "# tick_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting and storing raw streaming data with two functions into ticker_tables dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_row_data(entry, content):\n",
    "    \n",
    "    # Create a row dictionary with the relevant fields\n",
    "    row = { # For futures\n",
    "        'timestamp': entry['timestamp'],\n",
    "        'key': content.get('key', None),\n",
    "        'bid_price': content.get('1', None),\n",
    "        'ask_price': content.get('2', None),\n",
    "        # 'last_price': last_price,  # Use updated last_price\n",
    "        'last_price': content.get('3', None),\n",
    "        'bid_size': content.get('4', None), # These two variables can be correlated \n",
    "        'ask_size': content.get('5', None), # with direction of price action\n",
    "        'total_volume': content.get('8', None),\n",
    "        'last_size': content.get('9', 0),\n",
    "        'high_price': content.get('12', None),\n",
    "        'low_price': content.get('13', None),\n",
    "        'close_price': content.get('14', None),\n",
    "        'open_price': content.get('18', None),\n",
    "        'net_change': content.get('19', None),\n",
    "        'future_pct_change': content.get('20', None),\n",
    "        'open_interest': content.get('23', None),\n",
    "        'tick': content.get('25', None),\n",
    "        'tick_amount': content.get('26', None),\n",
    "        'future_exp_date': content.get('35', None),\n",
    "        'ask_time': content.get('37', None),\n",
    "        'bid_time': content.get('38', None)\n",
    "        }\n",
    "\n",
    "    # row = { # For equities\n",
    "    #     'timestamp': entry['timestamp'],               # Same\n",
    "    #     'key': content.get('key', None),               # Same\n",
    "    #     'bid_price': content.get('1', None),           # Matches bid_price\n",
    "    #     'ask_price': content.get('2', None),           # Matches ask_price\n",
    "    #     'last_price': last_price,                      # Matches last_price (updated)\n",
    "    #     'bid_size': content.get('4', None),            # Matches bid_size\n",
    "    #     'ask_size': content.get('5', None),            # Matches ask_size\n",
    "    #     'total_volume': content.get('8', None),        # Matches total_volume\n",
    "    #     'last_size': content.get('9', 0),              # Matches last_size\n",
    "    #     'high_price': content.get('10', None),         # Matches high_price\n",
    "    #     'low_price': content.get('11', None),          # Matches low_price\n",
    "    #     'close_price': content.get('12', None),        # Matches close_price\n",
    "    #     'open_price': content.get('17', None),         # Matches open_price\n",
    "    #     'net_change': content.get('18', None),         # Matches net_change\n",
    "    #     '52_week_high': content.get('19', None),       # Matches 52_week_high\n",
    "    #     '52_week_low': content.get('20', None),        # Matches 52_week_low\n",
    "    #     'pe_ratio': content.get('21', None),           # Matches pe_ratio\n",
    "    #     'net_pct_change': content.get('42', None)}      # Matches net_pct_change  \n",
    "    \n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_tables = 'nothing'\n",
    "ticker_tables = {ticker: pd.DataFrame() for ticker in tickers.split(\",\")}\n",
    "\n",
    "# Initialize a dictionary to store the last known price for each ticker\n",
    "last_known_price = {ticker: None for ticker in tickers.split(\",\")}\n",
    "\n",
    "def process_message_data(data):\n",
    "    global ticker_tables, last_known_price\n",
    "    \n",
    "    # Loop through the content and extract the relevant fields\n",
    "    if \"data\" in data:\n",
    "        for entry in data['data']:\n",
    "            if 'content' in entry:\n",
    "                for content in entry['content']:\n",
    "                    # Extract row data\n",
    "                    row = extract_row_data(entry, content)\n",
    "\n",
    "                    # Get the ticker (key) and last_price\n",
    "                    ticker = row['key']\n",
    "                    last_price = row.get('last_price')\n",
    "\n",
    "                    # Check if last_price is None, if so use the last known price for the ticker\n",
    "                    if last_price is None and ticker in last_known_price:\n",
    "                        row['last_price'] = last_known_price[ticker]\n",
    "                    else:\n",
    "                        # Update the last known price\n",
    "                        last_known_price[ticker] = last_price\n",
    "\n",
    "                    # Convert the row dictionary to a DataFrame\n",
    "                    row_df = pd.DataFrame([row])\n",
    "\n",
    "                    # Check the ticker and append the row to the correct ticker table\n",
    "                    if ticker in ticker_tables:\n",
    "                        ticker_tables[ticker] = pd.concat([ticker_tables[ticker], row_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compressing minute candles from the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "minute_candles = 'nothing'\n",
    "minute_candles = {ticker: pd.DataFrame(columns=['datetime', 'ticker', 'open', 'high', 'low', 'close', 'accumulative_volume']) for ticker in tickers.split(\",\")}\n",
    "\n",
    "def update_minute_candles(ticker_tables, time_frame='min'):\n",
    "    global minute_candles\n",
    "\n",
    "    for ticker, df in ticker_tables.items():\n",
    "        if df.empty:\n",
    "            continue\n",
    "        \n",
    "        # Ensure that 'datetime' column exists and is properly set\n",
    "        if 'timestamp' in df.columns:\n",
    "            df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True).dt.tz_convert('US/Eastern')\n",
    "\n",
    "        # Ensure that the 'datetime' column is set as the DataFrame index (only if not already set)\n",
    "        if df.index.name != 'datetime':\n",
    "            df.set_index('datetime', inplace=True)\n",
    "\n",
    "        # Group the data by minute intervals using 'min'\n",
    "        current_minute = df.index.floor(time_frame)[-1]  # Get the most recent minute\n",
    "\n",
    "        # Select only the rows from the most recent minute\n",
    "        recent_data = df[df.index.floor(time_frame) == current_minute]\n",
    "\n",
    "        if not recent_data.empty:\n",
    "            # Extract OHLC values for the minute candle\n",
    "            open_price = recent_data['last_price'].iloc[0]  # Open is the first last_price\n",
    "            high_price = recent_data['last_price'].max()     # High is the max last_price\n",
    "            low_price = recent_data['last_price'].min()      # Low is the min last_price\n",
    "            close_price = recent_data['last_price'].iloc[-1] # Close is the last last_price\n",
    "            total_volume = recent_data['total_volume'].max() # Total volume is the max value in this interval\n",
    "\n",
    "            # Create a dictionary with the new candle data\n",
    "            new_candle = {\n",
    "                'datetime': current_minute,\n",
    "                'ticker': ticker,\n",
    "                'open': open_price,\n",
    "                'high': high_price,\n",
    "                'low': low_price,\n",
    "                'close': close_price,\n",
    "                'accumulative_volume': total_volume\n",
    "            }\n",
    "\n",
    "            # Convert the dictionary to a DataFrame and update the respective ticker's DataFrame in minute_candles\n",
    "            new_candle_df = pd.DataFrame([new_candle])\n",
    "\n",
    "            # Replace or append the most recent candle for this ticker\n",
    "            if not minute_candles[ticker].empty and minute_candles[ticker]['datetime'].iloc[-1] == current_minute:\n",
    "                minute_candles[ticker].iloc[-1] = new_candle  # Update the last candle\n",
    "            else:\n",
    "                minute_candles[ticker] = pd.concat([minute_candles[ticker], new_candle_df], ignore_index=True)  # Append new candle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to calculate a price action envelope indicator for trend determination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_price_action_envelope(df, high_col='high', low_col='low', window=5):\n",
    "    \"\"\"\n",
    "    Calculate the continuous higher highs, higher lows, lower highs, and lower lows\n",
    "    to create an envelope that wraps around the price action.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the price data.\n",
    "    high_col (str): The column name for the high prices (default 'high').\n",
    "    low_col (str): The column name for the low prices (default 'low').\n",
    "    window (int): The window size for detecting local highs and lows (default 5).\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with added columns for the price action envelope.\n",
    "    \"\"\"\n",
    "    # Calculate rolling max (higher highs) and rolling min (lower lows)\n",
    "    df['higher_high'] = df[high_col].rolling(window=window, min_periods=1).max()\n",
    "    df['lower_low'] = df[low_col].rolling(window=window, min_periods=1).min()\n",
    "    \n",
    "    # Calculate rolling min of high for higher lows, and rolling max of low for lower highs\n",
    "    df['higher_low'] = df[low_col].rolling(window=window, min_periods=1).min().cummax()\n",
    "    df['lower_high'] = df[high_col].rolling(window=window, min_periods=1).max().cummin()\n",
    "    \n",
    "    # Create the envelope by combining these calculated values\n",
    "    df['price_action_upper'] = df[['higher_high', 'lower_high']].max(axis=1)\n",
    "    df['price_action_lower'] = df[['lower_low', 'higher_low']].min(axis=1)\n",
    "    \n",
    "    # Drop intermediate columns if not needed\n",
    "    df.drop(columns=['higher_high', 'lower_low', 'higher_low', 'lower_high'], inplace=True)\n",
    "\n",
    "    # Add moving averages for price_action_upper and price_action_lower\n",
    "    ma_period = window  # Set the moving average period length to 5\n",
    "    df['ma_price_action_upper'] = df['price_action_upper'].rolling(window=ma_period, min_periods=1).mean()\n",
    "    df['ma_price_action_lower'] = df['price_action_lower'].rolling(window=ma_period, min_periods=1).mean()\n",
    "\n",
    "    # Define a function to calculate trend score for an entire column over a 5-period window\n",
    "    def calculate_trend_score(col_series):\n",
    "        score_series = []\n",
    "        for i in range(len(col_series)):\n",
    "            score = 0\n",
    "            for j in range(1, window):\n",
    "                if i - j >= 0:\n",
    "                    if col_series.iloc[i] > col_series.iloc[i - j]:\n",
    "                        score += 10\n",
    "                    elif col_series.iloc[i] < col_series.iloc[i - j]:\n",
    "                        score -= 10\n",
    "            score_series.append(score)\n",
    "        return score_series\n",
    "\n",
    "    # Calculate the trend indicator score for each relevant column and combine them\n",
    "    scores_upper = calculate_trend_score(df['price_action_upper'])\n",
    "    scores_lower = calculate_trend_score(df['price_action_lower'])\n",
    "\n",
    "    # Sum the scores for the final trend indicator based only on price_action_upper and price_action_lower\n",
    "    total_score = [u + l for u, l in zip(scores_upper, scores_lower)]\n",
    "\n",
    "    # Rescale total_score to a 0-100 range and assign to 'trend_indicator'\n",
    "    df['trend_indicator'] = [(score + 100) / 2 for score in total_score]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that calculates a number of indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_indicators(df, price_col='close', acc_vol_col='accumulative_volume', sma_periods=None, wma_periods=None, rsi_periods=None, volume_col='volume', candle_window=10):\n",
    "    \"\"\"\n",
    "    Calculate SMAs and WMAs for given periods and append them to the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the price data.\n",
    "    price_col (str): The column name for the price data (default 'close').\n",
    "    acc_vol_col (str): The column name for the accumulative volume data (default 'accumulative_volume').\n",
    "    sma_periods (list): A list of periods for SMAs (e.g., [3, 5, 10, 20]).\n",
    "    wma_periods (list): A list of periods for WMAs (e.g., [3, 5, 10, 20]).\n",
    "    volume_col (str): The column name to store the volume difference (default 'volume').\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with the calculated SMAs and WMAs appended.\n",
    "    \"\"\"\n",
    "    # Calculate volume differences\n",
    "    df[volume_col] = df[acc_vol_col].diff()\n",
    "    \n",
    "    # Calculate SMAs\n",
    "    if sma_periods:\n",
    "        for period in sma_periods:\n",
    "            sma_col_name = f'sma_{period}'\n",
    "            df[sma_col_name] = df[price_col].rolling(window=period).mean()\n",
    "    \n",
    "    # Calculate WMAs\n",
    "    if wma_periods:\n",
    "        for period in wma_periods:\n",
    "            weights = np.arange(1, period + 1)  # Generate weights from 1 to the period length\n",
    "            wma_col_name = f'wma_{period}'\n",
    "            df[wma_col_name] = df[price_col].rolling(window=period).apply(lambda prices: np.dot(prices, weights) / weights.sum(), raw=True)\n",
    "\n",
    "    # calculate_vwap_and_bands(df, high_col='high', low_col='low', close_col='close', volume_col='volume')\n",
    "\n",
    "    # Assume 'periods' is a predefined list of different periods for which you want to calculate RSI\n",
    "    for period in rsi_periods:\n",
    "        # Calculate RSI for each period\n",
    "        delta = df[price_col].diff()\n",
    "        gain = delta.where(delta > 0, 0).rolling(window=period).mean()\n",
    "        loss = -delta.where(delta < 0, 0).rolling(window=period).mean()\n",
    "        rs = gain / loss\n",
    "        df[f'rsi_{period}'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # Calculate price action envelope\n",
    "    df = calculate_price_action_envelope(df, high_col='high', low_col='low', window=5)\n",
    "\n",
    "    df['ohlc_average'] = df[['open', 'high', 'low', 'close']].mean(axis=1)\n",
    "    df['candle_span'] = df['high'] - df['low']\n",
    "    df['candle_body'] = (df['close'] - df['open']).abs()\n",
    "    df['candle_span_avg'] = df['candle_span'].rolling(window=candle_window, min_periods=1).mean()\n",
    "    df['candle_span_max'] = df['candle_span'].rolling(window=candle_window, min_periods=1).max()\n",
    "    df['candle_span_maxavg_mean'] =(df['candle_span_avg'] + df['candle_span_max'])/2\n",
    "    df['hundred_line'] = 100\n",
    "    df['fifty_line'] = 50\n",
    "    df['zero_line'] = 0\n",
    "    df['trend_high_threshold'] = 75\n",
    "    df['trend_low_threshold'] = 25\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"color:red;\">Milestone Branch of Trade Logic and Visualization Functions</h>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trading_signals(candles, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5'):\n",
    "    \"\"\" Vectorized without iterrows\n",
    "    Generate buy/sell signals based on moving averages and RSI indicators and save position states.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): The DataFrame containing candle data.\n",
    "    - ma_name1 (str): Column name for the first moving average.\n",
    "    - ma_name2 (str): Column name for the second moving average.\n",
    "    - rsi_column (str): Column name for the RSI indicator.\n",
    "\n",
    "    Returns:\n",
    "    - candles (pd.DataFrame): The DataFrame with updated signal and position state columns.\n",
    "    \"\"\"\n",
    "    # Dynamically generate signal column names\n",
    "    ma_signal_column = f'signal_{ma_name1}_{ma_name2}'\n",
    "    rsi_signal_column = f'signal_{rsi_column}'\n",
    "    ma_position_open_col = f'position_open_{ma_name1}_{ma_name2}'\n",
    "    rsi_position_open_col = f'position_open_{rsi_column}'\n",
    "\n",
    "    # Initialize signal and position state columns\n",
    "    candles[ma_signal_column] = 0\n",
    "    candles[rsi_signal_column] = 0\n",
    "    candles[ma_position_open_col] = False\n",
    "    candles[rsi_position_open_col] = False\n",
    "\n",
    "    # Moving average signal and position state generation\n",
    "    ma_position_open = False  # Local variable for tracking state\n",
    "    ma_signals = []\n",
    "    ma_positions = []\n",
    "    for ma1, ma2 in zip(candles[ma_name1], candles[ma_name2]):\n",
    "        if not ma_position_open and ma1 <= ma2:\n",
    "            ma_position_open = True\n",
    "            ma_signals.append(1)\n",
    "        elif ma_position_open and ma1 > ma2:\n",
    "            ma_position_open = False\n",
    "            ma_signals.append(0)\n",
    "        else:\n",
    "            ma_signals.append(ma_signals[-1] if ma_signals else 0)\n",
    "        ma_positions.append(ma_position_open)\n",
    "    candles[ma_signal_column] = ma_signals\n",
    "    candles[ma_position_open_col] = ma_positions\n",
    "\n",
    "    # RSI signal and position state generation\n",
    "    rsi_position_open = False  # Local variable for tracking state\n",
    "    rsi_signals = []\n",
    "    rsi_positions = []\n",
    "    for rsi in candles[rsi_column]:\n",
    "        if not rsi_position_open and rsi < 50:\n",
    "            rsi_position_open = True\n",
    "            rsi_signals.append(1)\n",
    "        elif rsi_position_open and rsi >= 50:\n",
    "            rsi_position_open = False\n",
    "            rsi_signals.append(0)\n",
    "        else:\n",
    "            rsi_signals.append(rsi_signals[-1] if rsi_signals else 0)\n",
    "        rsi_positions.append(rsi_position_open)\n",
    "    candles[rsi_signal_column] = rsi_signals\n",
    "    candles[rsi_position_open_col] = rsi_positions\n",
    "\n",
    "    return candles\n",
    "\n",
    "def update_position_open(candles, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5'):\n",
    "    \"\"\"\n",
    "    Update the 'ma_position_open' and 'rsi_position_open' columns for a given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): The DataFrame containing the signals and position columns.\n",
    "    - ma_name1 (str): Column name for the first moving average.\n",
    "    - ma_name2 (str): Column name for the second moving average.\n",
    "    - rsi_column (str): Column name for the RSI signal.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated DataFrame with 'ma_position_open' and 'rsi_position_open' columns.\n",
    "    \"\"\"\n",
    "    # Dynamically generate signal column names\n",
    "    ma_signal_column = f'signal_{ma_name1}_{ma_name2}'\n",
    "    rsi_signal_column = f'signal_{rsi_column}'\n",
    "    ma_position_open = f'position_open_{ma_name1}_{ma_name2}'\n",
    "    rsi_position_open = f'position_open_{rsi_column}'\n",
    "    \n",
    "    # Update position open columns\n",
    "    candles[ma_position_open] = candles[ma_signal_column] == 1\n",
    "    candles[rsi_position_open] = candles[rsi_signal_column] == 1\n",
    "    \n",
    "    return candles\n",
    "\n",
    "def determine_entry_prices(candles, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5', ticker_to_tick_size=None, ticker=None):\n",
    "    \"\"\"\n",
    "    Determine entry prices for MA and RSI strategies based on signals.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): The DataFrame containing candle data.\n",
    "    - ma_name1 (str): Column name for the first moving average.\n",
    "    - ma_name2 (str): Column name for the second moving average.\n",
    "    - rsi_column (str): Column name for the RSI indicator.\n",
    "    - ticker_to_tick_size (dict): Mapping of tickers to their tick sizes.\n",
    "    - ticker (str): The ticker for which the tick size applies.\n",
    "\n",
    "    Returns:\n",
    "    - candles (pd.DataFrame): The DataFrame with updated entry price columns.\n",
    "    \"\"\"\n",
    "    # Dynamically generate column names\n",
    "    ma_signal_column = f'signal_{ma_name1}_{ma_name2}'\n",
    "    rsi_signal_column = f'signal_{rsi_column}'\n",
    "    ma_entry_price = f'entry_price_{ma_name1}_{ma_name2}'\n",
    "    rsi_entry_price = f'entry_price_{rsi_column}'\n",
    "\n",
    "    # Initialize entry price columns\n",
    "    candles[ma_entry_price] = None\n",
    "    candles[rsi_entry_price] = None\n",
    "\n",
    "    # Get tick size\n",
    "    tick_size = ticker_to_tick_size.get(ticker, 0) if ticker_to_tick_size else 0\n",
    "\n",
    "    # Moving Average Strategy\n",
    "    ma_signals = candles[ma_signal_column]\n",
    "    ma_close_prices = candles['close']\n",
    "    ma_entry_mask = (ma_signals == 1) & (ma_signals.shift(1) != 1)\n",
    "    candles.loc[ma_entry_mask, ma_entry_price] = ma_close_prices[ma_entry_mask] + tick_size\n",
    "\n",
    "    # RSI Strategy\n",
    "    rsi_signals = candles[rsi_signal_column]\n",
    "    rsi_entry_mask = (rsi_signals == 1) & (rsi_signals.shift(1) != 1)\n",
    "    candles.loc[rsi_entry_mask, rsi_entry_price] = ma_close_prices[rsi_entry_mask] + tick_size\n",
    "\n",
    "    return candles\n",
    "\n",
    "def determine_exit_prices(candles, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5', ticker_to_tick_size=None, ticker=None):\n",
    "    \"\"\"\n",
    "    Determine exit prices for MA and RSI strategies based on signals.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): The DataFrame containing candle data.\n",
    "    - ma_name1 (str): Column name for the first moving average.\n",
    "    - ma_name2 (str): Column name for the second moving average.\n",
    "    - rsi_column (str): Column name for the RSI indicator.\n",
    "    - ticker_to_tick_size (dict): Mapping of tickers to their tick sizes.\n",
    "    - ticker (str): The ticker for which the tick size applies.\n",
    "\n",
    "    Returns:\n",
    "    - candles (pd.DataFrame): The DataFrame with updated exit price columns.\n",
    "    \"\"\"\n",
    "    # Dynamically generate column names\n",
    "    ma_signal_column = f'signal_{ma_name1}_{ma_name2}'\n",
    "    rsi_signal_column = f'signal_{rsi_column}'\n",
    "    ma_exit_price = f'exit_price_{ma_name1}_{ma_name2}'\n",
    "    rsi_exit_price = f'exit_price_{rsi_column}'\n",
    "\n",
    "    # Initialize exit price columns\n",
    "    candles[ma_exit_price] = None\n",
    "    candles[rsi_exit_price] = None\n",
    "\n",
    "    # Get tick size\n",
    "    tick_size = ticker_to_tick_size.get(ticker, 0) if ticker_to_tick_size else 0\n",
    "\n",
    "    # Moving Average Strategy\n",
    "    ma_signals = candles[ma_signal_column]\n",
    "    ma_close_prices = candles['close']\n",
    "    ma_exit_mask = (ma_signals == 0) & (ma_signals.shift(1) == 1)\n",
    "    candles.loc[ma_exit_mask, ma_exit_price] = ma_close_prices[ma_exit_mask] - tick_size\n",
    "\n",
    "    # RSI Strategy\n",
    "    rsi_signals = candles[rsi_signal_column]\n",
    "    rsi_exit_mask = (rsi_signals == 0) & (rsi_signals.shift(1) == 1)\n",
    "    candles.loc[rsi_exit_mask, rsi_exit_price] = ma_close_prices[rsi_exit_mask] - tick_size\n",
    "\n",
    "    return candles\n",
    "\n",
    "def calculate_stop_losses(candles, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5'):\n",
    "    \"\"\"\n",
    "    Dynamically calculate stop loss levels for MA and RSI strategies and ensure they persist while positions are open.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): The DataFrame containing candle data.\n",
    "    - ma_name1 (str): Column name for the first moving average.\n",
    "    - ma_name2 (str): Column name for the second moving average.\n",
    "    - rsi_column (str): Column name for the RSI indicator.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated DataFrame with dynamically named stop loss columns.\n",
    "    \"\"\"\n",
    "    # Dynamically generate column names\n",
    "    ma_entry_price = f'entry_price_{ma_name1}_{ma_name2}'\n",
    "    ma_exit_price = f'exit_price_{ma_name1}_{ma_name2}'\n",
    "    rsi_entry_price = f'entry_price_{rsi_column}'\n",
    "    rsi_exit_price = f'exit_price_{rsi_column}'\n",
    "    stop_loss_ma = f'stop_loss_{ma_name1}_{ma_name2}'\n",
    "    stop_loss_rsi = f'stop_loss_{rsi_column}'\n",
    "\n",
    "    # Initialize stop loss columns\n",
    "    candles[stop_loss_ma] = None\n",
    "    candles[stop_loss_rsi] = None\n",
    "\n",
    "    # Moving Average Stop Loss\n",
    "    ma_entry_mask = candles[ma_entry_price].notnull()\n",
    "    ma_exit_mask = candles[ma_exit_price].notnull()\n",
    "    \n",
    "    # Set stop loss where positions open\n",
    "    candles.loc[ma_entry_mask, stop_loss_ma] = candles[ma_entry_price] - candles['candle_span_max']\n",
    "\n",
    "    # Reset stop loss and close position where positions close\n",
    "    candles.loc[ma_exit_mask, stop_loss_ma] = None\n",
    "\n",
    "    # RSI Stop Loss\n",
    "    rsi_entry_mask = candles[rsi_entry_price].notnull()\n",
    "    rsi_exit_mask = candles[rsi_exit_price].notnull()\n",
    "    \n",
    "    # Set stop loss where positions open\n",
    "    candles.loc[rsi_entry_mask, stop_loss_rsi] = candles[rsi_entry_price] - candles['candle_span_max']\n",
    "\n",
    "    # Reset stop loss and close position where positions close\n",
    "    candles.loc[rsi_exit_mask, stop_loss_rsi] = None\n",
    "\n",
    "    # Forward-fill stop loss for both strategies\n",
    "    candles[stop_loss_ma] = candles[stop_loss_ma].ffill()\n",
    "    candles[stop_loss_rsi] = candles[stop_loss_rsi].ffill()\n",
    "\n",
    "    return candles\n",
    "\n",
    "def track_stop_loss_hits(candles, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5', ticker_to_tick_size=None, ticker=None): # Tick size is not being used, address this here and in the function calling\n",
    "    \"\"\"\n",
    "    Track whether stop losses have been hit for MA and RSI strategies and update dynamically named columns.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): The DataFrame containing candle data.\n",
    "    - ma_name1 (str): Column name for the first moving average.\n",
    "    - ma_name2 (str): Column name for the second moving average.\n",
    "    - rsi_column (str): Column name for the RSI indicator.\n",
    "    - ticker_to_tick_size (dict): Mapping of tickers to their tick sizes.\n",
    "    - ticker (str): The ticker for which the tick size applies.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated DataFrame with dynamically named stop loss hit flags.\n",
    "    \"\"\"\n",
    "    # Dynamically generate column names\n",
    "    stop_loss_ma = f'stop_loss_{ma_name1}_{ma_name2}'\n",
    "    stop_loss_rsi = f'stop_loss_{rsi_column}'\n",
    "    ma_stop_loss_hit = f'stop_loss_hit_{ma_name1}_{ma_name2}'\n",
    "    rsi_stop_loss_hit = f'stop_loss_hit_{rsi_column}'\n",
    "    ma_position_open = f'position_open_{ma_name1}_{ma_name2}'\n",
    "    rsi_position_open = f'position_open_{rsi_column}'\n",
    "    \n",
    "    # Initialize stop loss hit columns\n",
    "    candles[ma_stop_loss_hit] = False\n",
    "    candles[rsi_stop_loss_hit] = False\n",
    "\n",
    "    # Get tick size\n",
    "    tick_size = ticker_to_tick_size.get(ticker, 0) if ticker_to_tick_size else 0\n",
    "\n",
    "    # Ensure stop loss values are numerical (convert None to NaN)\n",
    "    candles[stop_loss_ma] = candles[stop_loss_ma].fillna(float('inf'))\n",
    "    candles[stop_loss_rsi] = candles[stop_loss_rsi].fillna(float('inf'))\n",
    "\n",
    "    # Moving Average Stop Loss Hit Logic\n",
    "    ma_hit_condition = (candles[stop_loss_ma].notnull()) & (candles['close'] <= candles[stop_loss_ma]) & candles[ma_position_open]\n",
    "    candles.loc[ma_hit_condition, ma_stop_loss_hit] = True\n",
    "\n",
    "    # RSI Stop Loss Hit Logic\n",
    "    rsi_hit_condition = (candles[stop_loss_rsi].notnull()) & (candles['close'] <= candles[stop_loss_rsi]) & candles[rsi_position_open]\n",
    "    candles.loc[rsi_hit_condition, rsi_stop_loss_hit] = True\n",
    "\n",
    "    return candles\n",
    "\n",
    "def adjust_signals_for_stop_loss(candles, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5'):\n",
    "    \"\"\"\n",
    "    Adjust MA and RSI signals to 0 where stop loss has been hit.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): The DataFrame containing candle data.\n",
    "    - ma_name1 (str): Column name for the first moving average.\n",
    "    - ma_name2 (str): Column name for the second moving average.\n",
    "    - rsi_column (str): Column name for the RSI indicator.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated DataFrame with adjusted signals.\n",
    "    \"\"\"\n",
    "    # Dynamically generate column names\n",
    "    ma_signal_column = f'signal_{ma_name1}_{ma_name2}'\n",
    "    rsi_signal_column = f'signal_{rsi_column}'\n",
    "    ma_stop_loss_hit_column = f'stop_loss_hit_{ma_name1}_{ma_name2}'\n",
    "    rsi_stop_loss_hit_column = f'stop_loss_hit_{rsi_column}'\n",
    "\n",
    "    # Adjust MA and RSI signals where stop loss has been hit\n",
    "    candles.loc[candles[ma_stop_loss_hit_column], ma_signal_column] = 0\n",
    "    candles.loc[candles[rsi_stop_loss_hit_column], rsi_signal_column] = 0\n",
    "\n",
    "    return candles\n",
    "\n",
    "def update_stop_loss(candles, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5'):\n",
    "    \"\"\"\n",
    "    Dynamically set stop loss columns to NaN where corresponding signal columns are 0.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): The DataFrame containing candle data.\n",
    "    - ma_name1 (str): Column name for the first moving average.\n",
    "    - ma_name2 (str): Column name for the second moving average.\n",
    "    - rsi_column (str): Column name for the RSI indicator.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated DataFrame with dynamically named stop loss columns modified.\n",
    "    \"\"\"\n",
    "    # Dynamically generate column names\n",
    "    ma_signal_column = f'signal_{ma_name1}_{ma_name2}'\n",
    "    rsi_signal_column = f'signal_{rsi_column}'\n",
    "    stop_loss_ma = f'stop_loss_{ma_name1}_{ma_name2}'\n",
    "    stop_loss_rsi = f'stop_loss_{rsi_column}'\n",
    "\n",
    "    # Update stop loss columns to NaN where signals are 0\n",
    "    candles.loc[candles[ma_signal_column] == 0, stop_loss_ma] = float('nan')\n",
    "    candles.loc[candles[rsi_signal_column] == 0, stop_loss_rsi] = float('nan')\n",
    "\n",
    "    return candles\n",
    "\n",
    "def calculate_profit_loss(candles, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5', contract_multiplier=1, trade_commission=1.5):\n",
    "    \"\"\" \n",
    "    Dynamically calculate profit and loss based on entry and exit price columns, including cumulative commission costs.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): The DataFrame containing candle data with entry and exit price columns.\n",
    "    - multiplier (float): The multiplier for PnL calculation (e.g., contract size).\n",
    "    - trade_commission (float): The commission cost per trade.\n",
    "    - ma_name1 (str): Column name for the first moving average.\n",
    "    - ma_name2 (str): Column name for the second moving average.\n",
    "    - rsi_column (str): Column name for the RSI indicator.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with dynamically named profit/loss and commission cost columns.\n",
    "    \"\"\"\n",
    "    # Dynamically generate column names\n",
    "    ma_entry_price = f'entry_price_{ma_name1}_{ma_name2}'\n",
    "    ma_exit_price = f'exit_price_{ma_name1}_{ma_name2}'\n",
    "    rsi_entry_price = f'entry_price_{rsi_column}'\n",
    "    rsi_exit_price = f'exit_price_{rsi_column}'\n",
    "    pnl_ma_col = f'pnl_{ma_name1}_{ma_name2}'\n",
    "    pnl_rsi_col = f'pnl_{rsi_column}'\n",
    "    cum_pnl_ma_col = f'cum_{pnl_ma_col}'\n",
    "    cum_pnl_rsi_col = f'cum_{pnl_rsi_col}'\n",
    "    cum_pnl_all_col = f'cum_pnl_all_{ma_name1}_{ma_name2}_{rsi_column}'\n",
    "    ma_commission_col = f'commission_cost_{ma_name1}_{ma_name2}'\n",
    "    rsi_commission_col = f'commission_cost_{rsi_column}'\n",
    "\n",
    "    # Initialize PnL and commission columns\n",
    "    candles[pnl_ma_col] = 0.0\n",
    "    candles[pnl_rsi_col] = 0.0\n",
    "    candles[ma_commission_col] = 0.0\n",
    "    candles[rsi_commission_col] = 0.0\n",
    "\n",
    "    # Moving Average Strategy PnL and Commission Calculation\n",
    "    ma_entry_indices = candles.index[candles[ma_entry_price].notnull()]\n",
    "    ma_exit_indices = candles.index[candles[ma_exit_price].notnull()]\n",
    "\n",
    "    # Pair up entry and exit prices\n",
    "    valid_pairs_ma = min(len(ma_entry_indices), len(ma_exit_indices))\n",
    "    ma_entry_prices = candles.loc[ma_entry_indices[:valid_pairs_ma], ma_entry_price].values\n",
    "    ma_exit_prices = candles.loc[ma_exit_indices[:valid_pairs_ma], ma_exit_price].values\n",
    "\n",
    "    # Calculate commission costs for MA strategy\n",
    "    candles[ma_commission_col] = candles[ma_entry_price].notna().astype(int) * trade_commission + \\\n",
    "                                 candles[ma_exit_price].notna().astype(int) * trade_commission\n",
    "    candles[ma_commission_col] = candles[ma_commission_col].cumsum()  # Accumulate commission costs\n",
    "\n",
    "    # Calculate PnL for MA strategy\n",
    "    ma_pnl = (ma_exit_prices - ma_entry_prices) * contract_multiplier\n",
    "    candles.loc[ma_exit_indices[:valid_pairs_ma], pnl_ma_col] = ma_pnl\n",
    "\n",
    "    # RSI Strategy PnL and Commission Calculation\n",
    "    rsi_entry_indices = candles.index[candles[rsi_entry_price].notnull()]\n",
    "    rsi_exit_indices = candles.index[candles[rsi_exit_price].notnull()]\n",
    "\n",
    "    # Pair up entry and exit prices\n",
    "    valid_pairs_rsi = min(len(rsi_entry_indices), len(rsi_exit_indices))\n",
    "    rsi_entry_prices = candles.loc[rsi_entry_indices[:valid_pairs_rsi], rsi_entry_price].values\n",
    "    rsi_exit_prices = candles.loc[rsi_exit_indices[:valid_pairs_rsi], rsi_exit_price].values\n",
    "\n",
    "    # Calculate commission costs for RSI strategy\n",
    "    candles[rsi_commission_col] = candles[rsi_entry_price].notna().astype(int) * trade_commission + \\\n",
    "                                  candles[rsi_exit_price].notna().astype(int) * trade_commission\n",
    "    candles[rsi_commission_col] = candles[rsi_commission_col].cumsum()  # Accumulate commission costs\n",
    "\n",
    "    # Calculate PnL for RSI strategy\n",
    "    rsi_pnl = (rsi_exit_prices - rsi_entry_prices) * contract_multiplier\n",
    "    candles.loc[rsi_exit_indices[:valid_pairs_rsi], pnl_rsi_col] = rsi_pnl\n",
    "\n",
    "    # Calculate cumulative PnL for both strategies\n",
    "    candles[cum_pnl_ma_col] = candles[pnl_ma_col].cumsum()\n",
    "    candles[cum_pnl_rsi_col] = candles[pnl_rsi_col].cumsum()\n",
    "\n",
    "    # Calculate combined cumulative PnL\n",
    "    candles[cum_pnl_all_col] = candles[cum_pnl_ma_col] + candles[cum_pnl_rsi_col]\n",
    "\n",
    "    return candles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trading_strategies(candles, \n",
    "                           ma_name1='wma_5', \n",
    "                           ma_name2='sma_5', \n",
    "                           rsi_column='rsi_5',  \n",
    "                           figsize=(40, 20), \n",
    "                           font_size=10, \n",
    "                           ma_markersize=50, \n",
    "                           signal_markersize_y=400, \n",
    "                           signal_markersize_b=250\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Plots the minute_candles DataFrame with two selected moving averages and optional RSI.\n",
    "    Also plots cumulative profit for MA and RSI strategies on a secondary axis.\n",
    "\n",
    "    Parameters:\n",
    "    - ma_name1 (str): The column name of the first moving average to plot.\n",
    "    - ma_name2 (str): The column name of the second moving average to plot.\n",
    "    - signal_column (str): The column name of the signal data (default is 'signal').\n",
    "    - figsize (tuple): The size of the plot (width, height) in inches (default is (30, 20)).\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Clean the data to ensure numeric columns are valid\n",
    "        columns_to_convert = ['open', 'high', 'low', 'close', 'volume', ma_name1, ma_name2, rsi_column] \n",
    "        candles[columns_to_convert] = candles[columns_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "        # Generate dynamic column names for PnL and signals\n",
    "        ma_entry_price = f'entry_price_{ma_name1}_{ma_name2}'\n",
    "        ma_exit_price = f'exit_price_{ma_name1}_{ma_name2}'\n",
    "        rsi_entry_price = f'entry_price_{rsi_column}'\n",
    "        rsi_exit_price = f'exit_price_{rsi_column}'\n",
    "        cum_pnl_ma_col = f'cum_pnl_{ma_name1}_{ma_name2}'\n",
    "        cum_pnl_rsi_col = f'cum_pnl_{rsi_column}'\n",
    "        cum_pnl_all_col = f'cum_pnl_all_{ma_name1}_{ma_name2}_{rsi_column}'\n",
    "        stop_loss_ma = f'stop_loss_{ma_name1}_{ma_name2}'\n",
    "        stop_loss_rsi = f'stop_loss_{rsi_column}'\n",
    "\n",
    "        # Select the columns to plot\n",
    "        plot_data = candles[['datetime', 'open', 'high', 'low', 'close', 'volume', \n",
    "                             ma_name1, ma_name2, rsi_column, \n",
    "                             ma_entry_price, ma_exit_price, rsi_entry_price, rsi_exit_price,\n",
    "                             cum_pnl_ma_col, cum_pnl_rsi_col, cum_pnl_all_col,\n",
    "                             stop_loss_ma, stop_loss_rsi]].copy() # here\n",
    "        plot_data.set_index('datetime', inplace=True)\n",
    "\n",
    "        # Create the additional plots for the moving averages and RSI, but only if they are warmed up\n",
    "        add_plots = []\n",
    "\n",
    "        # Check if the moving averages have enough valid data to plot\n",
    "        if not candles[ma_name1].isnull().all() and not candles[ma_name2].isnull().all():\n",
    "            add_plots.append(mpf.make_addplot(plot_data[ma_name1], color='yellow', type='scatter', marker='o', markersize=ma_markersize, label=f'{ma_name1}'))\n",
    "            add_plots.append(mpf.make_addplot(plot_data[ma_name1], color='yellow', linestyle='-', width=0.75))\n",
    "            add_plots.append(mpf.make_addplot(plot_data[ma_name2], color='purple', type='scatter', marker='o', markersize=ma_markersize, label=f'{ma_name2}'))\n",
    "            add_plots.append(mpf.make_addplot(plot_data[ma_name2], color='purple', linestyle='-', width=0.75))\n",
    "        else:\n",
    "            print(\"Moving averages have not warmed up yet. Plotting without them.\")\n",
    "\n",
    "        # Check if the RSI has enough valid data to plot\n",
    "        if not candles[rsi_column].isnull().all():\n",
    "            add_plots.append(mpf.make_addplot(candles[rsi_column], panel=2, color='blue', type='scatter', marker='o', markersize=ma_markersize, label='RSI'))\n",
    "            add_plots.append(mpf.make_addplot(candles[rsi_column], panel=2, color='blue', linestyle='-', width=0.75))\n",
    "            add_plots.append(mpf.make_addplot(candles['trend_indicator'], panel=2, color='white', type='scatter', marker='o', markersize=ma_markersize, label='RSI'))\n",
    "            add_plots.append(mpf.make_addplot(candles['trend_indicator'], panel=2, color='white', linestyle='-', width=0.75))\n",
    "            add_plots.append(mpf.make_addplot(candles['hundred_line'], panel=2, color='red', linestyle=':', secondary_y=False))\n",
    "            add_plots.append(mpf.make_addplot(candles['fifty_line'], panel=2, color='yellow', linestyle=':', secondary_y=False))\n",
    "            add_plots.append(mpf.make_addplot(candles['zero_line'], panel=2, color='green', linestyle=':', secondary_y=False))\n",
    "            add_plots.append(mpf.make_addplot(candles['trend_high_threshold'], panel=2, color='white', linestyle=':', secondary_y=False))\n",
    "            add_plots.append(mpf.make_addplot(candles['trend_low_threshold'], panel=2, color='white', linestyle=':', secondary_y=False))\n",
    "        else:\n",
    "            print(\"RSI has not warmed up yet. Plotting without it.\")\n",
    "\n",
    "        # Add buy, sell, and neutral markers if signal_column exists. Eliminate the if else statement to revert to working order\n",
    "        if ma_entry_price in candles.columns and ma_exit_price in candles.columns:\n",
    "            add_plots.append(mpf.make_addplot(candles[ma_entry_price], type='scatter', marker='^', markersize=signal_markersize_y, color='yellow', panel=0, secondary_y=False))\n",
    "            add_plots.append(mpf.make_addplot(candles[ma_exit_price], type='scatter', marker='o', markersize=signal_markersize_y, color='yellow', panel=0, secondary_y=False))\n",
    "        else:\n",
    "            print(\"Buy/Sell markers for MA strat have not warmed up yet. Plotting without them.\")\n",
    "\n",
    "        # Add buy, sell, and neutral markers for RSI strategy\n",
    "        if rsi_entry_price in candles.columns and rsi_exit_price in candles.columns:\n",
    "            add_plots.append(mpf.make_addplot(candles[rsi_entry_price], type='scatter', marker='^', markersize=signal_markersize_b, color='blue', panel=0, secondary_y=False))\n",
    "            add_plots.append(mpf.make_addplot(candles[rsi_exit_price], type='scatter', marker='o', markersize=signal_markersize_b, color='blue', panel=0, secondary_y=False))\n",
    "        else:\n",
    "            print(\"Buy/Sell markers for RSI strat have not warmed up yet. Plotting without them.\")\n",
    "\n",
    "        # Add cumulative profit plots on a secondary y-axis with dynamic names\n",
    "        add_plots.append(mpf.make_addplot(candles[cum_pnl_ma_col], panel=0, color='yellow', secondary_y=True, label=f'Cumulative PnL (MA: {ma_name1}_{ma_name2})', linestyle='-', width=1.25))\n",
    "        add_plots.append(mpf.make_addplot(candles[cum_pnl_rsi_col], panel=0, color='blue', secondary_y=True, label=f'Cumulative PnL (RSI: {rsi_column})', linestyle='-', width=1.25))\n",
    "        add_plots.append(mpf.make_addplot(candles[cum_pnl_all_col], panel=0, color='green', secondary_y=True, label=f'Cumulative PnL (Combined)', linestyle='-', width=1.25))\n",
    "\n",
    "        # Add stop-loss markers (x) for both MA and RSI strategies\n",
    "        # if 'stop_loss_ma' in candles.columns:\n",
    "        add_plots.append(mpf.make_addplot(candles[stop_loss_ma], type='scatter', marker='x', markersize=100, color='yellow', panel=0, secondary_y=False))\n",
    "        # else:\n",
    "        #     print(\"There are no stop loss markers for MA strat\")\n",
    "        # if 'stop_loss_rsi' in candles.columns:\n",
    "        add_plots.append(mpf.make_addplot(candles[stop_loss_rsi], type='scatter', marker='x', markersize=50, color='blue', panel=0, secondary_y=False))\n",
    "        # else:\n",
    "        #     print(\"There are no stop loss markers for RSI strat\")\n",
    "\n",
    "        # Add price action envelope as white lines\n",
    "        if 'price_action_upper' in candles.columns and 'price_action_lower' in candles.columns:\n",
    "            add_plots.append(mpf.make_addplot(candles['price_action_upper'], color='white', linestyle='-', width=0.5, label='Price Action Upper'))\n",
    "            add_plots.append(mpf.make_addplot(candles['price_action_lower'], color='white', linestyle='-', width=0.5, label='Price Action Lower'))\n",
    "            # add_plots.append(mpf.make_addplot(candles['ma_price_action_upper'], color='white', linestyle='-', width=0.5, label='Price Action Upper'))\n",
    "            # add_plots.append(mpf.make_addplot(candles['ma_price_action_lower'], color='white', linestyle='-', width=0.5, label='Price Action Lower'))\n",
    "        else:\n",
    "            print(\"Price action envelope not calculating properly\")\n",
    "\n",
    "        # Create a custom style with a black background\n",
    "        black_style = mpf.make_mpf_style(\n",
    "            base_mpf_style='charles',  # Start with the 'charles' style and modify it\n",
    "            facecolor='black',         # Set the background color to black\n",
    "            gridcolor='black',          # Set the grid line color\n",
    "            edgecolor='purple',          # Set the edge color for candles and boxes\n",
    "            figcolor='black',          # Set the figure background color to black\n",
    "            rc={'axes.labelcolor': 'yellow', \n",
    "                'xtick.color': 'yellow', \n",
    "                'ytick.color': 'yellow', \n",
    "                'axes.titlecolor': 'yellow',\n",
    "                'font.size': font_size, \n",
    "                'axes.labelsize': font_size,\n",
    "                'axes.titlesize': font_size,\n",
    "                'xtick.labelsize': font_size,\n",
    "                'ytick.labelsize': font_size,\n",
    "                'legend.fontsize': font_size}  # Set tick and label colors to white\n",
    "        )\n",
    "\n",
    "        # Plot using mplfinance\n",
    "        mpf.plot(plot_data, type='candle', style=black_style, \n",
    "                title='',\n",
    "                ylabel='Price', \n",
    "                addplot=add_plots, \n",
    "                figsize=figsize,\n",
    "                volume=True,\n",
    "                panel_ratios=(8, 2),\n",
    "                #  panel_ratios=(8, 2, 2),             \n",
    "                tight_layout=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Something wrong in the plotting_moving_averages function: {e}\")\n",
    "\n",
    "def visualize_trades(candles, ticker_to_tick_size, ticker_to_point_value, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5', lower_slice=0, upper_slice=-1):\n",
    "\n",
    "    \"\"\"\n",
    "    Visualize trades and print summary statistics, including tick size for each ticker.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (dict): Dictionary of DataFrames with candle data for each ticker.\n",
    "    - ticker_to_tick_size (dict): Dictionary mapping tickers to their respective tick sizes.\n",
    "    - ticker_to_point_value (dict): Dictionary mapping tickers to their respective point values.\n",
    "    - ma_name1, ma_name2, rsi_column: Names of MA and RSI columns.\n",
    "    - lower_slice, upper_slice: Range of rows to visualize.\n",
    "    \"\"\"\n",
    "    # Generate dynamic column names for PnL and trade metrics\n",
    "    pnl_ma_col = f'pnl_{ma_name1}_{ma_name2}'\n",
    "    pnl_rsi_col = f'pnl_{rsi_column}'\n",
    "    cum_pnl_ma_col = f'cum_{pnl_ma_col}'\n",
    "    cum_pnl_rsi_col = f'cum_{pnl_rsi_col}'\n",
    "    cum_pnl_all_col = f'cum_pnl_all_{ma_name1}_{ma_name2}_{rsi_column}'\n",
    "    ma_entry_price = f'entry_price_{ma_name1}_{ma_name2}'\n",
    "    ma_exit_price = f'exit_price_{ma_name1}_{ma_name2}'\n",
    "    rsi_entry_price = f'entry_price_{rsi_column}'\n",
    "    rsi_exit_price = f'exit_price_{rsi_column}'\n",
    "    ma_commission_col = f'commission_cost_{ma_name1}_{ma_name2}'\n",
    "    rsi_commission_col = f'commission_cost_{rsi_column}'\n",
    "\n",
    "    # Variable to accumulate total dollar PnL\n",
    "    total_dollar_pnl_sum = 0.0\n",
    "\n",
    "    # Iterate through the candles dictionary\n",
    "    for ticker, minute_candles_df in candles.items():\n",
    "        # Create a copy of the DataFrame for the specified slice\n",
    "        minute_candles_viz_1 = minute_candles_df[lower_slice:upper_slice].copy()\n",
    "        tick_size = ticker_to_tick_size.get(ticker, \"Unknown\")  # Retrieve tick size or default to \"Unknown\"\n",
    "        point_value = ticker_to_point_value.get(ticker, 1)  # Retrieve point value or default to 1\n",
    "\n",
    "        try:\n",
    "            # Plot moving averages\n",
    "            plot_trading_strategies(\n",
    "                minute_candles_viz_1,\n",
    "                ma_name1=ma_name1, ma_name2=ma_name2, rsi_column=rsi_column,\n",
    "                figsize=(40, 20), font_size=20,\n",
    "                ma_markersize=50, signal_markersize_y=450, signal_markersize_b=300\n",
    "            )\n",
    "            \n",
    "            # Calculate cumulative PnL and other statistics\n",
    "            ma_pnl = round(minute_candles_df[cum_pnl_ma_col].iloc[-1], 3)\n",
    "            rsi_pnl = round(minute_candles_df[cum_pnl_rsi_col].iloc[-1], 3)\n",
    "            total_pnl = round(minute_candles_df[cum_pnl_all_col].iloc[-1], 3)\n",
    "            close_price_diff = round(minute_candles_df[\"close\"].iloc[-1] - minute_candles_df[\"close\"].iloc[0], 3)\n",
    "            point_alpha = round(total_pnl - close_price_diff, 3)\n",
    "\n",
    "            # Retrieve total commission costs\n",
    "            ma_commission_total = round(minute_candles_df[ma_commission_col].iloc[-1], 3) if ma_commission_col in minute_candles_df else 0.0\n",
    "            rsi_commission_total = round(minute_candles_df[rsi_commission_col].iloc[-1], 3) if rsi_commission_col in minute_candles_df else 0.0\n",
    "            total_commission_cost = ma_commission_total + rsi_commission_total\n",
    "\n",
    "            # Calculate total dollar PnLs\n",
    "            ma_dollar_pnl = (ma_pnl * point_value) - ma_commission_total\n",
    "            rsi_dollar_pnl = (rsi_pnl * point_value) - rsi_commission_total\n",
    "            total_dollar_pnl = (total_pnl * point_value) - total_commission_cost\n",
    "            total_dollar_pnl_sum += total_dollar_pnl\n",
    "            close_price_dollar_diff = close_price_diff * point_value\n",
    "            dollar_alpha = (point_alpha * point_value) - total_commission_cost\n",
    "\n",
    "            # Count the number of trades for MA and RSI strategies\n",
    "            ma_trades = (minute_candles_df[ma_exit_price].notna().sum() + minute_candles_df[ma_entry_price].notna().sum())/2\n",
    "            rsi_trades = (minute_candles_df[rsi_exit_price].notna().sum() + minute_candles_df[rsi_entry_price].notna().sum())/2\n",
    "            total_trades = ma_trades + rsi_trades\n",
    "\n",
    "            # Calculate max gain and max loss for MA, RSI, and total strategies\n",
    "            ma_max_gain = round(minute_candles_df[cum_pnl_ma_col].max(), 3)\n",
    "            ma_max_loss = round(minute_candles_df[cum_pnl_ma_col].min(), 3)\n",
    "            rsi_max_gain = round(minute_candles_df[cum_pnl_rsi_col].max(), 3)\n",
    "            rsi_max_loss = round(minute_candles_df[cum_pnl_rsi_col].min(), 3)\n",
    "            total_max_gain = round(minute_candles_df[cum_pnl_all_col].max(), 3)\n",
    "            total_max_loss = round(minute_candles_df[cum_pnl_all_col].min(), 3)\n",
    "            ma_max_dollar_gain = (ma_max_gain * point_value) - ma_commission_total\n",
    "            ma_max_dollar_loss = (ma_max_loss * point_value) - ma_commission_total\n",
    "            rsi_max_dollar_gain = (rsi_max_gain * point_value) - rsi_commission_total\n",
    "            rsi_max_dollar_loss = (rsi_max_loss * point_value) - rsi_commission_total\n",
    "            total_max_dollar_gain = (total_max_gain * point_value) - total_commission_cost\n",
    "            total_max_dollar_loss = (total_max_loss * point_value) - total_commission_cost\n",
    "\n",
    "            # Print detailed statistics for the ticker\n",
    "            print(\n",
    "                f\"{ticker}: {len(minute_candles_df)} rows, \"\n",
    "                f\"Total PnL: {total_pnl:.2f}pt/${total_dollar_pnl:.2f}, Total trades: {total_trades}, Total Commission Cost: ${total_commission_cost:.2f}, Total Max Gain: {total_max_gain:.2f}pt/${total_max_dollar_gain}, Total Max Loss: {total_max_loss:.2f}pt/${total_max_dollar_loss}, \"\n",
    "                f\"MA PnL: {ma_pnl:.2f}pt/${ma_dollar_pnl:.2f}, MA trades: {ma_trades}, MA Commission Cost: ${ma_commission_total:.2f}, MA Max Gain: {ma_max_gain:.2f}pt/${ma_max_dollar_gain}, MA Max Loss: {ma_max_loss:.2f}pt/${ma_max_dollar_loss}, \"\n",
    "                f\"RSI PnL: {rsi_pnl:.2f}pt/${rsi_dollar_pnl:.2f}, RSI trades: {rsi_trades}, RSI Commission Cost: ${rsi_commission_total:.2f}, RSI Max Gain: {rsi_max_gain:.2f}pt/${rsi_max_dollar_gain}, RSI Max Loss: {rsi_max_loss:.2f}pt/${rsi_max_dollar_loss}, \"\n",
    "                f\"Close Price Difference: {close_price_diff:.2f}pt/${close_price_dollar_diff:.2f}, Alpha: {point_alpha:.2f}pt/${dollar_alpha:.2f}, Tick Size: {tick_size}, \"                \n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Handle any errors that occur during the plotting\n",
    "            print(f\"Error in visualize_trades_1 for {ticker}: {e}\")\n",
    "\n",
    "    # Print total dollar PnL sum across all tickers\n",
    "    print(f\"\\nTotal Dollar PnL Across All Tickers: {total_dollar_pnl_sum:.2f}\")\n",
    "\n",
    "# /ES: 398 rows, Total PnL: nanpt/$nan, Total trades: 90.0, Total Commission Cost: $nan, Total Max Gain: 2.00pt/$nan, Total Max Loss: -28.00pt/$nan, MA PnL: nanpt/$nan,\n",
    "# MA trades: 46.5, MA Commission Cost: $nan, MA Max Gain: 0.75pt/$nan, MA Max Loss: -17.25pt/$nan, RSI PnL: nanpt/$nan, RSI trades: 43.5, RSI Commission Cost: $nan, \n",
    "# RSI Max Gain: 1.50pt/$nan, RSI Max Loss: -16.75pt/$nan, Close Price Difference: 34.00pt/$1700.00, Alpha: nanpt/$nan, Tick Size: 0.2\n",
    "\n",
    "def print_all_pnls(candles, compression_factor, ticker_to_tick_size, ticker_to_point_value, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5'):\n",
    "    \"\"\"\n",
    "    Prints detailed PnL and trade statistics for all tickers in a given dictionary of DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - candles: Dictionary of DataFrames containing trading data for multiple tickers.\n",
    "    - compression_factor: The compression factor associated with the current dataset.\n",
    "    - ticker_to_tick_size: Dictionary mapping tickers to tick sizes.\n",
    "    - ticker_to_point_value: Dictionary mapping tickers to point values.\n",
    "    - ma_name1, ma_name2: Moving average column names.\n",
    "    - rsi_column: RSI column name.\n",
    "    \"\"\"\n",
    "    # Generate dynamic column names for PnL and trade metrics\n",
    "    pnl_ma_col = f'pnl_{ma_name1}_{ma_name2}'\n",
    "    pnl_rsi_col = f'pnl_{rsi_column}'\n",
    "    cum_pnl_ma_col = f'cum_{pnl_ma_col}'\n",
    "    cum_pnl_rsi_col = f'cum_{pnl_rsi_col}'\n",
    "    cum_pnl_all_col = f'cum_pnl_all_{ma_name1}_{ma_name2}_{rsi_column}'\n",
    "    ma_entry_price = f'entry_price_{ma_name1}_{ma_name2}'\n",
    "    ma_exit_price = f'exit_price_{ma_name1}_{ma_name2}'\n",
    "    rsi_entry_price = f'entry_price_{rsi_column}'\n",
    "    rsi_exit_price = f'exit_price_{rsi_column}'\n",
    "    ma_commission_col = f'commission_cost_{ma_name1}_{ma_name2}'\n",
    "    rsi_commission_col = f'commission_cost_{rsi_column}'\n",
    "\n",
    "    # Accumulate total dollar PnL across tickers\n",
    "    total_dollar_pnl_sum = 0.0\n",
    "\n",
    "    for ticker, minute_candles_df in candles.items():\n",
    "        try:\n",
    "            # Retrieve tick size and point value for the ticker\n",
    "            tick_size = ticker_to_tick_size.get(ticker, \"Unknown\")\n",
    "            point_value = ticker_to_point_value.get(ticker, 1)\n",
    "\n",
    "            # Calculate cumulative PnL and other statistics\n",
    "            ma_pnl = round(minute_candles_df[cum_pnl_ma_col].iloc[-1], 3)\n",
    "            rsi_pnl = round(minute_candles_df[cum_pnl_rsi_col].iloc[-1], 3)\n",
    "            total_pnl = round(minute_candles_df[cum_pnl_all_col].iloc[-1], 3)\n",
    "            close_price_diff = round(minute_candles_df[\"close\"].iloc[-1] - minute_candles_df[\"close\"].iloc[0], 3)\n",
    "            point_alpha = round(total_pnl - close_price_diff, 3)\n",
    "\n",
    "            # Retrieve total commission costs\n",
    "            ma_commission_total = round(minute_candles_df[ma_commission_col].iloc[-1], 3) if ma_commission_col in minute_candles_df else 0.0\n",
    "            rsi_commission_total = round(minute_candles_df[rsi_commission_col].iloc[-1], 3) if rsi_commission_col in minute_candles_df else 0.0\n",
    "            total_commission_cost = ma_commission_total + rsi_commission_total\n",
    "\n",
    "            # Calculate total dollar PnLs\n",
    "            ma_dollar_pnl = (ma_pnl * point_value) - ma_commission_total\n",
    "            rsi_dollar_pnl = (rsi_pnl * point_value) - rsi_commission_total\n",
    "            total_dollar_pnl = (total_pnl * point_value) - total_commission_cost\n",
    "            total_dollar_pnl_sum += total_dollar_pnl\n",
    "            close_price_dollar_diff = close_price_diff * point_value\n",
    "            dollar_alpha = (point_alpha * point_value) - total_commission_cost\n",
    "\n",
    "            # Count the number of trades for MA and RSI strategies\n",
    "            ma_trades = (minute_candles_df[ma_exit_price].notna().sum() + minute_candles_df[ma_entry_price].notna().sum())/2\n",
    "            rsi_trades = (minute_candles_df[rsi_exit_price].notna().sum() + minute_candles_df[rsi_entry_price].notna().sum())/2\n",
    "            total_trades = ma_trades + rsi_trades\n",
    "\n",
    "            # Calculate max gain and max loss for MA, RSI, and total strategies\n",
    "            ma_max_gain = round(minute_candles_df[cum_pnl_ma_col].max(), 3)\n",
    "            ma_max_loss = round(minute_candles_df[cum_pnl_ma_col].min(), 3)\n",
    "            rsi_max_gain = round(minute_candles_df[cum_pnl_rsi_col].max(), 3)\n",
    "            rsi_max_loss = round(minute_candles_df[cum_pnl_rsi_col].min(), 3)\n",
    "            total_max_gain = round(minute_candles_df[cum_pnl_all_col].max(), 3)\n",
    "            total_max_loss = round(minute_candles_df[cum_pnl_all_col].min(), 3)\n",
    "            ma_max_dollar_gain = (ma_max_gain * point_value) - ma_commission_total\n",
    "            ma_max_dollar_loss = (ma_max_loss * point_value) - ma_commission_total\n",
    "            rsi_max_dollar_gain = (rsi_max_gain * point_value) - rsi_commission_total\n",
    "            rsi_max_dollar_loss = (rsi_max_loss * point_value) - rsi_commission_total\n",
    "            total_max_dollar_gain = (total_max_gain * point_value) - total_commission_cost\n",
    "            total_max_dollar_loss = (total_max_loss * point_value) - total_commission_cost\n",
    "\n",
    "            # Print detailed statistics for the ticker\n",
    "            print(\n",
    "                f\"{ticker}: {len(minute_candles_df)} rows, {compression_factor}-Minute Compression Factor, \"\n",
    "                f\"Total PnL: {total_pnl:.2f}pt/${total_dollar_pnl:.2f}, Total trades: {total_trades}, Total Commission Cost: ${total_commission_cost:.2f}, Total Max Gain: {total_max_gain:.2f}pt/${total_max_dollar_gain}, Total Max Loss: {total_max_loss:.2f}pt/${total_max_dollar_loss}, \"\n",
    "                f\"MA PnL: {ma_pnl:.2f}pt/${ma_dollar_pnl:.2f}, MA trades: {ma_trades}, MA Commission Cost: ${ma_commission_total:.2f}, MA Max Gain: {ma_max_gain:.2f}pt/${ma_max_dollar_gain}, MA Max Loss: {ma_max_loss:.2f}pt/${ma_max_dollar_loss}, \"\n",
    "                f\"RSI PnL: {rsi_pnl:.2f}pt/${rsi_dollar_pnl:.2f}, RSI trades: {rsi_trades}, RSI Commission Cost: ${rsi_commission_total:.2f}, RSI Max Gain: {rsi_max_gain:.2f}pt/${rsi_max_dollar_gain}, RSI Max Loss: {rsi_max_loss:.2f}pt/${rsi_max_dollar_loss}, \"\n",
    "                f\"Close Price Difference: {close_price_diff:.2f}pt/${close_price_dollar_diff:.2f}, Alpha: {point_alpha:.2f}pt/${dollar_alpha:.2f}, Tick Size: {tick_size}, \"                \n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing ticker {ticker}: {e}\")\n",
    "\n",
    "    # Print total dollar PnL across all tickers\n",
    "    print('     *!*!*')\n",
    "    print(f\"TOTAL DOLLAR PNL ACROSS ALL TICKERS: {total_dollar_pnl_sum:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize a variable to track if a position is open (market neutral) for function `trade_logic`\n",
    "# position_open = False  # False means market neutral, True means a trade is active\n",
    "# entry_price = None  # Store the entry price when a trade is opened\n",
    "\n",
    "# # Initialize a variable to track if a position is open (market neutral) for function `trade_logic`\n",
    "# sim_position_open = False  # False means market neutral, True means a trade is active\n",
    "# sim_entry_price = None  # Store the entry price when a trade is opened\n",
    "\n",
    "# def trade_logic(candles, buy_order=None, sell_order=None):\n",
    "#     \"\"\"\n",
    "#     Function to decide whether to go long or close positions based on the signal column.\n",
    "\n",
    "#     Parameters:\n",
    "#     - candles (pd.DataFrame): The DataFrame containing minute candle data.\n",
    "#     - buy_order (dict): The buy order configuration (market order).\n",
    "#     - sell_order (dict): The sell order configuration (market order).\n",
    "#     \"\"\"\n",
    "\n",
    "#     global position_open, entry_price  # Ensure access to global variables\n",
    "\n",
    "#     current_hour = datetime.now(pytz.timezone('US/Eastern')).hour\n",
    "#     current_minute = datetime.now(pytz.timezone('US/Eastern')).minute\n",
    "#     current_second = datetime.now(pytz.timezone('US/Eastern')).second\n",
    "\n",
    "#     # Check if it's the start of a new minute (second == 00)\n",
    "#     if current_second == 0 or 1:\n",
    "#         # Ensure there is enough data in candles (at least 2 rows to check the previous signal)\n",
    "#         if len(candles) < 2:\n",
    "#             print(\"Not enough data to make a trade decision.\")\n",
    "#             return\n",
    "\n",
    "#         # Get the signal from the row before the one currently forming\n",
    "#         previous_signal = candles.iloc[-2]['signal']\n",
    "\n",
    "#         if previous_signal == 1:\n",
    "#             # If signal is 1 and no position is open, go long (buy)\n",
    "#             if not position_open:\n",
    "#                 # Place buy order\n",
    "#                 resp = client.order_place(account_hash, buy_order)\n",
    "#                 print(f\"Buy order placed. Response: {resp}\")\n",
    "                \n",
    "#                 # Mark position as open and record entry price\n",
    "#                 position_open = True\n",
    "#                 entry_price = candles.iloc[-2]['close']\n",
    "#                 print(f\"Bought at {entry_price}\")\n",
    "#             else:\n",
    "#                 print(\"Position already open, no action taken.\")\n",
    "\n",
    "#         elif previous_signal == 0 or previous_signal == -1:\n",
    "#             # If signal is 0 or -1 and a position is open, close the position (sell)\n",
    "#             if position_open:\n",
    "#                 # Place sell order\n",
    "#                 resp = client.order_place(account_hash, sell_order)\n",
    "#                 print(f\"Sell order placed. Response: {resp}\")\n",
    "                \n",
    "#                 # Close position\n",
    "#                 position_open = False\n",
    "#                 print(f\"Sold at {candles.iloc[-2]['close']}\")\n",
    "#             else:\n",
    "#                 print(\"No position open, no action taken.\")\n",
    "\n",
    "# def trade_simulator(candles):\n",
    "#     \"\"\"\n",
    "#     Simulated function to decide whether to go long or close positions based on the signal column.\n",
    "\n",
    "#     Parameters:\n",
    "#     - candles (pd.DataFrame): The DataFrame containing minute candle data.\n",
    "\n",
    "#     The function updates the 'sim_signal', 'sim_entry', and 'sim_exit' columns in the DataFrame.\n",
    "#     \"\"\"\n",
    "\n",
    "#     global sim_position_open, sim_entry_price  # Ensure access to global variables\n",
    "\n",
    "#     # Initialize the new columns if they don't exist\n",
    "#     if 'sim_signal' not in candles.columns:\n",
    "#         candles['sim_signal'] = None\n",
    "#     if 'sim_entry' not in candles.columns:\n",
    "#         candles['sim_entry'] = None\n",
    "#     if 'sim_exit' not in candles.columns:\n",
    "#         candles['sim_exit'] = None\n",
    "\n",
    "#     current_hour = datetime.now(pytz.timezone('US/Eastern')).hour\n",
    "#     current_minute = datetime.now(pytz.timezone('US/Eastern')).minute\n",
    "#     current_second = datetime.now(pytz.timezone('US/Eastern')).second\n",
    "\n",
    "#     # Check if it's the start of a new minute (second == 00)\n",
    "#     if current_second == 0 or 1:\n",
    "#         # Ensure there is enough data in candles (at least 2 rows to check the previous signal)\n",
    "#         if len(candles) < 2:\n",
    "#             print(\"Not enough data to make a trade decision.\")\n",
    "#             return\n",
    "\n",
    "#         # Get the signal from the row before the one currently forming\n",
    "#         sim_previous_signal = candles.iloc[-2]['signal']\n",
    "\n",
    "#         if sim_previous_signal == 1:\n",
    "#             # If signal is 1 and no position is open, simulate going long (buy)\n",
    "#             if not sim_position_open:\n",
    "#                 # Simulate buy entry\n",
    "#                 sim_entry_price = candles.iloc[-2]['close']\n",
    "#                 sim_position_open = True\n",
    "                \n",
    "#                 # Record simulated signal and entry price in the DataFrame\n",
    "#                 candles.at[candles.index[-2], 'sim_signal'] = 1\n",
    "#                 candles.at[candles.index[-2], 'sim_entry'] = sim_entry_price\n",
    "\n",
    "#                 print(f\"Simulated buy at {sim_entry_price}\")\n",
    "\n",
    "#             else:\n",
    "#                 print(\"Position already open, no action taken.\")\n",
    "\n",
    "#         elif sim_previous_signal == 0 or sim_previous_signal == -1:\n",
    "#             # If signal is 0 or -1 and a position is open, simulate closing the position (sell)\n",
    "#             if sim_position_open:\n",
    "#                 # Simulate sell exit\n",
    "#                 sim_exit_price = candles.iloc[-2]['close']\n",
    "#                 sim_position_open = False\n",
    "\n",
    "#                 # Record simulated signal and exit price in the DataFrame\n",
    "#                 candles.at[candles.index[-2], 'sim_signal'] = -1\n",
    "#                 candles.at[candles.index[-2], 'sim_exit'] = sim_exit_price\n",
    "\n",
    "#                 print(f\"Simulated sell at {sim_exit_price}\")\n",
    "#             else:\n",
    "#                 print(\"No position open, no action taken.\")\n",
    "\n",
    "# def trade_simulator(candles):\n",
    "#     \"\"\"\n",
    "#     Simulated function to decide whether to go long or close positions based on the signal column.\n",
    "\n",
    "#     Parameters:\n",
    "#     - candles (pd.DataFrame): The DataFrame containing minute candle data.\n",
    "\n",
    "#     The function updates the 'sim_signal' and 'sim_price' columns in the DataFrame.\n",
    "#     \"\"\"\n",
    "\n",
    "#     global sim_position_open, sim_entry_price  # Ensure access to global variables\n",
    "\n",
    "#     current_time = datetime.now(pytz.timezone('US/Eastern'))\n",
    "#     current_hour = datetime.now(pytz.timezone('US/Eastern')).hour    \n",
    "#     current_minute = datetime.now(pytz.timezone('US/Eastern')).minute\n",
    "#     current_second = datetime.now(pytz.timezone('US/Eastern')).second\n",
    "\n",
    "#     # Initialize the new columns if they don't exist\n",
    "#     if 'sim_signal' not in candles.columns:\n",
    "#         candles['sim_signal'] = None\n",
    "#     if 'sim_price' not in candles.columns:\n",
    "#         candles['sim_price'] = None\n",
    "\n",
    "#     # Check if it's the start of a new minute (second == 00)\n",
    "#     if current_time.second == 0:\n",
    "#         # Ensure there is enough data in candles (at least 2 rows to check the previous signal)\n",
    "#         if len(candles) < 2:\n",
    "#             print(\"Not enough data to make a trade decision.\")\n",
    "#             return\n",
    "\n",
    "#         # Get the signal from the row before the one currently forming\n",
    "#         sim_previous_signal = candles.iloc[-2]['signal']\n",
    "\n",
    "#         if sim_previous_signal == 1:\n",
    "#             # If signal is 1 and no position is open, simulate going long (buy)\n",
    "#             if not sim_position_open:\n",
    "#                 # Simulate buy entry\n",
    "#                 sim_entry_price = candles.iloc[-2]['close']\n",
    "#                 sim_position_open = True\n",
    "                \n",
    "#                 # Record simulated signal and entry price in the DataFrame\n",
    "#                 candles.at[candles.index[-2], 'sim_signal'] = 1\n",
    "#                 candles.at[candles.index[-2], 'sim_price'] = sim_entry_price\n",
    "\n",
    "#                 print(f\"Simulated buy at {sim_entry_price}\")\n",
    "\n",
    "#             else:\n",
    "#                 print(\"Position already open, no action taken.\")\n",
    "\n",
    "#         elif sim_previous_signal == 0 or sim_previous_signal == -1:\n",
    "#             # If signal is 0 or -1 and a position is open, simulate closing the position (sell)\n",
    "#             if sim_position_open:\n",
    "#                 # Simulate sell exit\n",
    "#                 sim_exit_price = candles.iloc[-2]['close']\n",
    "#                 sim_position_open = False\n",
    "\n",
    "#                 # Record simulated signal and exit price in the DataFrame\n",
    "#                 candles.at[candles.index[-2], 'sim_signal'] = -1\n",
    "#                 candles.at[candles.index[-2], 'sim_price'] = sim_exit_price\n",
    "\n",
    "#                 print(f\"Simulated sell at {sim_exit_price}\")\n",
    "#             else:\n",
    "#                 print(\"No position open, no action taken.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"color:red;\"><b>The Data Handler<b></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8C2363FAEB6002C9B8323540563E17BD7076A5C19E1F2C524AD4B43649C726A7\n"
     ]
    }
   ],
   "source": [
    "linked_accounts = client.account_linked().json()\n",
    "linked_accounts\n",
    "\n",
    "account_hash = linked_accounts[0].get('hashValue')\n",
    "print(account_hash)\n",
    "\n",
    "# equity_ticker = 'MARA'\n",
    "futures_ticker = '/NQ'\n",
    "\n",
    "buy_order = {\"orderType\": \"MARKET\",\n",
    "        \"session\": \"NORMAL\",\n",
    "        \"duration\": \"DAY\",\n",
    "        \"orderStrategyType\": \"SINGLE\",\n",
    "        \"orderLegCollection\": [\n",
    "            {\n",
    "                \"instruction\": \"BUY\",\n",
    "                \"quantity\": 1,\n",
    "                \"instrument\": {\n",
    "                    \"symbol\": futures_ticker,\n",
    "                    \"assetType\": \"EQUITY\"\n",
    "                }\n",
    "            }\n",
    "            ]\n",
    "        }  \n",
    "\n",
    "sell_order = {\"orderType\": \"MARKET\",\n",
    "        \"session\": \"NORMAL\",\n",
    "        \"duration\": \"DAY\",\n",
    "        \"orderStrategyType\": \"SINGLE\",\n",
    "        \"orderLegCollection\": [\n",
    "            {\n",
    "                \"instruction\": \"SELL\",\n",
    "                \"quantity\": 1,\n",
    "                \"instrument\": {\n",
    "                    \"symbol\": futures_ticker,\n",
    "                    \"assetType\": \"EQUITY\"\n",
    "                }\n",
    "            }\n",
    "            ]\n",
    "        }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = client.stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to handle and process incoming data. This is where everything from extraction, raw stream storage, candle compression, indicator calculation, trade logic execution, and profit calculation comes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_periods=[3, 5, 7, 9]\n",
    "sma_periods = ma_periods\n",
    "wma_periods = ma_periods\n",
    "rsi_periods = ma_periods\n",
    "\n",
    "# Dynamically create ma_combinations\n",
    "ma_combinations = [\n",
    "    (f'wma_{period}', f'sma_{period}', f'rsi_{period}') for period in ma_periods\n",
    "]\n",
    "\n",
    "def handle_data(message):\n",
    "    global minute_candles\n",
    "\n",
    "    current_time_USE = datetime.now(pytz.timezone('US/Eastern')).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    current_hour = datetime.now(pytz.timezone('US/Eastern')).hour\n",
    "    current_minute = datetime.now(pytz.timezone('US/Eastern')).minute\n",
    "    current_second = datetime.now(pytz.timezone('US/Eastern')).second\n",
    "\n",
    "    # Stop the streamer at 16:59:55\n",
    "    if current_hour == 16 and current_minute == 59 and current_second >= 55:\n",
    "        print(f\"{current_time_USE.strftime('%Y-%m-%d %H:%M:%S')} - Stopping the streamer.\")\n",
    "        streamer.stop(clear_subscriptions=True)\n",
    "        return  # Exit the function immediately after stopping the streamer\n",
    "\n",
    "    # Try parsing the incoming message\n",
    "    try: # Indent the code and uncomment the try-except block to revert\n",
    "        data = json.loads(message)\n",
    "\n",
    "        # Call the function to process the data and update todays_price_action\n",
    "        process_message_data(data)\n",
    "\n",
    "        # **Update the minute candles in real-time**\n",
    "        update_minute_candles(ticker_tables, time_frame='min')\n",
    "\n",
    "        # # Iterate through all the tickers in ticker_tables\n",
    "        # for ticker, df in minute_candles.items():\n",
    "        #     # Calculate moving averages and indicators for each DataFrame\n",
    "        #     minute_candles[ticker] = calculate_indicators(\n",
    "        #         df, \n",
    "        #         price_col='close', \n",
    "        #         acc_vol_col='accumulative_volume', \n",
    "        #         sma_periods=sma_periods,\n",
    "        #         wma_periods=wma_periods,\n",
    "        #         rsi_periods=rsi_periods,\n",
    "        #         candle_window=10\n",
    "        #     )\n",
    "        \n",
    "        # # ANY TRADE STRATEGY LOGIC IS TO GO BETWEEN THE LINES OF ASTERISKS\n",
    "        # # *********************************************************************************************************************************************************************************\n",
    "        # # Iterate through all the tickers in the current dictionary\n",
    "        # for ticker, df in minute_candles.items():\n",
    "            \n",
    "        #     # Iterate through all the ma_combinations\n",
    "        #     for sig_ma, con_ma, rsi_col in ma_combinations:\n",
    "        #         # Generate trading signals\n",
    "        #         minute_candles[ticker] = generate_trading_signals(\n",
    "        #             df,\n",
    "        #             ma_name1=sig_ma,\n",
    "        #             ma_name2=con_ma,\n",
    "        #             rsi_column=rsi_col\n",
    "        #         )\n",
    "\n",
    "        #         # Update position_open columns to be 1:1 verbal boolean with the signal\n",
    "        #         minute_candles[ticker] = update_position_open(\n",
    "        #             df,\n",
    "        #             ma_name1=sig_ma,\n",
    "        #             ma_name2=con_ma,\n",
    "        #             rsi_column=rsi_col\n",
    "        #         )\n",
    "\n",
    "        #         # Determine entry prices for each ticker\n",
    "        #         minute_candles[ticker] = determine_entry_prices(\n",
    "        #             df,\n",
    "        #             ma_name1=sig_ma,\n",
    "        #             ma_name2=con_ma,\n",
    "        #             rsi_column=rsi_col,\n",
    "        #             ticker_to_tick_size=ticker_to_tick_size,\n",
    "        #             ticker=ticker\n",
    "        #         )\n",
    "\n",
    "        #         # Determine exit prices for each ticker\n",
    "        #         minute_candles[ticker] = determine_exit_prices(\n",
    "        #             df,\n",
    "        #             ma_name1=sig_ma,\n",
    "        #             ma_name2=con_ma,\n",
    "        #             rsi_column=rsi_col,\n",
    "        #             ticker_to_tick_size=ticker_to_tick_size,\n",
    "        #             ticker=ticker\n",
    "        #         )\n",
    "\n",
    "        #         # Stop loss calculation\n",
    "        #         minute_candles[ticker] = calculate_stop_losses(\n",
    "        #             df,\n",
    "        #             ma_name1=sig_ma,\n",
    "        #             ma_name2=con_ma,\n",
    "        #             rsi_column=rsi_col\n",
    "        #         )\n",
    "\n",
    "        #         # Track stop loss hits\n",
    "        #         minute_candles[ticker] = track_stop_loss_hits(\n",
    "        #             df,\n",
    "        #             ma_name1=sig_ma,\n",
    "        #             ma_name2=con_ma,\n",
    "        #             rsi_column=rsi_col,\n",
    "        #             ticker_to_tick_size=ticker_to_tick_size,\n",
    "        #             ticker=ticker\n",
    "        #         )\n",
    "\n",
    "        #         # Adjust signals from stop loss hits\n",
    "        #         minute_candles[ticker] = adjust_signals_for_stop_loss(\n",
    "        #             df,\n",
    "        #             ma_name1=sig_ma,\n",
    "        #             ma_name2=con_ma,\n",
    "        #             rsi_column=rsi_col\n",
    "        #         )\n",
    "\n",
    "        #         # Re-update position_open column after stop loss hits\n",
    "        #         minute_candles[ticker] = update_position_open(\n",
    "        #             df,\n",
    "        #             ma_name1=sig_ma,\n",
    "        #             ma_name2=con_ma,\n",
    "        #             rsi_column=rsi_col\n",
    "        #         )\n",
    "\n",
    "        #         # Re-determine entry prices after stop loss hits\n",
    "        #         minute_candles[ticker] = determine_entry_prices(\n",
    "        #             df,\n",
    "        #             ma_name1=sig_ma,\n",
    "        #             ma_name2=con_ma,\n",
    "        #             rsi_column=rsi_col,\n",
    "        #             ticker_to_tick_size=ticker_to_tick_size,\n",
    "        #             ticker=ticker\n",
    "        #         )\n",
    "\n",
    "        #         # Re-determine exit prices after stop loss hits\n",
    "        #         minute_candles[ticker] = determine_exit_prices(\n",
    "        #             df,\n",
    "        #             ma_name1=sig_ma,\n",
    "        #             ma_name2=con_ma,\n",
    "        #             rsi_column=rsi_col,\n",
    "        #             ticker_to_tick_size=ticker_to_tick_size,\n",
    "        #             ticker=ticker\n",
    "        #         )\n",
    "\n",
    "        #         # Update stop loss levels after stop loss hits\n",
    "        #         minute_candles[ticker] = update_stop_loss(\n",
    "        #             df,\n",
    "        #             ma_name1=sig_ma,\n",
    "        #             ma_name2=con_ma,\n",
    "        #             rsi_column=rsi_col\n",
    "        #         )\n",
    "\n",
    "        #         # Calculate profit/loss for each ticker's DataFrame\n",
    "        #         minute_candles[ticker] = calculate_profit_loss(\n",
    "        #             df,\n",
    "        #             contract_multiplier=1,\n",
    "        #             ma_name1=sig_ma,\n",
    "        #             ma_name2=con_ma,\n",
    "        #             rsi_column=rsi_col\n",
    "        #         )\n",
    "        # *********************************************************************************************************************************************************************************        \n",
    "        # ANY TRADE STRATEGY LOGIC IS TO GO BETWEEN THE LINES OF ASTERISKS\n",
    "\n",
    "        # if current_second == 59 or 00 or 1 or 2:\n",
    "        #     # Call the trade_logic function to execute trades based on the signal\n",
    "        #     trade_logic(minute_candles, buy_order=buy_order, sell_order=sell_order)\n",
    "\n",
    "        # if current_second == 59 or 00 or 1 or 2:\n",
    "        #     # Call the trade_simulator function to simulate trades based on the signal\n",
    "        #     trade_simulator(minute_candles)\n",
    "\n",
    "            # print(message)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{current_time_USE}: Error processing message: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppress all warnings to avoid too much printing under the streamer cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the streamer and send the level one futures subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer.start(handle_data)\n",
    "streamer.send(streamer.level_one_futures(tickers, \"0,1,2,3,4,5,8,9,12,13,14,18,19,20,23,25,26,35,37,38\")) # For futures\n",
    "# streamer.send(streamer.level_one_equities(tickers, \"0,1,2,3,4,5,8,9,10,11,12,17,18,19,20,21,42\")) # For equities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{6**7:,}\")\n",
    "print(f\"{11**5:,}\")\n",
    "print(f\"{(6**9)/(11**5):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect data from the current session. Check current time to see if the streamer has been cut off. Restart the streamer if it has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>accumulative_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-12-31 13:46:00-05:00</td>\n",
       "      <td>/NQ</td>\n",
       "      <td>25,617.5000</td>\n",
       "      <td>25,617.5000</td>\n",
       "      <td>25,615.0000</td>\n",
       "      <td>25,615.5000</td>\n",
       "      <td>211852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-12-31 13:47:00-05:00</td>\n",
       "      <td>/NQ</td>\n",
       "      <td>25,615.5000</td>\n",
       "      <td>25,624.2500</td>\n",
       "      <td>25,615.0000</td>\n",
       "      <td>25,621.7500</td>\n",
       "      <td>212216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   datetime ticker        open        high         low       close accumulative_volume\n",
       "0 2025-12-31 13:46:00-05:00    /NQ 25,617.5000 25,617.5000 25,615.0000 25,615.5000              211852\n",
       "1 2025-12-31 13:47:00-05:00    /NQ 25,615.5000 25,624.2500 25,615.0000 25,621.7500              212216"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "key = 1\n",
    "\n",
    "tickers_list = list(ticker_tables.keys())\n",
    "# display(minute_candles)\n",
    "# display(tickers_list)\n",
    "# display(ticker_tables[tickers_list[key]])\n",
    "display(minute_candles[tickers_list[key]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer.stop(clear_subscriptions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append new data to the csv storage files that contain the raw, manually collected OHLC data from Schwab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dfs_to_csv(candles, save_path=\"raw_futures_data/\", columns_to_save=None):\n",
    "    \"\"\" After specifying column data types\n",
    "    Save each DataFrame in the dictionary to a separate CSV file, appending to the CSV\n",
    "    if the file already exists, saving only specified columns with enforced data types.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (dict): A dictionary where keys are names and values are DataFrames.\n",
    "    - save_path (str): The directory where the CSV files will be saved. Default is 'dataframes/'.\n",
    "    - columns_to_save (list): A list of columns to save from each DataFrame. Default is None (save all columns).\n",
    "    \"\"\"\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    # Define column data types\n",
    "    dtype_conversion = {\n",
    "        'open': 'float64',\n",
    "        'high': 'float64',\n",
    "        'low': 'float64',\n",
    "        'close': 'float64',\n",
    "        'accumulative_volume': 'float64'\n",
    "    }\n",
    "\n",
    "    # Save each DataFrame\n",
    "    for key, df in candles.items():\n",
    "        file_name = os.path.join(save_path, f\"{key.replace(\"/\", \"\")}.csv\")\n",
    "\n",
    "        # Filter the DataFrame to only the specified columns, if provided\n",
    "        if columns_to_save:\n",
    "            df_to_save = df[columns_to_save].copy()\n",
    "        else:\n",
    "            df_to_save = df.copy()\n",
    "\n",
    "        # Ensure correct data types for numeric columns\n",
    "        for col, dtype in dtype_conversion.items():\n",
    "            if col in df_to_save.columns:\n",
    "                df_to_save[col] = df_to_save[col].astype(dtype, errors='ignore')\n",
    "\n",
    "        # Save the DataFrame\n",
    "        if os.path.exists(file_name):\n",
    "            df_to_save.to_csv(file_name, mode='a', header=False, index=False)\n",
    "            print(f\"Appended to: {file_name}\")\n",
    "        else:\n",
    "            df_to_save.to_csv(file_name, index=False)\n",
    "            print(f\"Saved new file: {file_name}\")\n",
    "\n",
    "columns_to_save = ['datetime', 'ticker', 'open', 'high', 'low', 'close', 'accumulative_volume']\n",
    "save_dfs_to_csv(minute_candles, save_path=\"raw_futures_data/\", columns_to_save=columns_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 style=\"color:yellow;\">Less Frequently Used Code in this Section</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ticker_tables\n",
    "ticker_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del minute_candles\n",
    "minute_candles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(a, b, c):\n",
    "    print(a, b, c)\n",
    "my_list = [1, 2, 3]\n",
    "func(*my_list)  # Unpack the list into the function arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information on futures pricing and things to do next for the app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Think about the following:__\n",
    "- Using higher highs and lower lows to determine the trend\n",
    "- Switching between doing nothing and trading the strategy based on higher highs/lows and lower highs/lows\n",
    "- 0 and 100 RSI levels to intervene in the strategy when taking profit or entering a trade\n",
    "- Think about running simultaneous strategies on different timeframes or using different indicators\n",
    "- Mostly finding alpha (can we do so by simulation or backtesting)\n",
    "- Learn more about trading futures on Schwab\n",
    "- Try and approximate the perfect second derivative work\n",
    "- Find a way to make this all work trading both long and short trades\n",
    "- Write data directly to cloud or json and call it back for visualization\n",
    "- Factor down code and create a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Futures Point Values\n",
    "- MES Micro E-mini S&P 500:    $5/pt\n",
    "- MNQ Micro E-mini NASDAQ-100: $2/pt\n",
    "- MCL Micro Crude Oil:         $100/pt\n",
    "- MGC E-Micro Gold:            $10/pt\n",
    "- ES E-Mini S&P 500:           $50/pt\n",
    "- NQ E-Mini NASDAQ 100:        $20/pt\n",
    "- CL Crude Oil:                $1000/pt\n",
    "- GC Gold:                     $100/pt\n",
    "- NG Natural Gas               $10000/pt\n",
    "- ZN 10-Year T-Note            $1000/pt\n",
    "- ZF 5-Year T-Note             $1000/pt\n",
    "- ZT 2-Year T-Note             $2000/pt\n",
    "\n",
    "- tickers = \"/ES,/NQ,/CL,/GC,/NG,/SR3,/ZN,/ZF,/ZT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the automatic visualizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually Inspect minute_candles and ticker_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize candles, indicators, entries/exits, stop losses, and pnl curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minute_candles_vizualize = minute_candles.copy()\n",
    "minute_candles_vizualize['/ES']\n",
    "\n",
    "window_size = 5\n",
    "\n",
    "sig_ma = f'wma_{window_size}'\n",
    "con_ma = f'sma_{window_size}'\n",
    "rsi_col = f'rsi_{window_size}'\n",
    "\n",
    "visualize_trades(\n",
    "    candles=minute_candles_vizualize, \n",
    "    ticker_to_tick_size=ticker_to_tick_size,\n",
    "    ticker_to_point_value=ticker_to_point_value,\n",
    "    ma_name1=sig_ma, \n",
    "    ma_name2=con_ma, \n",
    "    rsi_column=rsi_col, \n",
    "    lower_slice=-100, \n",
    "    upper_slice=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_visualization()\n",
    "\n",
    "# separate entry function from exit function\n",
    "# add open interest to minute_candles and figure out a way to use it to determine trend strength\n",
    "# assess how/when this strategy loses and how to rectify it- stop loss to prevent loss from heavy down trends ****\n",
    "# figure out how to use the envelope to determine which version of the strategy to use, regular or inverse\n",
    "# largest/average running candle size as stop loss, don't get back in till next signal\n",
    "# we're getting caught in down trends and getting out of up trends- need to fix this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize a variable to hold the task reference\n",
    "# visualization_task = None  # This ensures visualization_task is defined\n",
    "\n",
    "# async def update_visualization():\n",
    "#     while True:\n",
    "#         # Clear the previous output\n",
    "#         clear_output(wait=True)\n",
    "       \n",
    "#         # Call the visualization function\n",
    "#         visualize_trades(lower_slice=0, upper_slice=-1)\n",
    "        \n",
    "#         # Wait for 5 seconds before repeating\n",
    "#         await asyncio.sleep(60)\n",
    "\n",
    "# def start_visualization():\n",
    "#     global visualization_task\n",
    "#     # Start the update_visualization function if it's not already running\n",
    "#     if visualization_task is None:\n",
    "#         loop = asyncio.get_event_loop()\n",
    "#         visualization_task = loop.create_task(update_visualization())\n",
    "\n",
    "# def stop_visualization():\n",
    "#     global visualization_task\n",
    "#     # Stop the update_visualization function if it's running\n",
    "#     if visualization_task is not None:\n",
    "#         visualization_task.cancel()  # Cancel the task\n",
    "#         visualization_task = None\n",
    "\n",
    "# # Start the visualization\n",
    "# start_visualization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print pnl and point alpha figures for each ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the updated print_all_pnls function\n",
    "print_all_pnls(\n",
    "    candles=minute_candles,  # Pass the dictionary of DataFrames\n",
    "    compression_factor=1,  # Pass the extracted compression factor\n",
    "    ticker_to_tick_size=ticker_to_tick_size,  # Pass tick size mapping\n",
    "    ticker_to_point_value=ticker_to_point_value,  # Pass point value mapping\n",
    "    ma_name1='wma_5',\n",
    "    ma_name2='sma_5',\n",
    "    rsi_column='rsi_5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in minute_candles.keys():\n",
    "#     minute_candles[key] = remove_zero_close_1(minute_candles[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`percentage_candles_containing_previous_close` does just what the title of the function describes. Need to apply this to every trading session and draw up some analytics to describe the phenomenon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_candles_containing_previous_close(candles):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of candles where the current candle's high and low\n",
    "    contain the previous candle's close price.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): A DataFrame containing 'high', 'low', and 'close' columns.\n",
    "\n",
    "    Returns:\n",
    "    - float: The percentage of candles containing the previous candle's close price.\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrame contains necessary columns\n",
    "    if not {'high', 'low', 'close'}.issubset(candles.columns):\n",
    "        raise ValueError(\"The DataFrame must contain 'high', 'low', and 'close' columns.\")\n",
    "\n",
    "    # Shift the close price to align it with the next candle\n",
    "    previous_close = candles['close'].shift(1)\n",
    "\n",
    "    # Check if the previous close is within the current candle's range\n",
    "    contains_previous_close = (candles['low'] <= previous_close) & (candles['high'] >= previous_close)\n",
    "\n",
    "    # Calculate percentage\n",
    "    percentage = contains_previous_close.mean() * 100\n",
    "\n",
    "    return round(percentage, 2)\n",
    "\n",
    "# Calculate the percentage for the /ES DataFrame\n",
    "percentage_candles_containing_previous_close(minute_candles['/CL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"color:red;\"><b>Research Branch</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another branch, all mashed together. Pulling stop loss functionality from the rest of things. Compressing candles further by some chosen factor to see the effect on profit using the same trading rules on each time frame. Vectorizing each function's operations to replace inefficient itterows implementation. Refer to the directly following markdown cells for guidance in improving the accuracy and performance of this iteration of the simulator logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI prompt asking how to make this project better.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any ways that you would refactor these functions to become more effectively sequential?\n",
    "\n",
    "To make these functions more sequential and cohesive, you could refactor them as follows:\n",
    "\n",
    "1. **Consolidate Shared Logic**: Combine common logic across functions (e.g., handling signals, positions, or stop losses) into smaller, reusable helper functions. This reduces redundancy and makes the flow clearer.\n",
    "\n",
    "2. **Pipeline Approach**: Create a clear processing pipeline that sequentially applies each function to the DataFrame. Each step (e.g., signal generation, entry/exit determination, stop loss tracking, profit/loss calculation) should be modular and explicitly linked.\n",
    "\n",
    "3. **Avoid Iterrows**: Replace `.iterrows()` loops with vectorized operations wherever possible to enhance performance and simplify logic.\n",
    "\n",
    "4. **State Management**: Use a single, centralized state management approach for tracking open positions, stop losses, and entry/exit prices. This avoids redundant logic for managing `ma_position_open` or `rsi_position_open` in multiple places.\n",
    "\n",
    "5. **Simplify Parameter Passing**: Pass only the necessary columns and settings into each function. For example, instead of passing the entire DataFrame, pass specific slices or columns.\n",
    "\n",
    "6. **Dynamic Configuration**: Use configuration objects or dictionaries for strategy settings (e.g., moving average names, RSI thresholds). This reduces the number of parameters and makes it easier to adapt to new strategies.\n",
    "\n",
    "7. **Validation and Error Handling**: Implement consistent input validation and error handling across all functions to prevent silent failures or misaligned assumptions.\n",
    "\n",
    "8. **Output Standardization**: Ensure all functions return consistently updated DataFrames, so they can be chained seamlessly.\n",
    "\n",
    "9. **Logging and Debugging**: Add optional logging or debugging output at key points to trace the execution flow and verify correctness during development.\n",
    "\n",
    "10. **Testing Framework**: Isolate logic into testable units and write test cases to ensure individual components behave as expected in isolation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which three of these do you see as the most important?\n",
    "\n",
    "The three most important refactoring priorities are:\n",
    "\n",
    "1. **Avoid Iterrows**: Replacing `.iterrows()` with vectorized operations is crucial for improving performance and scalability. Iterating over rows is slow and inefficient, especially for large datasets, which is often the case in financial trading applications.\n",
    "\n",
    "2. **Pipeline Approach**: Structuring the functions as a sequential pipeline ensures clarity and maintainability. Each step of the process should build upon the previous one, making it easy to debug, modify, or extend the workflow.\n",
    "\n",
    "3. **Consolidate Shared Logic**: Reducing redundancy by creating reusable helper functions makes the codebase more concise and easier to maintain. This change ensures consistency across different parts of the logic, reducing the risk of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = \"/ES,/NQ,/CL,/GC\"\n",
    "# tickers = \"MARA,PLUG,SOFI,SWN,RKLB\"\n",
    "\n",
    "ticker_to_point_value = {\n",
    "    \"/ES\": 50,       # E-Mini S&P 500\n",
    "    \"/NQ\": 20,       # E-Mini NASDAQ 100\n",
    "    \"/CL\": 1000,     # Crude Oil\n",
    "    \"/GC\": 100,      # Gold\n",
    "}\n",
    "\n",
    "ticker_to_tick_size = {\n",
    "    \"/ES\": 0.25,        # E-Mini S&P 500\n",
    "    \"/NQ\": 0.25,        # E-Mini NASDAQ 100\n",
    "    \"/CL\": 0.01,        # Crude Oil\n",
    "    \"/GC\": 0.1,         # Gold\n",
    "}\n",
    "\n",
    "tick_size_ES = ticker_to_tick_size.get(\"/ES\")\n",
    "tick_size_ES\n",
    "\n",
    "ma_periods = [1, 3, 5]\n",
    "sma_periods = ma_periods\n",
    "wma_periods = ma_periods\n",
    "rsi_periods = ma_periods\n",
    "\n",
    "ma_combinations = [\n",
    "    (f'wma_{wma}', f'sma_{sma}', f'rsi_{wma}')\n",
    "    for wma in wma_periods\n",
    "    for sma in sma_periods\n",
    "    if wma <= sma\n",
    "]\n",
    "\n",
    "compression_factors = range(1, 6, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 style=\"color:yellow;\">Research Functions</h2>\n",
    "- These are modifications of the base functions above that iterate over many more parameters to find the effect on profit of each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for pulling the whole of the manually collected data and working with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dfs_from_csv(load_path=\"raw_futures_data/\", columns_to_load=None):\n",
    "    \"\"\"\n",
    "    Load CSV files from a directory into a dictionary of DataFrames, \n",
    "    ensuring correct data types for specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - load_path (str): The directory from which the CSV files will be loaded.\n",
    "    - columns_to_load (list): A list of columns to load from each CSV file. Default is None (load all columns).\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are file names (prefixed with '/') and values are DataFrames.\n",
    "    \"\"\"\n",
    "    candles = {}\n",
    "    \n",
    "    # Define column data types\n",
    "    dtype_conversion = {\n",
    "        'open': 'float64',\n",
    "        'high': 'float64',\n",
    "        'low': 'float64',\n",
    "        'close': 'float64',\n",
    "        'accumulative_volume': 'float64'\n",
    "    }\n",
    "\n",
    "    # Load each CSV file\n",
    "    for file_name in os.listdir(load_path):\n",
    "        if file_name.endswith('.csv'):  # Only process CSV files\n",
    "            df_name = '/' + file_name.replace('.csv', '')  # Add '/' to the name\n",
    "            file_path = os.path.join(load_path, file_name)\n",
    "            \n",
    "            try:\n",
    "                # Load the DataFrame with specified columns\n",
    "                df = pd.read_csv(file_path, usecols=columns_to_load, parse_dates=['datetime'], on_bad_lines='skip')\n",
    "\n",
    "                # Ensure correct data types for numeric columns\n",
    "                for col, dtype in dtype_conversion.items():\n",
    "                    if col in df.columns:\n",
    "                        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "                # Filter out rows where 'close' is 0, NaN, or None\n",
    "                if 'close' in df.columns:\n",
    "                    df = df[(df['close'] != 0) & df['close'].notna()]\n",
    "\n",
    "                # Reset index for a numerical index\n",
    "                df.reset_index(drop=True, inplace=True)\n",
    "                \n",
    "                candles[df_name] = df  # Add DataFrame to the dictionary\n",
    "                print(f\"Loaded: {file_path}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_name}: {e}\")\n",
    "    print(f'Minute_candles dictionary rebuilt')\n",
    "    return candles\n",
    "\n",
    "def check_column_data_types(df_dict):\n",
    "    \"\"\"\n",
    "    Check and display the data types of all columns for each DataFrame in the dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - df_dict (dict): A dictionary where keys are names and values are DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary with DataFrame names as keys and column data types as values.\n",
    "    \"\"\"\n",
    "    data_types = {}\n",
    "    \n",
    "    for key, df in df_dict.items():\n",
    "        if not df.empty:\n",
    "            # Store the column data types for each DataFrame\n",
    "            data_types[key] = df.dtypes.to_dict()\n",
    "            print(f\"\\nData types for {key}:\")\n",
    "            print(df.dtypes)\n",
    "        else:\n",
    "            print(f\"\\n{key} is empty.\")\n",
    "            data_types[key] = None\n",
    "\n",
    "    return data_types\n",
    "\n",
    "def get_shared_unique_dates(candles, string_format=False):\n",
    "    \"\"\"\n",
    "    Returns a list of unique dates shared across all DataFrames in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        candles (dict): A dictionary where keys are tickers and values are DataFrames containing a 'datetime' column.\n",
    "        string_format (bool): If True, return dates as 'YYYY-MM-DD' strings.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique dates shared by all DataFrames in the dictionary.\n",
    "    \"\"\"\n",
    "    shared_dates = None\n",
    "    candles_copy = copy.deepcopy(candles)  # Deep copy ensures DataFrames aren't linked\n",
    "\n",
    "    for df in candles_copy.values():\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'], utc=True).dt.tz_convert(None)\n",
    "        unique_dates = set(df['datetime'].dt.date)\n",
    "\n",
    "        if shared_dates is None:\n",
    "            shared_dates = unique_dates\n",
    "        else:\n",
    "            shared_dates &= unique_dates\n",
    "\n",
    "    if string_format:\n",
    "        return sorted(date.isoformat() for date in shared_dates)\n",
    "    else:\n",
    "        return sorted(shared_dates)\n",
    "    \n",
    "def filter_columns_in_dict(candles, columns_to_keep):\n",
    "    \"\"\"\n",
    "    Remove all columns except the specified ones from each DataFrame in a dictionary,\n",
    "    and reset the index of each filtered DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df_dict (dict): A dictionary where keys are DataFrame names and values are DataFrames.\n",
    "    - columns_to_keep (list): List of columns to keep in each DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: The updated dictionary with filtered DataFrames.\n",
    "    \"\"\"\n",
    "    for key, df in candles.items():\n",
    "        # Keep only the specified columns\n",
    "        df_filtered = df[columns_to_keep].copy()\n",
    "        # Reset the index\n",
    "        candles[key] = df_filtered.reset_index(drop=True)\n",
    "        print(f\"Filtered columns for {key}. Remaining columns: {candles[key].columns.tolist()}\")\n",
    "        \n",
    "    return candles\n",
    "\n",
    "def percentage_candles_containing_previous_close_1(candles):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of candles where the current candle's high and low\n",
    "    contain the previous candle's close price.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): A DataFrame containing 'high', 'low', and 'close' columns.\n",
    "\n",
    "    Returns:\n",
    "    - float: The percentage of candles containing the previous candle's close price.\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrame contains necessary columns\n",
    "    if not {'high', 'low', 'close'}.issubset(candles.columns):\n",
    "        raise ValueError(\"The DataFrame must contain 'high', 'low', and 'close' columns.\")\n",
    "\n",
    "    # Shift the close price to align it with the next candle\n",
    "    previous_close = candles['close'].shift(1)\n",
    "\n",
    "    # Check if the previous close is within the current candle's range\n",
    "    contains_previous_close = (candles['low'] <= previous_close) & (candles['high'] >= previous_close)\n",
    "\n",
    "    # Calculate percentage\n",
    "    percentage = contains_previous_close.mean() * 100\n",
    "\n",
    "    return round(percentage, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For candle compression and base OHLCV column filtering, some cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_candles(candles, n):\n",
    "    \"\"\"\n",
    "    Compress candles for a given timeframe across a DataFrame.\n",
    "    Preserves time-derived columns if they exist.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): DataFrame containing the candle data.\n",
    "    - n (int): Number of minutes (rows) to compress into one candle.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A compressed DataFrame.\n",
    "    \"\"\"\n",
    "    if not isinstance(candles, pd.DataFrame) or n <= 0:\n",
    "        raise ValueError(\"Input must be a DataFrame, and 'n' must be a positive integer.\")\n",
    "\n",
    "    # Ensure sorted\n",
    "    candles = candles.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "    # Group every n rows\n",
    "    grouped = candles.groupby(candles.index // n)\n",
    "\n",
    "    # Columns to always aggregate\n",
    "    agg_dict = {\n",
    "        'datetime': 'first',\n",
    "        'ticker': 'first',\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last',\n",
    "        'accumulative_volume': 'sum'\n",
    "    }\n",
    "\n",
    "    # Optionally preserve time-derived columns\n",
    "    for col in ['year', 'month', 'week', 'day', 'weekday', 'hour', 'minute']:\n",
    "        if col in candles.columns:\n",
    "            agg_dict[col] = 'first'\n",
    "\n",
    "    compressed = grouped.agg(agg_dict).reset_index(drop=True)\n",
    "\n",
    "    return compressed\n",
    "\n",
    "def remove_zero_close(df):\n",
    "    \"\"\"\n",
    "    Removes all rows where any of the OHLC price metrics ('open', 'high', 'low', 'close') are below zero \n",
    "    or contain null (NaN) values using a row drop method.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): A DataFrame containing OHLC price metrics.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated DataFrame with rows filtered out where any OHLC metric is below zero or null.\n",
    "    \"\"\"\n",
    "    # Identify rows to drop based on conditions\n",
    "    rows_to_drop = df[\n",
    "        (df['close'] < 0) | (df['open'] < 0) | (df['high'] < 0) | (df['low'] < 0) | (df['accumulative_volume'] < 0) |\n",
    "        df['close'].isna() | df['open'].isna() | df['high'].isna() | df['low'].isna() | df['accumulative_volume'].isna()\n",
    "    ].index\n",
    "\n",
    "    # Drop the identified rows\n",
    "    df = df.drop(index=rows_to_drop).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    \"\"\"\n",
    "    Removes duplicate rows from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): A DataFrame to process.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated DataFrame with duplicate rows removed.\n",
    "    \"\"\"\n",
    "    return df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "def remove_5pm_hour_rows(df, time_column='datetime'):\n",
    "    \"\"\"\n",
    "    Remove all rows in the 17:00 to 17:59 Eastern Time window across all dates,\n",
    "    and add time-based columns including timezone offset and abbreviation.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame with datetime column\n",
    "    - time_column (str): Name of the datetime column\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered DataFrame with 5PM ET rows removed and new time columns added\n",
    "    \"\"\"\n",
    "    # Ensure datetime is UTC and timezone-aware\n",
    "    df['eastern_time'] = pd.to_datetime(df[time_column], utc=True)\n",
    "    df['eastern_time'] = df['eastern_time'].dt.tz_convert('US/Eastern')\n",
    "\n",
    "    # Extract time features\n",
    "    df['year'] = df['eastern_time'].dt.year\n",
    "    df['month'] = df['eastern_time'].dt.month\n",
    "    df['week'] = df['eastern_time'].dt.isocalendar().week\n",
    "    df['day'] = df['eastern_time'].dt.day\n",
    "    df['month_name'] = df['eastern_time'].dt.month_name()\n",
    "    df['weekday'] = df['eastern_time'].dt.day_name()\n",
    "    df['hour'] = df['eastern_time'].dt.hour\n",
    "    df['minute'] = df['eastern_time'].dt.minute\n",
    "\n",
    "    # Extract timezone offset and name\n",
    "    df['timezone'] = df['eastern_time'].astype(str).str.extract(r'([-+]\\d{2}:\\d{2})$')[0]\n",
    "    df['timezone_name'] = df['timezone'].map({'-04:00': 'EDT', '-05:00': 'EST'}).fillna('Unknown')\n",
    "\n",
    "    # Filter out rows where hour == 17 (5 PM ET)\n",
    "    df = df[df['hour'] != 17].copy()\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_price_action_envelope_1(df, high_col='high', low_col='low', base_window=5, adaptive_window=3):\n",
    "    \"\"\"\n",
    "    Calculate an adaptive price action envelope by detecting higher highs, lower lows,\n",
    "    and dynamically adjusting the envelope based on market trend conditions.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the price data.\n",
    "    high_col (str): The column name for the high prices (default 'high').\n",
    "    low_col (str): The column name for the low prices (default 'low').\n",
    "    base_window (int): The standard window size for detecting local highs and lows.\n",
    "    adaptive_window (int): The smaller window size for adaptive adjustments.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with added columns for the adaptive price action envelope.\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to count upward movements in a rolling window\n",
    "    def count_increases(series, window):\n",
    "        return series.diff().gt(0).rolling(window=window, min_periods=1).sum()\n",
    "\n",
    "    # Function to count downward movements in a rolling window\n",
    "    def count_decreases(series, window):\n",
    "        return series.diff().lt(0).rolling(window=window, min_periods=1).sum()\n",
    "\n",
    "    # Count the number of rising highs and falling lows\n",
    "    df['rising_highs'] = count_increases(df[high_col], base_window)\n",
    "    df['falling_lows'] = count_decreases(df[low_col], base_window)\n",
    "\n",
    "    # Adjust window size dynamically\n",
    "    df['hh_window'] = np.where(df['falling_lows'] >= 3, adaptive_window, base_window)\n",
    "    df['ll_window'] = np.where(df['rising_highs'] >= 3, adaptive_window, base_window)\n",
    "\n",
    "    # Use a rolling apply to respect row-wise variable window size\n",
    "    df['higher_high'] = df[high_col].rolling(window=base_window, min_periods=1).max()\n",
    "    df['lower_low'] = df[low_col].rolling(window=base_window, min_periods=1).min()\n",
    "\n",
    "    # Apply variable window sizes\n",
    "    for i in range(len(df)):\n",
    "        df.loc[df.index[i], 'higher_high'] = df[high_col].iloc[max(0, i - df['hh_window'].iloc[i] + 1): i + 1].max()\n",
    "        df.loc[df.index[i], 'lower_low'] = df[low_col].iloc[max(0, i - df['ll_window'].iloc[i] + 1): i + 1].min()\n",
    "\n",
    "    # Calculate rolling min of high for higher lows, and rolling max of low for lower highs\n",
    "    df['higher_low'] = df[low_col].rolling(window=base_window, min_periods=1).min().cummax()\n",
    "    df['lower_high'] = df[high_col].rolling(window=base_window, min_periods=1).max().cummin()\n",
    "\n",
    "    # Create the envelope by combining these calculated values\n",
    "    df['price_action_upper'] = df[['higher_high', 'lower_high']].max(axis=1)\n",
    "    df['price_action_lower'] = df[['lower_low', 'higher_low']].min(axis=1)\n",
    "\n",
    "    # Drop intermediate columns that are no longer needed\n",
    "    df.drop(columns=['higher_high', 'lower_low', 'higher_low', 'lower_high', \n",
    "                     'rising_highs', 'falling_lows', 'hh_window', 'll_window'], inplace=True)\n",
    "\n",
    "    # Add moving averages for price_action_upper and price_action_lower\n",
    "    df['ma_price_action_upper'] = df['price_action_upper'].rolling(window=base_window, min_periods=1).mean()\n",
    "    df['ma_price_action_lower'] = df['price_action_lower'].rolling(window=base_window, min_periods=1).mean()\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_trend_score_1(df, open_col='open', high_col='high', low_col='low', close_col='close', window=6):\n",
    "    \"\"\"\n",
    "    Calculate a trend score based on price action using a rolling window approach.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing OHLC price data.\n",
    "    open_col (str): The column name for open prices (default 'open').\n",
    "    high_col (str): The column name for high prices (default 'high').\n",
    "    low_col (str): The column name for low prices (default 'low').\n",
    "    close_col (str): The column name for close prices (default 'close').\n",
    "    window (int): The rolling window size for calculating trend score (default 6, as 5 deltas are needed).\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: A trend score scaled from 0 to 100.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate deltas for each OHLC component\n",
    "    high_diff = df[high_col].diff()\n",
    "    low_diff = df[low_col].diff()\n",
    "    open_diff = df[open_col].diff()\n",
    "    close_diff = df[close_col].diff()\n",
    "\n",
    "    # Convert deltas into trend scores\n",
    "    high_score = np.sign(high_diff)  # +1 for up, -1 for down, 0 for no change\n",
    "    low_score = np.sign(low_diff)\n",
    "    open_score = np.sign(open_diff)\n",
    "    close_score = np.sign(close_diff)\n",
    "\n",
    "    # Sum the trend scores over a rolling window\n",
    "    trend_score_raw = (\n",
    "        high_score.rolling(window=window, min_periods=1).sum() +\n",
    "        low_score.rolling(window=window, min_periods=1).sum() +\n",
    "        open_score.rolling(window=window, min_periods=1).sum() +\n",
    "        close_score.rolling(window=window, min_periods=1).sum()\n",
    "    )\n",
    "\n",
    "    # Normalize the trend score to a 0-100 scale\n",
    "    trend_score_scaled = ((trend_score_raw + 20) / 40) * 100\n",
    "\n",
    "    return trend_score_scaled\n",
    "\n",
    "def calculate_indicators_1(\n",
    "    df, \n",
    "    price_col='close', \n",
    "    acc_vol_col='accumulative_volume', \n",
    "    sma_periods=None, \n",
    "    wma_periods=None, \n",
    "    rsi_periods=None, \n",
    "    volume_col='volume', \n",
    "    candle_window=10, \n",
    "    base_window=5, \n",
    "    adaptive_window=3,\n",
    "    trend_window=6\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate technical indicators, including SMAs, WMAs, RSI, price action envelopes, and trend score.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the price data.\n",
    "    - price_col (str): The column name for the price data (default 'close').\n",
    "    - acc_vol_col (str): The column name for accumulative volume data (default 'accumulative_volume').\n",
    "    - sma_periods (list): A list of periods for SMAs.\n",
    "    - wma_periods (list): A list of periods for WMAs.\n",
    "    - rsi_periods (list): A list of periods for RSI.\n",
    "    - volume_col (str): The column name to store the volume difference (default 'volume').\n",
    "    - candle_window (int): The window size for candle metrics calculations (default 10).\n",
    "    - base_window (int): The window size for detecting local highs/lows in the price action envelope.\n",
    "    - adaptive_window (int): The alternative smaller window size for adaptive price action envelope.\n",
    "    - trend_window (int): The window size for calculating trend scores.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated DataFrame with calculated indicators.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure RSI periods is a valid list\n",
    "    if rsi_periods is None:\n",
    "        rsi_periods = []\n",
    "\n",
    "    # Calculate volume differences\n",
    "    df[volume_col] = df[acc_vol_col].diff()\n",
    "\n",
    "    # Calculate SMAs\n",
    "    if sma_periods:\n",
    "        for period in sma_periods:\n",
    "            df[f'sma_{period}'] = df[price_col].rolling(window=period).mean()\n",
    "\n",
    "    # Calculate WMAs\n",
    "    if wma_periods:\n",
    "        for period in wma_periods:\n",
    "            weights = np.arange(1, period + 1)  # Generate weights from 1 to the period length\n",
    "            df[f'wma_{period}'] = df[price_col].rolling(window=period).apply(\n",
    "                lambda prices: np.dot(prices, weights) / weights.sum(), raw=True\n",
    "            )\n",
    "\n",
    "    # calculate_vwap_and_bands(df, high_col='high', low_col='low', close_col='close', volume_col='volume')\n",
    "\n",
    "    # Calculate RSI\n",
    "    for period in rsi_periods:\n",
    "        delta = df[price_col].diff()\n",
    "        gain = delta.where(delta > 0, 0).rolling(window=period).mean()\n",
    "        loss = -delta.where(delta < 0, 0).rolling(window=period).mean()\n",
    "        rs = gain / loss\n",
    "        df[f'rsi_{period}'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # Calculate price action envelope (Updated with dynamic base_window & adaptive_window)\n",
    "    df = calculate_price_action_envelope_1(df, high_col='high', low_col='low', base_window=base_window, adaptive_window=adaptive_window)\n",
    "\n",
    "    # Calculate trend score (Added)\n",
    "    df['trend_indicator'] = calculate_trend_score_1(df, open_col='open', high_col='high', low_col='low', close_col='close', window=trend_window)\n",
    "\n",
    "    # Calculate additional candle properties\n",
    "    df['ohlc_average'] = df[['open', 'high', 'low', 'close']].mean(axis=1)\n",
    "    df['candle_span'] = df['high'] - df['low']\n",
    "    df['candle_body'] = (df['close'] - df['open']).abs()\n",
    "    df['candle_span_avg'] = df['candle_span'].rolling(window=candle_window, min_periods=1).mean()\n",
    "    df['candle_span_max'] = df['candle_span'].rolling(window=candle_window, min_periods=1).max()\n",
    "    df['candle_span_maxavg_mean'] = (df['candle_span_avg'] + df['candle_span_max']) / 2\n",
    "\n",
    "    # Add constant reference lines for trend analysis\n",
    "    df['hundred_line'] = 100\n",
    "    df['fifty_line'] = 50\n",
    "    df['zero_line'] = 0\n",
    "    df['trend_high_threshold'] = 75\n",
    "    df['trend_low_threshold'] = 25\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trade logic functions. These and the viz/metric functions go together. These function groups are bespoke to a strategy and this is the point at which a new strategy starts from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trading_signals_1(candles, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5', strategy_type='trend', order_type='market', directional_bias='long'):\n",
    "    \"\"\"\n",
    "    Generate buy/sell signals based on moving averages and RSI indicators for either\n",
    "    a trend-following or mean-reversion strategy.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): The DataFrame containing candle data.\n",
    "    - ma_name1 (str): Column name for the first moving average.\n",
    "    - ma_name2 (str): Column name for the second moving average.\n",
    "    - rsi_column (str): Column name for the RSI indicator.\n",
    "    - strategy_type (str): 'trend' or 'reversion'.\n",
    "\n",
    "    Returns:\n",
    "    - candles (pd.DataFrame): The DataFrame with updated signal and position state columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Append strategy type to column names\n",
    "    ma_signal_column = f'signal_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_signal_column = f'signal_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    ma_position_open_col = f'position_open_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_position_open_col = f'position_open_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "\n",
    "    # Initialize columns\n",
    "    candles[ma_signal_column] = 0\n",
    "    candles[rsi_signal_column] = 0\n",
    "    candles[ma_position_open_col] = False\n",
    "    candles[rsi_position_open_col] = False\n",
    "\n",
    "    # ======= MOVING AVERAGE LOGIC =======\n",
    "    ma_position_open = True\n",
    "    ma_signals = []\n",
    "    ma_positions = []\n",
    "\n",
    "    for ma1, ma2 in zip(candles[ma_name1], candles[ma_name2]):\n",
    "        if strategy_type == 'trend':\n",
    "            # Trend-following logic\n",
    "            if ma_position_open and ma1 <= ma2: # if position is neutral and wma is below sma\n",
    "                ma_position_open = False\n",
    "                ma_signals.append(0)  # Neutral\n",
    "            elif not ma_position_open and ma1 > ma2:\n",
    "                ma_position_open = True\n",
    "                ma_signals.append(1)  # Long\n",
    "            else:\n",
    "                ma_signals.append(ma_signals[-1] if ma_signals else 0)\n",
    "        elif strategy_type == 'reversion':\n",
    "            # Mean-reversion logic (inverted)\n",
    "            if ma_position_open and ma1 > ma2:\n",
    "                ma_position_open = False\n",
    "                ma_signals.append(0)  # Neutral\n",
    "            elif not ma_position_open and ma1 <= ma2:\n",
    "                ma_position_open = True\n",
    "                ma_signals.append(1)  # Long\n",
    "            else:\n",
    "                ma_signals.append(ma_signals[-1] if ma_signals else 0)\n",
    "\n",
    "        ma_positions.append(ma_position_open)\n",
    "\n",
    "    candles[ma_signal_column] = ma_signals\n",
    "    candles[ma_position_open_col] = ma_positions\n",
    "\n",
    "    # ======= RSI LOGIC =======\n",
    "    rsi_position_open = True\n",
    "    rsi_signals = []\n",
    "    rsi_positions = []\n",
    "\n",
    "    for rsi in candles[rsi_column]:\n",
    "        if strategy_type == 'trend':\n",
    "            if rsi_position_open and rsi < 50:\n",
    "                rsi_position_open = False\n",
    "                rsi_signals.append(0)  # Neutral\n",
    "            elif not rsi_position_open and rsi >= 50:\n",
    "                rsi_position_open = True\n",
    "                rsi_signals.append(1)  # Long\n",
    "            else:\n",
    "                rsi_signals.append(rsi_signals[-1] if rsi_signals else 0)\n",
    "        elif strategy_type == 'reversion':\n",
    "            if rsi_position_open and rsi >= 50:\n",
    "                rsi_position_open = False\n",
    "                rsi_signals.append(0)  # Neutral\n",
    "            elif not rsi_position_open and rsi < 50:\n",
    "                rsi_position_open = True\n",
    "                rsi_signals.append(1)  # Long\n",
    "            else:\n",
    "                rsi_signals.append(rsi_signals[-1] if rsi_signals else 0)\n",
    "\n",
    "        rsi_positions.append(rsi_position_open)\n",
    "\n",
    "    candles[rsi_signal_column] = rsi_signals\n",
    "    candles[rsi_position_open_col] = rsi_positions\n",
    "\n",
    "    return candles\n",
    "\n",
    "def update_position_open_1(candles, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5', strategy_type='trend', order_type='market', directional_bias='long'):\n",
    "    \"\"\"\n",
    "    Update the 'position_open' columns for MA and RSI strategies for a specific strategy type (trend or reversion).\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): The DataFrame containing the signals and position columns.\n",
    "    - ma_name1 (str): Column name for the first moving average.\n",
    "    - ma_name2 (str): Column name for the second moving average.\n",
    "    - rsi_column (str): Column name for the RSI signal.\n",
    "    - strategy_type (str): Either 'trend' or 'reversion'\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated DataFrame with 'position_open' columns for the chosen strategy type.\n",
    "    \"\"\"\n",
    "    # Append strategy type to column names\n",
    "    ma_signal_column = f'signal_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_signal_column = f'signal_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    ma_position_open = f'position_open_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_position_open = f'position_open_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    \n",
    "    # Update position open columns based on the signals\n",
    "    candles[ma_position_open] = candles[ma_signal_column] == 1\n",
    "    candles[rsi_position_open] = candles[rsi_signal_column] == 1\n",
    "    \n",
    "    return candles\n",
    "\n",
    "def determine_entry_prices_1(candles, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5', \n",
    "                             ticker_to_tick_size=None, ticker=None, strategy_type='trend', order_type='market', directional_bias='long'):\n",
    "    \"\"\"\n",
    "    Determine entry prices for MA and RSI strategies based on signals.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): The DataFrame containing candle data.\n",
    "    - ma_name1 (str): Column name for the first moving average.\n",
    "    - ma_name2 (str): Column name for the second moving average.\n",
    "    - rsi_column (str): Column name for the RSI indicator.\n",
    "    - ticker_to_tick_size (dict): Mapping of tickers to their tick sizes.\n",
    "    - ticker (str): The ticker for which the tick size applies.\n",
    "    - strategy_type (str): Either 'trend' or 'reversion', affecting the signal logic.\n",
    "    - order_type (str): 'market' or 'limit', defining how entry prices are set.\n",
    "\n",
    "    Returns:\n",
    "    - candles (pd.DataFrame): The DataFrame with updated entry price columns.\n",
    "    \"\"\"\n",
    "    # Dynamically generate column names including the strategy type\n",
    "    ma_signal_column = f'signal_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_signal_column = f'signal_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    ma_entry_price = f'entry_price_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_entry_price = f'entry_price_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "\n",
    "    # Initialize entry price columns\n",
    "    candles[ma_entry_price] = None\n",
    "    candles[rsi_entry_price] = None\n",
    "\n",
    "    # Get tick size\n",
    "    tick_size = ticker_to_tick_size.get(ticker, 0) if ticker_to_tick_size else 0\n",
    "\n",
    "    # Determine tick size adjustment based on order type\n",
    "    add_tick_size = order_type == 'market'\n",
    "\n",
    "    # Moving Average Strategy\n",
    "    ma_signals = candles[ma_signal_column]\n",
    "    ma_close_prices = candles['close']\n",
    "    ma_entry_mask = (ma_signals == 1) & (ma_signals.shift(1) != 1)\n",
    "    candles.loc[ma_entry_mask, ma_entry_price] = ma_close_prices[ma_entry_mask] + (tick_size if add_tick_size else 0)\n",
    "\n",
    "    # RSI Strategy\n",
    "    rsi_signals = candles[rsi_signal_column]\n",
    "    rsi_entry_mask = (rsi_signals == 1) & (rsi_signals.shift(1) != 1)\n",
    "    candles.loc[rsi_entry_mask, rsi_entry_price] = ma_close_prices[rsi_entry_mask] + (tick_size if add_tick_size else 0)\n",
    "\n",
    "    return candles\n",
    "\n",
    "def determine_exit_prices_1(candles, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5', \n",
    "                            ticker_to_tick_size=None, ticker=None, strategy_type='trend', order_type='market', directional_bias='long'):\n",
    "    \"\"\"\n",
    "    Determine exit prices for MA and RSI strategies based on signals.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): The DataFrame containing candle data.\n",
    "    - ma_name1 (str): Column name for the first moving average.\n",
    "    - ma_name2 (str): Column name for the second moving average.\n",
    "    - rsi_column (str): Column name for the RSI indicator.\n",
    "    - ticker_to_tick_size (dict): Mapping of tickers to their tick sizes.\n",
    "    - ticker (str): The ticker for which the tick size applies.\n",
    "    - strategy_type (str): Either 'trend' or 'reversion', affecting the signal logic.\n",
    "    - order_type (str): 'market' or 'limit', defining how exit prices are set.\n",
    "\n",
    "    Returns:\n",
    "    - candles (pd.DataFrame): The DataFrame with updated exit price columns.\n",
    "    \"\"\"\n",
    "    # Dynamically generate column names including the strategy type\n",
    "    ma_signal_column = f'signal_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_signal_column = f'signal_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    ma_exit_price = f'exit_price_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_exit_price = f'exit_price_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "\n",
    "    # Initialize exit price columns\n",
    "    candles[ma_exit_price] = None\n",
    "    candles[rsi_exit_price] = None\n",
    "\n",
    "    # Get tick size\n",
    "    tick_size = ticker_to_tick_size.get(ticker, 0) if ticker_to_tick_size else 0\n",
    "\n",
    "    # Determine tick size adjustment based on order type\n",
    "    subtract_tick_size = order_type == 'market'\n",
    "\n",
    "    # Moving Average Strategy\n",
    "    ma_signals = candles[ma_signal_column]\n",
    "    ma_close_prices = candles['close']\n",
    "    ma_exit_mask = (ma_signals == 0) & (ma_signals.shift(1) == 1)\n",
    "    candles.loc[ma_exit_mask, ma_exit_price] = ma_close_prices[ma_exit_mask] - (tick_size if subtract_tick_size else 0)\n",
    "\n",
    "    # RSI Strategy\n",
    "    rsi_signals = candles[rsi_signal_column]\n",
    "    rsi_exit_mask = (rsi_signals == 0) & (rsi_signals.shift(1) == 1)\n",
    "    candles.loc[rsi_exit_mask, rsi_exit_price] = ma_close_prices[rsi_exit_mask] - (tick_size if subtract_tick_size else 0)\n",
    "\n",
    "    return candles\n",
    "\n",
    "def calculate_stop_losses_1(candles, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5', strategy_type='trend', order_type='market', directional_bias='long'):\n",
    "    \"\"\"\n",
    "    Dynamically calculate stop loss levels for MA and RSI strategies and ensure they persist while positions are open.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): The DataFrame containing candle data.\n",
    "    - ma_name1 (str): Column name for the first moving average.\n",
    "    - ma_name2 (str): Column name for the second moving average.\n",
    "    - rsi_column (str): Column name for the RSI indicator.\n",
    "    - strategy_type (str): Either 'trend' or 'reversion', affecting column naming.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated DataFrame with dynamically named stop loss columns.\n",
    "    \"\"\"\n",
    "    # Dynamically generate column names including the strategy type\n",
    "    ma_entry_price = f'entry_price_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    ma_exit_price = f'exit_price_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_entry_price = f'entry_price_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_exit_price = f'exit_price_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    stop_loss_ma = f'stop_loss_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    stop_loss_rsi = f'stop_loss_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "\n",
    "    # Initialize stop loss columns\n",
    "    candles[stop_loss_ma] = None\n",
    "    candles[stop_loss_rsi] = None\n",
    "\n",
    "    # Moving Average Stop Loss\n",
    "    ma_entry_mask = candles[ma_entry_price].notnull()\n",
    "    ma_exit_mask = candles[ma_exit_price].notnull()\n",
    "    \n",
    "    # Set stop loss where positions open\n",
    "    candles.loc[ma_entry_mask, stop_loss_ma] = candles[ma_entry_price] - candles['candle_span_max']\n",
    "\n",
    "    # Reset stop loss and close position where positions close\n",
    "    candles.loc[ma_exit_mask, stop_loss_ma] = None\n",
    "\n",
    "    # RSI Stop Loss\n",
    "    rsi_entry_mask = candles[rsi_entry_price].notnull()\n",
    "    rsi_exit_mask = candles[rsi_exit_price].notnull()\n",
    "    \n",
    "    # Set stop loss where positions open\n",
    "    candles.loc[rsi_entry_mask, stop_loss_rsi] = candles[rsi_entry_price] - candles['candle_span_max']\n",
    "\n",
    "    # Reset stop loss and close position where positions close\n",
    "    candles.loc[rsi_exit_mask, stop_loss_rsi] = None\n",
    "\n",
    "    # Forward-fill stop loss for both strategies\n",
    "    candles[stop_loss_ma] = candles[stop_loss_ma].ffill()\n",
    "    candles[stop_loss_rsi] = candles[stop_loss_rsi].ffill()\n",
    "\n",
    "    return candles\n",
    "\n",
    "def track_stop_loss_hits_1(candles, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5', ticker_to_tick_size=None, ticker=None, strategy_type='trend', order_type='market', directional_bias='long'):\n",
    "    \"\"\"\n",
    "    Track whether stop losses have been hit for MA and RSI strategies and update dynamically named columns.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): The DataFrame containing candle data.\n",
    "    - ma_name1 (str): Column name for the first moving average.\n",
    "    - ma_name2 (str): Column name for the second moving average.\n",
    "    - rsi_column (str): Column name for the RSI indicator.\n",
    "    - ticker_to_tick_size (dict): Mapping of tickers to their tick sizes.\n",
    "    - ticker (str): The ticker for which the tick size applies.\n",
    "    - strategy_type (str): Either 'trend' or 'reversion', affecting column naming.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated DataFrame with dynamically named stop loss hit flags.\n",
    "    \"\"\"\n",
    "    # Dynamically generate column names including strategy type\n",
    "    stop_loss_ma = f'stop_loss_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    stop_loss_rsi = f'stop_loss_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    ma_stop_loss_hit = f'stop_loss_hit_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_stop_loss_hit = f'stop_loss_hit_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    ma_position_open = f'position_open_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_position_open = f'position_open_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "\n",
    "    # Initialize stop loss hit columns\n",
    "    candles[ma_stop_loss_hit] = False\n",
    "    candles[rsi_stop_loss_hit] = False\n",
    "\n",
    "    # Get tick size (ensure it's non-zero)\n",
    "    tick_size = ticker_to_tick_size.get(ticker, 0) if ticker_to_tick_size else 0\n",
    "\n",
    "    # Ensure stop loss values are numerical (convert None to NaN)\n",
    "    candles[stop_loss_ma] = candles[stop_loss_ma].fillna(float('inf'))\n",
    "    candles[stop_loss_rsi] = candles[stop_loss_rsi].fillna(float('inf'))\n",
    "\n",
    "    # Moving Average Stop Loss Hit Logic\n",
    "    ma_hit_condition = (\n",
    "        (candles[stop_loss_ma].notnull()) & \n",
    "        (candles['close'] <= (candles[stop_loss_ma])) & \n",
    "        candles[ma_position_open]\n",
    "    )\n",
    "    candles.loc[ma_hit_condition, ma_stop_loss_hit] = True\n",
    "\n",
    "    # RSI Stop Loss Hit Logic\n",
    "    rsi_hit_condition = (\n",
    "        (candles[stop_loss_rsi].notnull()) & \n",
    "        (candles['close'] <= (candles[stop_loss_rsi])) & \n",
    "        candles[rsi_position_open]\n",
    "    )\n",
    "    candles.loc[rsi_hit_condition, rsi_stop_loss_hit] = True\n",
    "\n",
    "    return candles\n",
    "\n",
    "def adjust_signals_for_stop_loss_1(candles, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5', strategy_type='trend', order_type='market', directional_bias='long'):\n",
    "    \"\"\"\n",
    "    Adjust MA and RSI signals to 0 where stop loss has been hit.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): The DataFrame containing candle data.\n",
    "    - ma_name1 (str): Column name for the first moving average.\n",
    "    - ma_name2 (str): Column name for the second moving average.\n",
    "    - rsi_column (str): Column name for the RSI indicator.\n",
    "    - strategy_type (str): Either 'trend' or 'reversion', affecting column naming.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated DataFrame with adjusted signals.\n",
    "    \"\"\"\n",
    "    # Dynamically generate column names including strategy type\n",
    "    ma_signal_column = f'signal_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_signal_column = f'signal_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    ma_stop_loss_hit_column = f'stop_loss_hit_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_stop_loss_hit_column = f'stop_loss_hit_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "\n",
    "    # Adjust MA and RSI signals where stop loss has been hit\n",
    "    candles.loc[candles[ma_stop_loss_hit_column], ma_signal_column] = 0\n",
    "    candles.loc[candles[rsi_stop_loss_hit_column], rsi_signal_column] = 0\n",
    "\n",
    "    return candles\n",
    "\n",
    "def update_stop_loss_1(candles, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5', strategy_type='trend', order_type='market', directional_bias='long'):\n",
    "    \"\"\"\n",
    "    Dynamically set stop loss columns to NaN where corresponding signal columns are 0.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): The DataFrame containing candle data.\n",
    "    - ma_name1 (str): Column name for the first moving average.\n",
    "    - ma_name2 (str): Column name for the second moving average.\n",
    "    - rsi_column (str): Column name for the RSI indicator.\n",
    "    - strategy_type (str): Either 'trend' or 'reversion', affecting column naming.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated DataFrame with dynamically named stop loss columns modified.\n",
    "    \"\"\"\n",
    "    # Dynamically generate column names including strategy type\n",
    "    ma_signal_column = f'signal_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_signal_column = f'signal_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    stop_loss_ma = f'stop_loss_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    stop_loss_rsi = f'stop_loss_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "\n",
    "    # Update stop loss columns to NaN where signals are 0\n",
    "    candles.loc[candles[ma_signal_column] == 0, stop_loss_ma] = float('nan')\n",
    "    candles.loc[candles[rsi_signal_column] == 0, stop_loss_rsi] = float('nan')\n",
    "\n",
    "    return candles\n",
    "\n",
    "def calculate_profit_loss_1(candles, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5', contract_multiplier=1, trade_commission=1.5, strategy_type='trend', order_type='market', directional_bias='long'):\n",
    "    \"\"\" \n",
    "    Dynamically calculate profit and loss based on entry and exit price columns, including cumulative commission costs.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (pd.DataFrame): The DataFrame containing candle data with entry and exit price columns.\n",
    "    - contract_multiplier (float): The multiplier for PnL calculation (e.g., contract size).\n",
    "    - trade_commission (float): The commission cost per trade.\n",
    "    - ma_name1 (str): Column name for the first moving average.\n",
    "    - ma_name2 (str): Column name for the second moving average.\n",
    "    - rsi_column (str): Column name for the RSI indicator.\n",
    "    - strategy_type (str): The type of strategy ('trend' or 'reversion').\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with dynamically named profit/loss and commission cost columns.\n",
    "    \"\"\"\n",
    "    # Dynamically generate column names with strategy type\n",
    "    ma_entry_price = f'entry_price_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    ma_exit_price = f'exit_price_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_entry_price = f'entry_price_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_exit_price = f'exit_price_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    pnl_ma_col = f'pnl_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    pnl_rsi_col = f'pnl_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    cum_pnl_ma_col = f'cum_{pnl_ma_col}'\n",
    "    cum_pnl_rsi_col = f'cum_{pnl_rsi_col}'\n",
    "    cum_pnl_all_col = f'cum_pnl_all_{ma_name1}_{ma_name2}_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    ma_commission_col = f'commission_cost_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_commission_col = f'commission_cost_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "\n",
    "    # Initialize PnL and commission columns\n",
    "    candles[pnl_ma_col] = 0.0\n",
    "    candles[pnl_rsi_col] = 0.0\n",
    "    candles[ma_commission_col] = 0.0\n",
    "    candles[rsi_commission_col] = 0.0\n",
    "\n",
    "    # Moving Average Strategy PnL and Commission Calculation\n",
    "    ma_entry_indices = candles.index[candles[ma_entry_price].notnull()]\n",
    "    ma_exit_indices = candles.index[candles[ma_exit_price].notnull()]\n",
    "\n",
    "    # Pair up entry and exit prices\n",
    "    valid_pairs_ma = min(len(ma_entry_indices), len(ma_exit_indices))\n",
    "    ma_entry_prices = candles.loc[ma_entry_indices[:valid_pairs_ma], ma_entry_price].values\n",
    "    ma_exit_prices = candles.loc[ma_exit_indices[:valid_pairs_ma], ma_exit_price].values\n",
    "\n",
    "    # Calculate commission costs for MA strategy\n",
    "    candles[ma_commission_col] = candles[ma_entry_price].notna().astype(int) * trade_commission + \\\n",
    "                                 candles[ma_exit_price].notna().astype(int) * trade_commission\n",
    "    candles[ma_commission_col] = candles[ma_commission_col].cumsum()  # Accumulate commission costs\n",
    "\n",
    "    # Calculate PnL for MA strategy\n",
    "    ma_pnl = (ma_exit_prices - ma_entry_prices) * contract_multiplier\n",
    "    candles.loc[ma_exit_indices[:valid_pairs_ma], pnl_ma_col] = ma_pnl\n",
    "\n",
    "    # RSI Strategy PnL and Commission Calculation\n",
    "    rsi_entry_indices = candles.index[candles[rsi_entry_price].notnull()]\n",
    "    rsi_exit_indices = candles.index[candles[rsi_exit_price].notnull()]\n",
    "\n",
    "    # Pair up entry and exit prices\n",
    "    valid_pairs_rsi = min(len(rsi_entry_indices), len(rsi_exit_indices))\n",
    "    rsi_entry_prices = candles.loc[rsi_entry_indices[:valid_pairs_rsi], rsi_entry_price].values\n",
    "    rsi_exit_prices = candles.loc[rsi_exit_indices[:valid_pairs_rsi], rsi_exit_price].values\n",
    "\n",
    "    # Calculate commission costs for RSI strategy\n",
    "    candles[rsi_commission_col] = candles[rsi_entry_price].notna().astype(int) * trade_commission + \\\n",
    "                                  candles[rsi_exit_price].notna().astype(int) * trade_commission\n",
    "    candles[rsi_commission_col] = candles[rsi_commission_col].cumsum()  # Accumulate commission costs\n",
    "\n",
    "    # Calculate PnL for RSI strategy\n",
    "    rsi_pnl = (rsi_exit_prices - rsi_entry_prices) * contract_multiplier\n",
    "    candles.loc[rsi_exit_indices[:valid_pairs_rsi], pnl_rsi_col] = rsi_pnl\n",
    "\n",
    "    # Calculate cumulative PnL for both strategies\n",
    "    candles[cum_pnl_ma_col] = candles[pnl_ma_col].cumsum()\n",
    "    candles[cum_pnl_rsi_col] = candles[pnl_rsi_col].cumsum()\n",
    "\n",
    "    # Calculate combined cumulative PnL\n",
    "    candles[cum_pnl_all_col] = candles[cum_pnl_ma_col] + candles[cum_pnl_rsi_col]\n",
    "\n",
    "    return candles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 style=\"color:yellow;\">Load All Data for Extensive Back-testing, CLean it, Prepare for Compression</h2>\n",
    "#### **- Only take it this far when you want to get straight to the stored data without having to wait on the heavy computation in the section right after**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the raw streamed/stored data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_load = ['datetime', 'ticker', 'open', 'high', 'low', 'close', 'accumulative_volume']\n",
    "minute_candles_nmhr = load_dfs_from_csv(load_path=\"raw_futures_data/\", columns_to_load=columns_to_load)\n",
    "minute_candles_nmhr['/NQ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure the data types of each column are correct and consistent across all tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "data_types = check_column_data_types(minute_candles_nmhr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a quick overview of the dictionary of data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = \"/ES,/NQ,/CL,/GC\"\n",
    "tickers_list = list(minute_candles_nmhr.keys())\n",
    "\n",
    "key = 1\n",
    "\n",
    "display(minute_candles_nmhr)\n",
    "for key in range(len(tickers_list)):\n",
    "    display(minute_candles_nmhr[tickers_list[key]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a list of all dates on which streamed data was collected and stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_dates = get_shared_unique_dates(minute_candles_nmhr, string_format=True)\n",
    "print(len(shared_dates))\n",
    "display(shared_dates)\n",
    "display(minute_candles_nmhr['/NQ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure to only include the selected columns in case there are any unwanted ones floating around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_save = ['datetime', 'ticker', 'open', 'high', 'low', 'close', 'accumulative_volume']\n",
    "\n",
    "# Apply the function to the minute_candles dictionary\n",
    "minute_candles_nmhr = filter_columns_in_dict(minute_candles_nmhr, columns_to_save)\n",
    "minute_candles_nmhr['/NQ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data before splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in minute_candles_nmhr.keys():\n",
    "    minute_candles_nmhr[key] = remove_zero_close(minute_candles_nmhr[key])\n",
    "minute_candles_nmhr['/NQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in minute_candles_nmhr.keys():\n",
    "    minute_candles_nmhr[key] = remove_duplicates(minute_candles_nmhr[key])\n",
    "minute_candles_nmhr['/NQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in minute_candles_nmhr:\n",
    "    minute_candles_nmhr[key] = remove_5pm_hour_rows(minute_candles_nmhr[key])\n",
    "minute_candles_nmhr['/NQ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some metadata about `minute_candles_nmhr` that is needed to know if intentions are being properly executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Column names so far: {minute_candles_nmhr['/NQ'].columns.tolist()}')\n",
    "print()\n",
    "print(f'length of minute_candles_nmhr[/NQ]: {len(minute_candles_nmhr['/NQ'])/60} hours, {len(minute_candles_nmhr['/NQ'])/60/24} full days')\n",
    "print(\"Unique timezones:\", minute_candles_nmhr['/NQ']['timezone'].unique())\n",
    "print(\"Unique timezone names:\", minute_candles_nmhr['/NQ']['timezone_name'].unique())\n",
    "print()\n",
    "print(minute_candles_nmhr['/NQ']['timezone'].value_counts())\n",
    "print()\n",
    "print(minute_candles_nmhr['/NQ']['timezone_name'].value_counts())\n",
    "print()\n",
    "\n",
    "# Extract the /NQ DataFrame\n",
    "df = minute_candles_nmhr['/NQ'].copy()\n",
    "\n",
    "# Count rows per hour\n",
    "row_counts_by_hour = df['hour'].value_counts().sort_index()\n",
    "\n",
    "# Display result\n",
    "print(f'There should be an abscence of 17th hour rows.')\n",
    "print(row_counts_by_hour)\n",
    "\n",
    "# Filter rows where the hour is 17\n",
    "df_17 = df[df['hour'] == 17].copy()\n",
    "\n",
    "hour_17 = len(df_17)\n",
    "\n",
    "# Display the first 25 rows\n",
    "print(f'number of rows in 17th hour: {hour_17}')\n",
    "print(f'number of 17th hours: {hour_17/60}')\n",
    "print()\n",
    "print(f'This df_17 should be empty if all minutes in the 17th hour were removed correctly.')\n",
    "display(df_17.head(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell measures any discrepancy between what the timezone should be (Eastern standard EST or Eastern daylight savings EDT) and what is actually being recorded from live stream data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_timezone_name(row):\n",
    "    # Create a naive datetime object\n",
    "    dt = datetime(row['year'], row['month'], row['day'], row['hour'], row['minute'])\n",
    "    # Determine DST status manually based on U.S. Eastern rules\n",
    "    # DST starts: second Sunday in March at 2am\n",
    "    # DST ends: first Sunday in November at 2am\n",
    "    year = row['year']\n",
    "    dst_start = datetime(year, 3, 8)\n",
    "    while dst_start.weekday() != 6:  # Sunday\n",
    "        dst_start = dst_start.replace(day=dst_start.day + 1)\n",
    "    dst_start = dst_start.replace(hour=2)\n",
    "\n",
    "    dst_end = datetime(year, 11, 1)\n",
    "    while dst_end.weekday() != 6:  # Sunday\n",
    "        dst_end = dst_end.replace(day=dst_end.day + 1)\n",
    "    dst_end = dst_end.replace(hour=2)\n",
    "\n",
    "    return 'EDT' if dst_start <= dt < dst_end else 'EST'\n",
    "\n",
    "# Apply the logic to your DataFrame\n",
    "df = minute_candles_nmhr['/NQ'].copy()\n",
    "df['expected_timezone_name'] = df.apply(get_expected_timezone_name, axis=1)\n",
    "\n",
    "# Compare and count matches/discrepancies\n",
    "df['tz_match'] = df['timezone_name'] == df['expected_timezone_name']\n",
    "print(\"Match count:\", df['tz_match'].sum())\n",
    "print(\"Mismatch count:\", (~df['tz_match']).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the data by minute-to-minute continuity, and optionally by `division_factor`, and creating a new dictionary with one more key-pair degree of `ticker` : `session`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits on continuity and `division_factor` (splitting longer sessions into smaller ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_data_continuity(minute_candles, \n",
    "                             time_column='datetime', \n",
    "                             continuity_threshold='2min', \n",
    "                             min_rows=60, \n",
    "                             divide_long_sessions=False, \n",
    "                             division_factor=180):\n",
    "    \"\"\"\n",
    "    Splits each DataFrame in a dictionary by data continuity into a new dictionary of dictionaries,\n",
    "    and optionally divides longer sessions into smaller chunks.\n",
    "\n",
    "    Parameters:\n",
    "    - minute_candles (dict): Dictionary of DataFrames containing OHLC price data for multiple tickers.\n",
    "    - time_column (str): Name of the column containing datetime information.\n",
    "    - continuity_threshold (str or pd.Timedelta): Threshold for detecting discontinuity (e.g., '2min').\n",
    "    - min_rows (int): Minimum number of rows required to keep a session.\n",
    "    - divide_long_sessions (bool): Whether to further divide long sessions into fixed-size chunks.\n",
    "    - division_factor (int): Number of rows per chunk when dividing long sessions.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Nested dictionary {ticker: {time_slice: DataFrame}} excluding short sessions.\n",
    "    \"\"\"\n",
    "    split_data = {}\n",
    "\n",
    "    for ticker, df in minute_candles.items():\n",
    "        df[time_column] = pd.to_datetime(df[time_column])\n",
    "        df = df.sort_values(by=time_column).reset_index(drop=True)\n",
    "\n",
    "        time_diffs = df[time_column].diff().fillna(pd.Timedelta(seconds=0))\n",
    "        session_breaks = time_diffs > pd.to_timedelta(continuity_threshold)\n",
    "        session_ids = session_breaks.cumsum()\n",
    "\n",
    "        ticker_sessions = {}\n",
    "\n",
    "        for session_id, session_df in df.groupby(session_ids):\n",
    "            session_df = session_df.reset_index(drop=True)\n",
    "\n",
    "            if len(session_df) >= min_rows:\n",
    "                if divide_long_sessions and len(session_df) >= 2 * division_factor:\n",
    "                    num_chunks = len(session_df) // division_factor\n",
    "                    for i in range(num_chunks):\n",
    "                        chunk_df = session_df.iloc[i * division_factor : (i + 1) * division_factor].copy()\n",
    "                        start_time = chunk_df[time_column].iloc[0].strftime('%Y-%m-%d %H:%M')\n",
    "                        end_time = chunk_df[time_column].iloc[-1].strftime('%Y-%m-%d %H:%M')\n",
    "                        chunk_slice = f\"{start_time} to {end_time}\"\n",
    "                        ticker_sessions[chunk_slice] = chunk_df\n",
    "                else:\n",
    "                    start_time = session_df[time_column].iloc[0].strftime('%Y-%m-%d %H:%M')\n",
    "                    end_time = session_df[time_column].iloc[-1].strftime('%Y-%m-%d %H:%M')\n",
    "                    time_slice = f\"{start_time} to {end_time}\"\n",
    "                    ticker_sessions[time_slice] = session_df\n",
    "\n",
    "        if ticker_sessions:\n",
    "            split_data[ticker] = ticker_sessions\n",
    "\n",
    "    return split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_sessions = split_by_data_continuity(\n",
    "    minute_candles_nmhr,\n",
    "    continuity_threshold='2min',\n",
    "    min_rows=60,\n",
    "    divide_long_sessions=True,\n",
    "    division_factor=180\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing sessions for every ticker\n",
    "for ticker, sessions in split_sessions.items():\n",
    "    print(f\"Ticker: {ticker}, Total Sessions: {len(sessions)}\")\n",
    "\n",
    "    total_weighted_percentage = 0  # Sum of weighted percentages\n",
    "    total_rows = 0  # Total number of rows across sessions\n",
    "\n",
    "    for time_slice, session_df in sessions.items():\n",
    "        # Calculate the percentage of candles containing the previous close\n",
    "        percentage_containing_previous_close = percentage_candles_containing_previous_close_1(session_df)\n",
    "        \n",
    "        # Calculate the average close price for the session\n",
    "        average_close_price = session_df['close'].mean().round(2)\n",
    "        \n",
    "        # Calculate weighted contribution\n",
    "        session_rows = len(session_df)\n",
    "        total_weighted_percentage += percentage_containing_previous_close * session_rows\n",
    "        total_rows += session_rows  # Accumulate total rows\n",
    "\n",
    "        # Print the session-level details\n",
    "        print(\n",
    "            f\"  Time Slice: {time_slice}, Rows: {session_rows}, \"\n",
    "            f\"Percentage Containing Previous Close: {percentage_containing_previous_close:.2f}%, \"\n",
    "            f\"Average Close Price in Points: {average_close_price:.2f}\"\n",
    "        )\n",
    "    \n",
    "    # Calculate the weighted average percentage for the current ticker\n",
    "    weighted_average_percentage = (total_weighted_percentage / total_rows) if total_rows > 0 else 0\n",
    "    \n",
    "    # Print the weighted average\n",
    "    print(f\"\\n  Weighted Average Percentage Containing Previous Close for {ticker}: {weighted_average_percentage:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking to make sure that the sessions divided smoothly without changing the format of datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the nth session key (e.g., index 0, 1, 2, ...)\n",
    "session_index_view = 0\n",
    "ticker_view = '/NQ'\n",
    "\n",
    "# Extract the session key by index\n",
    "session_keys = list(split_sessions[ticker].keys())\n",
    "selected_session_view = session_keys[session_index_view]\n",
    "\n",
    "# Access and display the DataFrame\n",
    "display(split_sessions[ticker_view][selected_session_view])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing the distribution of session lengths for each ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_session_lengths(split_sessions):\n",
    "    \"\"\"\n",
    "    Plots histograms of session lengths for each ticker,\n",
    "    syncing x-axis ticks and bin edges every 60,\n",
    "    and y-axis ticks every 2,\n",
    "    with black-colored bars and red lines for mean, median, and quartiles.\n",
    "    Also prints mean, median, and quartiles after each chart,\n",
    "    in both minutes and hours.\n",
    "    \"\"\"\n",
    "    for ticker, sessions in split_sessions.items():\n",
    "        session_lengths = [len(session_df) for session_df in sessions.values()]\n",
    "        \n",
    "        if not session_lengths:\n",
    "            continue\n",
    "\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        \n",
    "        # Calculate bin edges and x-ticks at 60-step intervals\n",
    "        max_x = max(session_lengths)\n",
    "        bin_edges = np.arange(0, max_x + 60, 60)\n",
    "\n",
    "        # Plot histogram with black bars\n",
    "        plt.hist(session_lengths, bins=bin_edges, edgecolor='orange', color='orange')\n",
    "        \n",
    "        # Calculate statistics\n",
    "        mean_val = np.mean(session_lengths)\n",
    "        median_val = np.median(session_lengths)\n",
    "        q1 = np.percentile(session_lengths, 25)\n",
    "        q3 = np.percentile(session_lengths, 75)\n",
    "\n",
    "        # Plot vertical lines\n",
    "        for stat, label in zip([mean_val, median_val, q1, q3], ['Mean', 'Median', 'Q1', 'Q3']):\n",
    "            plt.axvline(stat, color='red', linestyle='--', linewidth=2)\n",
    "            plt.text(\n",
    "                stat, plt.gca().get_ylim()[1]*0.92, \n",
    "                f'{label}: {stat:.0f}', \n",
    "                color='red', rotation=90, va='top', ha='right', \n",
    "                fontsize=12, fontweight='bold'\n",
    "            )\n",
    "\n",
    "        # Bigger title and axis labels\n",
    "        plt.title(f'Session Length Distribution for {ticker}', fontsize=18, fontweight='bold')\n",
    "        plt.xlabel('Number of Rows (Session Length)', fontsize=14)\n",
    "        plt.ylabel('Frequency', fontsize=14)\n",
    "\n",
    "        # Make xticks and yticks a little bigger\n",
    "        plt.xticks(bin_edges, fontsize=12)\n",
    "        \n",
    "        ax = plt.gca()\n",
    "        max_y = ax.get_ylim()[1]\n",
    "        plt.yticks(np.arange(0, max_y + 2, 2), fontsize=12)  # y-axis increments of 2\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # === Print summary statistics ===\n",
    "        print(f\"Summary Statistics for {ticker}:\")\n",
    "        for label, value in zip(['Mean', 'Median', 'Q1', 'Q3'], [mean_val, median_val, q1, q3]):\n",
    "            hours = value / 60\n",
    "            print(f\"  {label} Session Length: {value:.2f} minutes ({hours:.2f} hours)\")\n",
    "        print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "plot_session_lengths(split_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 style=\"color:yellow;\">Compressing Each Session of Each Ticker in the Dictionary, Calculating Indicators, Applying Strategy</h2> \n",
    "#### **- This is where we get bound by big computation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h3 style=\"color:magenta;\">Compressing Each Session of Each Ticker in the Dictionary</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a new key:value degree ( `session` : `compression` ) to the candle dictionary and call it `compressed_sessions`. Every session now has as many compressions as there are in the `compression_factors` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new nested dictionary\n",
    "compressed_sessions = {}\n",
    "\n",
    "# Iterate through tickers\n",
    "for ticker, sessions in split_sessions.items():\n",
    "    compressed_sessions[ticker] = {}  # Initialize ticker dictionary\n",
    "\n",
    "    # Iterate through sessions (time slices)\n",
    "    for time_slice, session_df in sessions.items():\n",
    "        compressed_sessions[ticker][time_slice] = {}  # Initialize session dictionary\n",
    "\n",
    "        # Apply each compression factor\n",
    "        for compression_factor in compression_factors:\n",
    "            dynamic_name = f\"{compression_factor}_minute_compression\"\n",
    "            compressed_sessions[ticker][time_slice][dynamic_name] = compress_candles(session_df, compression_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dictionary[ticker][session][compression][lower_slice:upper_slice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the nth session key (e.g., index 0, 1, 2, ...)\n",
    "session_index_view = 0\n",
    "ticker_view = '/NQ'\n",
    "compression_view = '1_minute_compression'\n",
    "\n",
    "# Extract the session key by index\n",
    "session_keys = list(compressed_sessions[ticker].keys())\n",
    "selected_session_view = session_keys[session_index_view]\n",
    "\n",
    "# Access and display the DataFrame\n",
    "display(compressed_sessions[ticker_view][selected_session_view][compression_view])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eastern_time` should not exist in any data frame in the dictionary. Here, we check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compressed_sessions[ticker_view][selected_session_view][compression_view]\n",
    "\n",
    "if 'eastern_time' in df.columns:\n",
    "    print(\"'eastern_time' column exists.\")\n",
    "else:\n",
    "    print(\"'eastern_time' column does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing percentage of candles that contain prior candles' close for each compression of each session of each ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_candles_containing_price_1(df, price_column, offset=0):\n",
    "    \"\"\"Calculate the % of candles where (prev close  offset) is within [low, high].\"\"\"\n",
    "    prev_close = df[price_column].shift(1)\n",
    "    target_price = prev_close + offset\n",
    "    contains = (df['low'] <= target_price) & (df['high'] >= target_price)\n",
    "    return contains.mean() * 100  # percentage\n",
    "\n",
    "for ticker, sessions in compressed_sessions.items():\n",
    "    print(f\"\\nTicker: {ticker}, Total Sessions: {len(sessions)}\")\n",
    "    tick_size = ticker_to_tick_size.get(ticker, 0)\n",
    "\n",
    "    # Storage\n",
    "    weighted_sums_center = {}\n",
    "    weighted_sums_minus_tick = {}\n",
    "    weighted_sums_plus_tick = {}\n",
    "    row_counts = {}\n",
    "\n",
    "    for time_slice, compressions in sessions.items():\n",
    "        print(f\"  {ticker} Time Slice: {time_slice}, Total Compressions: {len(compressions)}\")\n",
    "\n",
    "        for compression_name, session_df in compressions.items():\n",
    "            compression_factor = int(compression_name.split('_')[0])\n",
    "\n",
    "            pct_center = percentage_candles_containing_price_1(session_df, 'close', offset=0)\n",
    "            pct_minus_tick = percentage_candles_containing_price_1(session_df, 'close', offset=-tick_size)\n",
    "            pct_plus_tick = percentage_candles_containing_price_1(session_df, 'close', offset=tick_size)\n",
    "\n",
    "            avg_close = session_df['close'].mean().round(2)\n",
    "            session_rows = len(session_df)\n",
    "\n",
    "            # Init storage\n",
    "            if compression_factor not in weighted_sums_center:\n",
    "                weighted_sums_center[compression_factor] = 0\n",
    "                weighted_sums_minus_tick[compression_factor] = 0\n",
    "                weighted_sums_plus_tick[compression_factor] = 0\n",
    "                row_counts[compression_factor] = 0\n",
    "\n",
    "            # Weighted accumulation\n",
    "            weighted_sums_center[compression_factor] += pct_center * session_rows\n",
    "            weighted_sums_minus_tick[compression_factor] += pct_minus_tick * session_rows\n",
    "            weighted_sums_plus_tick[compression_factor] += pct_plus_tick * session_rows\n",
    "            row_counts[compression_factor] += session_rows\n",
    "\n",
    "            # Print session-level stats\n",
    "            print(\n",
    "                f\"    Compression: {compression_factor}-Min, Rows: {session_rows}, \"\n",
    "                f\"Avg Close: {avg_close:.2f}, \"\n",
    "                f\"% Close In Candle: {pct_center:.2f}%, \"\n",
    "                f\"% Close - Tick In Candle: {pct_minus_tick:.2f}%, \"\n",
    "                f\"% Close + Tick In Candle: {pct_plus_tick:.2f}%\"\n",
    "            )\n",
    "\n",
    "    # Print weighted averages per compression\n",
    "    print(\"\\n  Weighted Averages by Compression Factor:\")\n",
    "    for compression_factor in sorted(row_counts.keys()):\n",
    "        total_rows = row_counts[compression_factor]\n",
    "        if total_rows == 0:\n",
    "            continue\n",
    "        print(\n",
    "            f\"    {ticker} {compression_factor}-Min:\"\n",
    "            f\" Center={weighted_sums_center[compression_factor] / total_rows:.2f}%,\"\n",
    "            f\" -Tick={weighted_sums_minus_tick[compression_factor] / total_rows:.2f}%,\"\n",
    "            f\" +Tick={weighted_sums_plus_tick[compression_factor] / total_rows:.2f}%\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h3 style=\"color:magenta;\">Calculating Indicators for Each Session of Each Ticker in the Dictionary</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating indicators for each compression of each session of each ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the result to verify\n",
    "print(\"MA Periods:\", ma_periods)\n",
    "print(\"MA Combinations:\", ma_combinations)\n",
    "\n",
    "# Iterate through all tickers in compressed_sessions\n",
    "for ticker, sessions in compressed_sessions.items():\n",
    "    print(f\"Processing ticker: {ticker}\")\n",
    "\n",
    "    # Iterate through all sessions of the current ticker\n",
    "    for time_slice, compressions in sessions.items():\n",
    "        print(f\"{ticker}  Processing session: {time_slice}, Total Compressions: {len(compressions)}\")\n",
    "\n",
    "        # Iterate through all compression levels of the current session\n",
    "        for compression_name, df in compressions.items():\n",
    "            print(f\"    Processing compression: {compression_name} in session {time_slice}\")\n",
    "\n",
    "            # Calculate moving averages and indicators for each DataFrame\n",
    "            compressions[compression_name] = calculate_indicators_1(\n",
    "                df, \n",
    "                price_col='close', \n",
    "                acc_vol_col='accumulative_volume', \n",
    "                sma_periods=sma_periods,\n",
    "                wma_periods=wma_periods,\n",
    "                rsi_periods=rsi_periods,\n",
    "                candle_window=5,\n",
    "                base_window=5,\n",
    "                adaptive_window=3,\n",
    "                trend_window=6\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(compressed_sessions[ticker_view][selected_session_view][compression_view])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at some counts to think about the size of the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_compressed_sessions(compressed_sessions):\n",
    "    num_tickers = len(compressed_sessions)\n",
    "    num_sessions = 0\n",
    "    num_compressions = 0\n",
    "    total_rows = 0\n",
    "    total_columns = 0\n",
    "\n",
    "    for ticker, sessions in compressed_sessions.items():\n",
    "        num_sessions += len(sessions)\n",
    "        for session, compressions in sessions.items():\n",
    "            num_compressions += len(compressions)\n",
    "            for compression, df in compressions.items():\n",
    "                total_rows += len(df)\n",
    "                total_columns += len(df.columns)\n",
    "\n",
    "    print(f\"Total number of tickers: {num_tickers}\")\n",
    "    print(f\"Total number of sessions across all tickers: {num_sessions}\")\n",
    "    print(f\"Total number of compressions across all tickers and all sessions: {num_compressions}\")\n",
    "    print(f\"Total number of columns across all tickers, sessions, and compressions: {total_columns}\")\n",
    "    print(f\"Total number of rows across all tickers, sessions, and compressions: {total_rows}\")\n",
    "\n",
    "summarize_compressed_sessions(compressed_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access and display the DataFrame\n",
    "display(compressed_sessions[ticker_view][selected_session_view][compression_view])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h3 style=\"color:magenta;\">Applying the Trading Strategy to Each Session of Each Ticker in the Dictionary</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the trading strategy to each compression of each session of each ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_trading_signals_1, update_position_open_1, determine_entry_prices_1,\n",
    "# determine_exit_prices_1, calculate_stop_losses_1, track_stop_loss_hits_1,\n",
    "# adjust_signals_for_stop_loss_1, update_stop_loss_1, calculate_profit_loss_1\n",
    "\n",
    "strategy_types = ['trend', 'reversion']\n",
    "order_types = ['market', 'limit']\n",
    "directional_biases = ['long', 'short']\n",
    "directional_bias = 'long'\n",
    "\n",
    "# Iterate through all tickers in compressed_sessions\n",
    "for ticker, sessions in compressed_sessions.items():\n",
    "    print(f\"Processing ticker: {ticker}\")\n",
    "\n",
    "    # Iterate through all sessions of the current ticker\n",
    "    for time_slice, compressions in sessions.items():\n",
    "        print(f\"  {ticker} Processing session: {time_slice}, Total Compressions: {len(compressions)}\")\n",
    "\n",
    "        # Iterate through all compression levels of the current session\n",
    "        for compression_name, df in compressions.items():\n",
    "            print(f\"    {ticker} Processing compression: {compression_name} in session {time_slice}\")\n",
    "\n",
    "            # Iterate through all the ma_combinations\n",
    "            for sig_ma, con_ma, rsi_col in ma_combinations:\n",
    "\n",
    "                for strategy_type in strategy_types:\n",
    "                    print(f\"      {ticker} {time_slice} {compression_name} Applying {strategy_type} strategy for {sig_ma} and {con_ma}\")\n",
    "                    \n",
    "                    for order_type in order_types:\n",
    "                        print(f\"        {ticker} Applying {order_type} order_type\")\n",
    "\n",
    "                        # for directional_bias in directional_biases:\n",
    "                        #     print(f\"          Applying {directional_bias} directional bias\")\n",
    "\n",
    "                        # Generate trading signals for the current strategy\n",
    "                        compressions[compression_name] = generate_trading_signals_1(\n",
    "                            df,\n",
    "                            ma_name1=sig_ma,\n",
    "                            ma_name2=con_ma,\n",
    "                            rsi_column=rsi_col,\n",
    "                            strategy_type=strategy_type,\n",
    "                            order_type=order_type,\n",
    "                            directional_bias=directional_bias\n",
    "                        )\n",
    "\n",
    "                        # Update position_open columns to be 1:1 verbal boolean with the signal\n",
    "                        compressions[compression_name] = update_position_open_1(\n",
    "                            df,\n",
    "                            ma_name1=sig_ma,\n",
    "                            ma_name2=con_ma,\n",
    "                            rsi_column=rsi_col,\n",
    "                            strategy_type=strategy_type,\n",
    "                            order_type=order_type,\n",
    "                            directional_bias=directional_bias\n",
    "                        )\n",
    "\n",
    "                        # Determine entry prices for each ticker\n",
    "                        compressions[compression_name] = determine_entry_prices_1(\n",
    "                            df,\n",
    "                            ma_name1=sig_ma,\n",
    "                            ma_name2=con_ma,\n",
    "                            rsi_column=rsi_col,\n",
    "                            ticker_to_tick_size=ticker_to_tick_size,\n",
    "                            ticker=ticker,\n",
    "                            strategy_type=strategy_type,\n",
    "                            order_type=order_type,\n",
    "                            directional_bias=directional_bias\n",
    "                        )\n",
    "\n",
    "                        # Determine exit prices for each ticker\n",
    "                        compressions[compression_name] = determine_exit_prices_1(\n",
    "                            df,\n",
    "                            ma_name1=sig_ma,\n",
    "                            ma_name2=con_ma,\n",
    "                            rsi_column=rsi_col,\n",
    "                            ticker_to_tick_size=ticker_to_tick_size,\n",
    "                            ticker=ticker,\n",
    "                            strategy_type=strategy_type,\n",
    "                            order_type=order_type,\n",
    "                            directional_bias=directional_bias\n",
    "                        )\n",
    "\n",
    "                        # Stop loss calculation\n",
    "                        compressions[compression_name] = calculate_stop_losses_1(\n",
    "                            df,\n",
    "                            ma_name1=sig_ma,\n",
    "                            ma_name2=con_ma,\n",
    "                            rsi_column=rsi_col,\n",
    "                            strategy_type=strategy_type,\n",
    "                            order_type=order_type,\n",
    "                            directional_bias=directional_bias\n",
    "                        )\n",
    "\n",
    "                        # Track stop loss hits\n",
    "                        compressions[compression_name] = track_stop_loss_hits_1(\n",
    "                            df,\n",
    "                            ma_name1=sig_ma,\n",
    "                            ma_name2=con_ma,\n",
    "                            rsi_column=rsi_col,\n",
    "                            ticker_to_tick_size=ticker_to_tick_size,\n",
    "                            ticker=ticker,\n",
    "                            strategy_type=strategy_type,\n",
    "                            order_type=order_type,\n",
    "                            directional_bias=directional_bias\n",
    "                        )\n",
    "\n",
    "                        # Adjust signals from stop loss hits\n",
    "                        compressions[compression_name] = adjust_signals_for_stop_loss_1(\n",
    "                            df,\n",
    "                            ma_name1=sig_ma,\n",
    "                            ma_name2=con_ma,\n",
    "                            rsi_column=rsi_col,\n",
    "                            strategy_type=strategy_type,\n",
    "                            order_type=order_type,\n",
    "                            directional_bias=directional_bias\n",
    "                        )\n",
    "\n",
    "                        # Re-update position_open column after stop loss hits\n",
    "                        compressions[compression_name] = update_position_open_1(\n",
    "                            df,\n",
    "                            ma_name1=sig_ma,\n",
    "                            ma_name2=con_ma,\n",
    "                            rsi_column=rsi_col,\n",
    "                            strategy_type=strategy_type,\n",
    "                            order_type=order_type,\n",
    "                            directional_bias=directional_bias\n",
    "                        )\n",
    "\n",
    "                        # Re-determine entry prices after stop loss hits\n",
    "                        compressions[compression_name] = determine_entry_prices_1(\n",
    "                            df,\n",
    "                            ma_name1=sig_ma,\n",
    "                            ma_name2=con_ma,\n",
    "                            rsi_column=rsi_col,\n",
    "                            ticker_to_tick_size=ticker_to_tick_size,\n",
    "                            ticker=ticker,\n",
    "                            strategy_type=strategy_type,\n",
    "                            order_type=order_type,\n",
    "                            directional_bias=directional_bias\n",
    "                        )\n",
    "\n",
    "                        # Re-determine exit prices after stop loss hits\n",
    "                        compressions[compression_name] = determine_exit_prices_1(\n",
    "                            df,\n",
    "                            ma_name1=sig_ma,\n",
    "                            ma_name2=con_ma,\n",
    "                            rsi_column=rsi_col,\n",
    "                            ticker_to_tick_size=ticker_to_tick_size,\n",
    "                            ticker=ticker,\n",
    "                            strategy_type=strategy_type,\n",
    "                            order_type=order_type,\n",
    "                            directional_bias=directional_bias\n",
    "                        )\n",
    "\n",
    "                        # Update stop loss levels after stop loss hits\n",
    "                        compressions[compression_name] = update_stop_loss_1(\n",
    "                            df,\n",
    "                            ma_name1=sig_ma,\n",
    "                            ma_name2=con_ma,\n",
    "                            rsi_column=rsi_col,\n",
    "                            strategy_type=strategy_type,\n",
    "                            order_type=order_type,\n",
    "                            directional_bias=directional_bias\n",
    "                            \n",
    "                        )\n",
    "\n",
    "                        # Calculate profit/loss for each ticker's DataFrame\n",
    "                        compressions[compression_name] = calculate_profit_loss_1(\n",
    "                            df,\n",
    "                            contract_multiplier=1,\n",
    "                            ma_name1=sig_ma,\n",
    "                            ma_name2=con_ma,\n",
    "                            rsi_column=rsi_col,\n",
    "                            strategy_type=strategy_type,\n",
    "                            order_type=order_type,\n",
    "                            directional_bias=directional_bias   \n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dictionary to see how the application of the trading strategy affected its structure and size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_compressed_sessions(compressed_sessions)\n",
    "display(compressed_sessions[ticker_view][selected_session_view][compression_view])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get column names of the following DataFrame\n",
    "print(compressed_sessions[ticker_view][selected_session_view][compression_view].columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the evolution of each trade after entry in an attempt to find a pattern that would help optimize a \"fixed bar exit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the nth session key (e.g., index 0, 1, 2, ...)\n",
    "session_index_view1 = 300\n",
    "ticker_view1 = '/NQ'\n",
    "compression_view1 = '3_minute_compression'\n",
    "\n",
    "# Extract the session key by index\n",
    "session_keys1 = list(compressed_sessions[ticker].keys())\n",
    "selected_session_view1 = session_keys1[session_index_view1]\n",
    "\n",
    "def plot_pnl_curves_from_entries(\n",
    "    df,\n",
    "    ma_name1='wma_1',\n",
    "    ma_name2='sma_1',\n",
    "    strategy_type='trend',\n",
    "    order_type='market',\n",
    "    directional_bias='long',\n",
    "    max_bars=60\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot how PnL evolves from each entry point using the 'close' column.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame with OHLC data and entry prices.\n",
    "    - ma_name1, ma_name2, strategy_type, order_type, directional_bias: Used to dynamically define the entry column.\n",
    "    - max_bars (int): Maximum number of bars to track after entry.\n",
    "    \"\"\"\n",
    "    entry_col = f'entry_price_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "\n",
    "    if entry_col not in df.columns:\n",
    "        raise ValueError(f\"{entry_col} not found in DataFrame.\")\n",
    "\n",
    "    curves = []\n",
    "\n",
    "    for idx, entry_price in df[entry_col].dropna().items():\n",
    "        close_slice = df['close'].iloc[idx:idx + max_bars].values\n",
    "        if len(close_slice) > 1:\n",
    "            pnl_curve = close_slice - entry_price\n",
    "            curves.append(pnl_curve)\n",
    "\n",
    "    # Plot all curves\n",
    "    plt.figure(figsize=(18, 9))\n",
    "    for curve in curves:\n",
    "        plt.plot(curve, alpha=1, linewidth=2)\n",
    "\n",
    "    # Plot average curve\n",
    "    if curves:\n",
    "        max_len = max(len(c) for c in curves)\n",
    "        padded = np.full((len(curves), max_len), np.nan)\n",
    "        for i, curve in enumerate(curves):\n",
    "            padded[i, :len(curve)] = curve\n",
    "        avg_curve = np.nanmean(padded, axis=0)\n",
    "        plt.plot(avg_curve, color='black', linewidth=4, label='Average PnL')\n",
    "\n",
    "    plt.title(f\"PnL Curves from Entry  {entry_col}\")\n",
    "    plt.xlabel(\"Bars Since Entry\")\n",
    "    plt.ylabel(\"PnL\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display function for user to call\n",
    "plot_pnl_curves_from_entries(\n",
    "    df=compressed_sessions[ticker_view1][selected_session_view1][compression_view1],\n",
    "    ma_name1='wma_3',\n",
    "    ma_name2='sma_3',\n",
    "    strategy_type='reversion',\n",
    "    order_type='limit',\n",
    "    directional_bias='long',\n",
    "    max_bars=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting all average profit evolution curve averages across sessions to see if there is some pattern for optimizing fixed bar exit. There does not seem to be one. I may be presenting this data incorrectly for this question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_avg_pnl_curves_across_sessions(\n",
    "    sessions_dict,\n",
    "    ticker,\n",
    "    compression,\n",
    "    ma_name1='wma_1',\n",
    "    ma_name2='sma_1',\n",
    "    strategy_type='trend',\n",
    "    order_type='market',\n",
    "    directional_bias='long',\n",
    "    max_bars=60\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot average PnL curve for each session in the compressed_sessions dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - sessions_dict (dict): Dictionary of sessions (e.g., compressed_sessions).\n",
    "    - ticker (str): Ticker symbol to access.\n",
    "    - compression (str): Compression key (e.g., '3_minute_compression').\n",
    "    - ma_name1, ma_name2, strategy_type, order_type, directional_bias: Used to construct entry column.\n",
    "    - max_bars (int): Number of bars to track PnL after entry.\n",
    "    \"\"\"\n",
    "    entry_col = f'entry_price_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    avg_curves = []\n",
    "\n",
    "    for session_key in sessions_dict[ticker].keys():\n",
    "        df = sessions_dict[ticker][session_key][compression]\n",
    "        if entry_col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        curves = []\n",
    "        for idx, entry_price in df[entry_col].dropna().items():\n",
    "            close_slice = df['close'].iloc[idx:idx + max_bars].values\n",
    "            if len(close_slice) > 1:\n",
    "                pnl_curve = close_slice - entry_price\n",
    "                curves.append(pnl_curve)\n",
    "\n",
    "        if curves:\n",
    "            max_len = max(len(c) for c in curves)\n",
    "            padded = np.full((len(curves), max_len), np.nan)\n",
    "            for i, curve in enumerate(curves):\n",
    "                padded[i, :len(curve)] = curve\n",
    "            avg_curve = np.nanmean(padded, axis=0)\n",
    "            avg_curves.append(avg_curve)\n",
    "\n",
    "    # Plot all average curves\n",
    "    plt.figure(figsize=(18, 9))\n",
    "    for curve in avg_curves:\n",
    "        plt.plot(curve, linewidth=3, alpha=0.7)\n",
    "\n",
    "    plt.title(f\"Avg PnL Curves  All Sessions  {entry_col}\")\n",
    "    plt.xlabel(\"Bars Since Entry\")\n",
    "    plt.ylabel(\"PnL\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_avg_pnl_curves_across_sessions(\n",
    "    sessions_dict=compressed_sessions,\n",
    "    ticker='/NQ',\n",
    "    compression='3_minute_compression',\n",
    "    ma_name1='wma_3',\n",
    "    ma_name2='sma_3',\n",
    "    strategy_type='reversion',\n",
    "    order_type='limit',\n",
    "    directional_bias='long',\n",
    "    max_bars=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 style=\"color:yellow;\">Storing Calculated Data in Dictionary and Flat DF Formats and Pulling Them Back into Environment</h2>\n",
    "#### **- I'm doing this to reduce the time it takes to get into the analytics of the strategies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store `compressed_sessions` in json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_compressed_sessions_to_json(data, filepath):\n",
    "    serializable_dict = {}\n",
    "\n",
    "    for ticker, sessions in data.items():\n",
    "        serializable_dict[ticker] = {}\n",
    "        for session, compressions in sessions.items():\n",
    "            serializable_dict[ticker][session] = {}\n",
    "            for compression, df in compressions.items():\n",
    "                df_copy = df.copy()\n",
    "\n",
    "                # Convert any Timestamp (including tz-aware) to ISO string globally\n",
    "                df_copy = df_copy.applymap(\n",
    "                    lambda x: x.isoformat() if isinstance(x, pd.Timestamp) else x\n",
    "                )\n",
    "\n",
    "                serializable_dict[ticker][session][compression] = df_copy.to_dict(orient='records')\n",
    "\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(serializable_dict, f, indent=2)\n",
    "\n",
    "# save_compressed_sessions_to_json(compressed_sessions, 'compressed_sessions.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull `compressed_sessions' back into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_compressed_sessions_from_json(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    reconstructed = {}\n",
    "\n",
    "    for ticker, ticker_data in raw_data.items():\n",
    "        reconstructed[ticker] = {}\n",
    "        for session, session_data in ticker_data.items():\n",
    "            reconstructed[ticker][session] = {}\n",
    "            for compression, df_data in session_data.items():\n",
    "                df = pd.DataFrame(df_data)\n",
    "\n",
    "                # Convert 'datetime' column to datetime format (keep as a column)\n",
    "                if 'datetime' in df.columns:\n",
    "                    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "                # Keep default numeric index, don't set datetime as index\n",
    "                reconstructed[ticker][session][compression] = df\n",
    "\n",
    "    return reconstructed\n",
    "\n",
    "# compressed_sessions = load_compressed_sessions_from_json('compressed_sessions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(compressed_sessions[ticker_view][selected_session_view][compression_view])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten the nested dictionary `compressed_sessions` to the data frame `compressed_sessions_flattened` and call it back after storing it. This will allow for more detailed analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def flatten_compressed_sessions_to_dataframe(compressed_sessions_dictionary):\n",
    "#     all_rows = []\n",
    "\n",
    "#     for ticker, sessions in compressed_sessions_dictionary.items():\n",
    "#         for session, compressions in sessions.items():\n",
    "#             for compression, df in compressions.items():\n",
    "#                 temp_df = df.copy()\n",
    "#                 temp_df[\"ticker\"] = ticker\n",
    "#                 temp_df[\"session\"] = session\n",
    "#                 temp_df[\"compression_factor\"] = compression.split('_')[0]\n",
    "#                 all_rows.append(temp_df)\n",
    "\n",
    "#     combined_df = pd.concat(all_rows, ignore_index=True)\n",
    "\n",
    "#     # Reorder columns: datetime  session  compression  rest\n",
    "#     cols = combined_df.columns.tolist()\n",
    "\n",
    "#     if 'datetime' in cols:\n",
    "#         front = ['datetime', 'session', 'compression_factor']\n",
    "#         rest = [col for col in cols if col not in front]\n",
    "#         combined_df = combined_df[front + rest]\n",
    "\n",
    "#     return combined_df\n",
    "\n",
    "# compressed_sessions_flat = flatten_compressed_sessions_to_dataframe(compressed_sessions)\n",
    "\n",
    "# # Reorder columns: datetime  session  compression  rest\n",
    "# cols = compressed_sessions_flat.columns.tolist()\n",
    "\n",
    "# if 'datetime' in cols:\n",
    "#     front = ['datetime', 'session', 'compression_factor']\n",
    "#     rest = [col for col in cols if col not in front]\n",
    "#     compressed_sessions_flat = compressed_sessions_flat[front + rest]\n",
    "\n",
    "def flatten_compressed_sessions_to_dataframe(compressed_sessions_dictionary, keep_cols=None, downcast=True):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Generator to avoid a giant intermediate list\n",
    "    def _rows():\n",
    "        for ticker, sessions in compressed_sessions_dictionary.items():\n",
    "            for session, compressions in sessions.items():\n",
    "                for compression, df in compressions.items():\n",
    "                    # Optionally select only needed columns to reduce size\n",
    "                    g = df if keep_cols is None else df.loc[:, [c for c in keep_cols if c in df.columns]]\n",
    "                    # Add context columns (no deep copy; assign will copy only whats needed)\n",
    "                    yield g.assign(\n",
    "                        ticker=ticker,\n",
    "                        session=session,\n",
    "                        compression_factor=str(compression).split('_')[0]\n",
    "                    )\n",
    "\n",
    "    combined_df = pd.concat(_rows(), ignore_index=True)\n",
    "\n",
    "    # One-time column ordering\n",
    "    if 'datetime' in combined_df.columns:\n",
    "        front = ['datetime', 'session', 'compression_factor']\n",
    "        front = [c for c in front if c in combined_df.columns]\n",
    "        rest = [c for c in combined_df.columns if c not in front]\n",
    "        combined_df = combined_df.loc[:, front + rest]\n",
    "\n",
    "    if downcast:\n",
    "        # Cheap dtype slimming\n",
    "        for c in combined_df.select_dtypes(include=['float64']).columns:\n",
    "            combined_df[c] = pd.to_numeric(combined_df[c], downcast='float')\n",
    "        for c in combined_df.select_dtypes(include=['int64', 'int32']).columns:\n",
    "            combined_df[c] = pd.to_numeric(combined_df[c], downcast='integer')\n",
    "        # Many object cols with repeated labels compress well as category\n",
    "        for c in combined_df.select_dtypes(include=['object']).columns:\n",
    "            nunique = combined_df[c].nunique(dropna=False)\n",
    "            if nunique and nunique / max(1, len(combined_df)) < 0.5:\n",
    "                combined_df[c] = combined_df[c].astype('category')\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "compressed_sessions_flat = flatten_compressed_sessions_to_dataframe(compressed_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_df = compressed_sessions_flat[compressed_sessions_flat['ticker'] == '/NQ'].copy()\n",
    "display(nq_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write `compressed_sessions_flattened` to csv and pull back into data frame format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compressed_sessions_flat.to_csv(\"compressed_sessions_flat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compressed_sessions_flat = pd.read_csv(\"compressed_sessions_flat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_df = compressed_sessions_flat[compressed_sessions_flat['ticker'] == '/NQ'].copy()\n",
    "display(nq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_1m_df = compressed_sessions_flat[(compressed_sessions_flat['ticker'] == '/NQ') & (compressed_sessions_flat['compression_factor'] == 1)]\n",
    "nq_1m_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of column names for compressed_sessions_flat\n",
    "compressed_sessions_flat.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't delete, might need this on occasion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reconstruct_nested_dict_from_flattened_dataframe(compressed_sessions_flat):\n",
    "#     nested_dict = {}\n",
    "\n",
    "#     grouped = compressed_sessions_flat.groupby(['ticker', 'session', 'compression'])\n",
    "\n",
    "#     for (ticker, session, compression), group in grouped:\n",
    "#         df = group.drop(columns=['ticker', 'session', 'compression'])\n",
    "#         nested_dict.setdefault(ticker, {}).setdefault(session, {})[compression] = df.reset_index(drop=True)\n",
    "\n",
    "#     return nested_dict\n",
    "\n",
    "# compressed_sessions = reconstruct_nested_dict_from_flattened_dataframe(compressed_sessions_flat)\n",
    "# compressed_sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are suspect... not yet to be trusted. Distributions don't look how I expect. Especially the 17th hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_avg_hourly_pnl_from_flat(\n",
    "    df,\n",
    "    ticker,\n",
    "    compression_factor,\n",
    "    ma_name1='wma_1',\n",
    "    ma_name2='sma_1',\n",
    "    rsi_column='rsi_1',\n",
    "    strategy_type='trend',\n",
    "    order_type='market',\n",
    "    directional_bias='long',\n",
    "    source='ma'  # 'ma' or 'rsi'\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot average hourly PnL from a flat DataFrame for a given strategy configuration.\n",
    "    Filters by ticker and compression factor.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Flat DataFrame (e.g., compressed_sessions_flat)\n",
    "    - ticker (str): Ticker symbol to filter\n",
    "    - compression_factor (int): Compression factor to filter\n",
    "    - ma_name1, ma_name2, rsi_column: Indicator names\n",
    "    - strategy_type, order_type, directional_bias: Strategy metadata\n",
    "    - source (str): 'ma' or 'rsi' to indicate which pnl column to use\n",
    "    \"\"\"\n",
    "    # Construct dynamic column name\n",
    "    if source == 'ma':\n",
    "        pnl_col = f'pnl_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    elif source == 'rsi':\n",
    "        pnl_col = f'pnl_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    else:\n",
    "        raise ValueError(\"source must be 'ma' or 'rsi'\")\n",
    "\n",
    "    if pnl_col not in df.columns:\n",
    "        raise ValueError(f\"Column '{pnl_col}' not found in DataFrame.\")\n",
    "    if 'datetime' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain a 'datetime' column.\")\n",
    "\n",
    "    # Filter by ticker and compression factor\n",
    "    filtered = df[(df['ticker'] == ticker) & (df['compression_factor'] == compression_factor)]\n",
    "\n",
    "    if filtered.empty:\n",
    "        print(f\"No matching data for selected ticker {ticker} and compression factor {compression_factor}.\")\n",
    "        return\n",
    "\n",
    "    # Parse datetime and extract hour\n",
    "    filtered = filtered.copy()\n",
    "    filtered['datetime'] = pd.to_datetime(filtered['datetime'], utc=True)\n",
    "    filtered['datetime'] = filtered['datetime'].dt.tz_convert(None)\n",
    "    filtered['hour'] = filtered['datetime'].dt.hour\n",
    "\n",
    "    # Group by hour and average\n",
    "    hourly_avg = filtered.groupby('hour')[pnl_col].sum()\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.bar(hourly_avg.index, hourly_avg.values, color='black', edgecolor='white')\n",
    "    plt.title(f\"Avg Hourly PnL  {ticker} {compression_factor}min  {pnl_col}\", fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Hour of Day\", fontsize=14)\n",
    "    plt.ylabel(\"Average PnL\", fontsize=14)\n",
    "    plt.xticks(range(0, 24))\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_avg_hourly_pnl_from_flat(\n",
    "    df=compressed_sessions_flat,\n",
    "    ticker='/NQ',\n",
    "    compression_factor=3,\n",
    "    ma_name1='wma_3',\n",
    "    ma_name2='sma_3',\n",
    "    rsi_column='rsi_3',\n",
    "    strategy_type='reversion',\n",
    "    order_type='limit',\n",
    "    directional_bias='long',\n",
    "    source='ma'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hourly_row_count(\n",
    "    df,\n",
    "    ticker,\n",
    "    compression_factor\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the number of rows in the dataset per hour of the day, filtered by ticker and compression_factor.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Flat DataFrame (e.g., compressed_sessions_flat)\n",
    "    - ticker (str): Ticker symbol to filter\n",
    "    - compression_factor (int): Compression factor to filter\n",
    "    \"\"\"\n",
    "    if 'datetime' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain a 'datetime' column.\")\n",
    "\n",
    "    # Filter by ticker and compression factor\n",
    "    filtered = df[(df['ticker'] == ticker) & (df['compression_factor'] == compression_factor)]\n",
    "\n",
    "    if filtered.empty:\n",
    "        print(f\"No matching data for selected ticker {ticker} and compression factor {compression_factor}.\")\n",
    "        return\n",
    "\n",
    "    # Parse datetime and extract hour\n",
    "    filtered = filtered.copy()\n",
    "    filtered['datetime'] = pd.to_datetime(filtered['datetime'], utc=True).dt.tz_convert(None)\n",
    "    filtered['hour'] = filtered['datetime'].dt.hour\n",
    "\n",
    "    # Count rows per hour\n",
    "    hourly_counts = filtered.groupby('hour').size()\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.bar(hourly_counts.index, hourly_counts.values, color='black', edgecolor='white')\n",
    "    plt.title(f\"Hourly Row Count  {ticker} {compression_factor}min\", fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Hour of Day\", fontsize=14)\n",
    "    plt.ylabel(\"Row Count\", fontsize=14)\n",
    "    plt.xticks(range(0, 24))\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_hourly_row_count(\n",
    "    df=compressed_sessions_flat,\n",
    "    ticker='/NQ',\n",
    "    compression_factor=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_1m_df = compressed_sessions_flat[(compressed_sessions_flat['ticker'] == '/NQ') & (compressed_sessions_flat['compression_factor'] == 1)]\n",
    "nq_1m_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 style=\"color:yellow;\">Generate PNL Data Frame</h2>\n",
    "#### **This section generates a data frame that contains pnl figures for each compression of each session of each ticker of each strategy type (trend and reversion).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pnl_dataframe(compressed_sessions, ticker_to_tick_size, ticker_to_point_value, \n",
    "                           ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5', \n",
    "                           strategy_type='trend', order_type='market', directional_bias='long',\n",
    "                           daily_stop_loss_dollars=-1000):\n",
    "    \"\"\"\n",
    "    Generates a DataFrame containing detailed PnL, trade statistics, and max gain/loss for all tickers, \n",
    "    sessions, and compression factors, now including trend/reversion strategy type and MA/RSI periods.\n",
    "\n",
    "    Parameters:\n",
    "    - compressed_sessions: Dictionary of nested dictionaries containing trading data structured as:\n",
    "      {ticker: {session: {compression_factor: DataFrame}}}\n",
    "    - ticker_to_tick_size: Dictionary mapping tickers to tick sizes.\n",
    "    - ticker_to_point_value: Dictionary mapping tickers to point values.\n",
    "    - ma_name1, ma_name2: Moving average column names.\n",
    "    - rsi_column: RSI column name.\n",
    "    - strategy_type: \"trend\" or \"reversion\".\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with PnL, trade statistics, max gain/loss, session names, compression factors, \n",
    "      strategy type, and indicator periods.\n",
    "    \"\"\"\n",
    "    def compute_stop_loss_metrics(dollar_pnl, dollar_max_loss, daily_stop_loss_dollars):\n",
    "        hit = int(dollar_max_loss <= daily_stop_loss_dollars)\n",
    "        cost = daily_stop_loss_dollars - dollar_pnl if dollar_pnl > daily_stop_loss_dollars and hit else 0.0\n",
    "        gain = daily_stop_loss_dollars - dollar_pnl if dollar_pnl < daily_stop_loss_dollars and hit else 0.0\n",
    "        return hit, cost, gain\n",
    "\n",
    "    # Extract period lengths from indicator names (assumes format like \"wma_5\", \"sma_10\", \"rsi_14\")\n",
    "    ma1_period = int(ma_name1.split('_')[-1])  # Extract last numeric part\n",
    "    ma2_period = int(ma_name2.split('_')[-1])\n",
    "    rsi_period = int(rsi_column.split('_')[-1])\n",
    "\n",
    "    # Generate dynamic column names for PnL and trade metrics\n",
    "    pnl_ma_col = f'pnl_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    pnl_rsi_col = f'pnl_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    cum_pnl_ma_col = f'cum_{pnl_ma_col}'\n",
    "    cum_pnl_rsi_col = f'cum_{pnl_rsi_col}'\n",
    "    cum_pnl_all_col = f'cum_pnl_all_{ma_name1}_{ma_name2}_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    ma_entry_price = f'entry_price_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    ma_exit_price = f'exit_price_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_entry_price = f'entry_price_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_exit_price = f'exit_price_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    ma_commission_col = f'commission_cost_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_commission_col = f'commission_cost_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "\n",
    "    # Create a list to hold rows of the DataFrame\n",
    "    pnl_rows = []\n",
    "\n",
    "    # Iterate over tickers\n",
    "    for ticker, sessions in compressed_sessions.items():\n",
    "        tick_size = ticker_to_tick_size.get(ticker, \"Unknown\")\n",
    "        point_value = ticker_to_point_value.get(ticker, 1)\n",
    "\n",
    "        # Iterate over sessions\n",
    "        for session_name, compressions in sessions.items():\n",
    "\n",
    "            # Iterate over compression factors\n",
    "            for compression_name, df in compressions.items():\n",
    "                try:\n",
    "                    # Extract compression factor (e.g., '5_minute_compression'  5)\n",
    "                    compression_factor = int(compression_name.split('_')[0])\n",
    "\n",
    "                    # Calculate cumulative PnL and other statistics\n",
    "                    ma_pnl = round(df[cum_pnl_ma_col].iloc[-1], 3)\n",
    "                    rsi_pnl = round(df[cum_pnl_rsi_col].iloc[-1], 3)\n",
    "                    total_pnl = round(df[cum_pnl_all_col].iloc[-1], 3)\n",
    "                    close_price_diff = round(df[\"close\"].iloc[-1] - df[\"close\"].iloc[0], 3)\n",
    "                    point_alpha = round(total_pnl - close_price_diff, 3)\n",
    "\n",
    "                    # Retrieve total commission costs\n",
    "                    ma_commission_total = round(df[ma_commission_col].iloc[-1], 3) if ma_commission_col in df else 0.0\n",
    "                    rsi_commission_total = round(df[rsi_commission_col].iloc[-1], 3) if rsi_commission_col in df else 0.0\n",
    "                    total_commission_cost = ma_commission_total + rsi_commission_total\n",
    "\n",
    "                    # Calculate total dollar PnLs\n",
    "                    # Dollar PnL adjusted for commissions (optional)\n",
    "                    ma_dollar_pnl = ma_pnl * point_value\n",
    "                    rsi_dollar_pnl = rsi_pnl * point_value\n",
    "                    total_dollar_pnl = total_pnl * point_value\n",
    "                    ma_dollar_pnl_sub_comms = ma_pnl * point_value - ma_commission_total\n",
    "                    rsi_dollar_pnl_sub_comms = rsi_pnl * point_value - rsi_commission_total\n",
    "                    total_dollar_pnl_sub_comms = total_pnl * point_value - total_commission_cost\n",
    "                    close_price_dollar_diff = close_price_diff * point_value\n",
    "                    dollar_alpha = point_alpha * point_value\n",
    "\n",
    "                    # Count the number of trades for MA and RSI strategies\n",
    "                    ma_trades = (df[ma_exit_price].notna().sum() + df[ma_entry_price].notna().sum()) / 2\n",
    "                    rsi_trades = (df[rsi_exit_price].notna().sum() + df[rsi_entry_price].notna().sum()) / 2\n",
    "\n",
    "                    # Calculate max gain and max loss for MA, RSI, and total strategies (point and dollar)\n",
    "                    ma_max_gain = round(df[cum_pnl_ma_col].max(), 3)\n",
    "                    ma_max_loss = round(df[cum_pnl_ma_col].min(), 3)\n",
    "                    rsi_max_gain = round(df[cum_pnl_rsi_col].max(), 3)\n",
    "                    rsi_max_loss = round(df[cum_pnl_rsi_col].min(), 3)\n",
    "                    total_max_gain = round(df[cum_pnl_all_col].max(), 3)\n",
    "                    total_max_loss = round(df[cum_pnl_all_col].min(), 3)\n",
    "\n",
    "                    ma_max_dollar_gain = ma_max_gain * point_value\n",
    "                    ma_max_dollar_loss = ma_max_loss * point_value\n",
    "                    rsi_max_dollar_gain = rsi_max_gain * point_value\n",
    "                    rsi_max_dollar_loss = rsi_max_loss * point_value\n",
    "                    total_max_dollar_gain = total_max_gain * point_value\n",
    "                    total_max_dollar_loss = total_max_loss * point_value\n",
    "\n",
    "                    ma_gain_idx = df[cum_pnl_ma_col].idxmax()\n",
    "                    ma_loss_idx = df[cum_pnl_ma_col].idxmin()\n",
    "                    rsi_gain_idx = df[cum_pnl_rsi_col].idxmax()\n",
    "                    rsi_loss_idx = df[cum_pnl_rsi_col].idxmin()\n",
    "                    total_gain_idx = df[cum_pnl_all_col].idxmax()\n",
    "                    total_loss_idx = df[cum_pnl_all_col].idxmin()\n",
    "\n",
    "                    ma_max_dollar_gain_sub_comms = ma_max_dollar_gain - df[ma_commission_col].loc[ma_gain_idx]\n",
    "                    ma_max_dollar_loss_sub_comms = ma_max_dollar_loss - df[ma_commission_col].loc[ma_loss_idx]\n",
    "                    rsi_max_dollar_gain_sub_comms = rsi_max_dollar_gain - df[rsi_commission_col].loc[rsi_gain_idx]\n",
    "                    rsi_max_dollar_loss_sub_comms = rsi_max_dollar_loss - df[rsi_commission_col].loc[rsi_loss_idx]\n",
    "                    total_max_dollar_gain_sub_comms = total_max_dollar_gain - (\n",
    "                        df[ma_commission_col].loc[total_gain_idx] + df[rsi_commission_col].loc[total_gain_idx]\n",
    "                    )\n",
    "                    total_max_dollar_loss_sub_comms = total_max_dollar_loss - (\n",
    "                        df[ma_commission_col].loc[total_loss_idx] + df[rsi_commission_col].loc[total_loss_idx]\n",
    "                    )\n",
    "\n",
    "                    # Stop loss logic for total, MA, and RSI\n",
    "                    stop_loss_hit_total, loss_prevention_cost_total, loss_prevention_gain_total = compute_stop_loss_metrics(\n",
    "                        total_dollar_pnl, total_max_dollar_loss, daily_stop_loss_dollars)\n",
    "                    stop_loss_hit_ma, loss_prevention_cost_ma, loss_prevention_gain_ma = compute_stop_loss_metrics(\n",
    "                        ma_dollar_pnl, ma_max_dollar_loss, daily_stop_loss_dollars)\n",
    "                    stop_loss_hit_rsi, loss_prevention_cost_rsi, loss_prevention_gain_rsi = compute_stop_loss_metrics(\n",
    "                        rsi_dollar_pnl, rsi_max_dollar_loss, daily_stop_loss_dollars)\n",
    "                    \n",
    "                    # Compute the new stop-loss metrics for sub_comms\n",
    "                    stop_loss_hit_total_sub_comms, loss_prevention_cost_total_sub_comms, loss_prevention_gain_total_sub_comms = compute_stop_loss_metrics(\n",
    "                        total_dollar_pnl_sub_comms, total_max_dollar_loss_sub_comms, daily_stop_loss_dollars)\n",
    "                    stop_loss_hit_ma_sub_comms, loss_prevention_cost_ma_sub_comms, loss_prevention_gain_ma_sub_comms = compute_stop_loss_metrics(\n",
    "                        ma_dollar_pnl_sub_comms, ma_max_dollar_loss_sub_comms, daily_stop_loss_dollars)\n",
    "                    stop_loss_hit_rsi_sub_comms, loss_prevention_cost_rsi_sub_comms, loss_prevention_gain_rsi_sub_comms = compute_stop_loss_metrics(\n",
    "                        rsi_dollar_pnl_sub_comms, rsi_max_dollar_loss_sub_comms, daily_stop_loss_dollars)\n",
    "                    \n",
    "                    # Calculate adjusted total PnL after applying stop loss protection\n",
    "                    total_dollar_pnl_stop_loss_adjusted = (total_dollar_pnl + loss_prevention_cost_total + loss_prevention_gain_total)\n",
    "                    ma_dollar_pnl_stop_loss_adjusted = (ma_dollar_pnl + loss_prevention_cost_ma + loss_prevention_gain_ma)\n",
    "                    rsi_dollar_pnl_stop_loss_adjusted = (rsi_dollar_pnl + loss_prevention_cost_rsi + loss_prevention_gain_rsi)\n",
    "                    total_dollar_pnl_sub_comms_stop_loss_adjusted = (total_dollar_pnl_sub_comms + loss_prevention_cost_total_sub_comms + loss_prevention_gain_total_sub_comms)\n",
    "                    ma_dollar_pnl_sub_comms_stop_loss_adjusted = (ma_dollar_pnl_sub_comms + loss_prevention_cost_ma_sub_comms + loss_prevention_gain_ma_sub_comms)\n",
    "                    rsi_dollar_pnl_sub_comms_stop_loss_adjusted = (rsi_dollar_pnl_sub_comms + loss_prevention_cost_rsi_sub_comms + loss_prevention_gain_rsi_sub_comms)\n",
    "\n",
    "                    # Append row with all calculated metrics\n",
    "                    pnl_rows.append({\n",
    "                        'ticker': ticker,\n",
    "                        'session': session_name,\n",
    "                        'compression_factor': compression_factor,\n",
    "\n",
    "                        'ma_period1': ma1_period,\n",
    "                        'ma_period2': ma2_period,\n",
    "                        'rsi_period': rsi_period,\n",
    "\n",
    "                        'strategy_type': strategy_type,\n",
    "                        'order_type': order_type,\n",
    "                        'directional_bias': directional_bias,\n",
    "\n",
    "                        'ma_trades': ma_trades,\n",
    "                        'rsi_trades': rsi_trades,\n",
    "\n",
    "                        'total_point_pnl': total_pnl,\n",
    "                        'ma_point_pnl': ma_pnl,\n",
    "                        'rsi_point_pnl': rsi_pnl,\n",
    "\n",
    "                        'total_dollar_pnl': total_dollar_pnl,\n",
    "                        'ma_dollar_pnl': ma_dollar_pnl,\n",
    "                        'rsi_dollar_pnl': rsi_dollar_pnl,\n",
    "\n",
    "                        'ma_commission_cost': ma_commission_total,\n",
    "                        'rsi_commission_cost': rsi_commission_total,\n",
    "                        'total_commission_cost': total_commission_cost,\n",
    "\n",
    "                        'total_dollar_pnl_sub_comms': total_dollar_pnl_sub_comms,\n",
    "                        'ma_dollar_pnl_sub_comms': ma_dollar_pnl_sub_comms,\n",
    "                        'rsi_dollar_pnl_sub_comms': rsi_dollar_pnl_sub_comms,\n",
    "\n",
    "                        'ma_max_point_gain': ma_max_gain,\n",
    "                        'ma_max_point_loss': ma_max_loss,\n",
    "                        'rsi_max_point_gain': rsi_max_gain,\n",
    "                        'rsi_max_point_loss': rsi_max_loss,\n",
    "                        'total_max_point_gain': total_max_gain,\n",
    "                        'total_max_point_loss': total_max_loss,\n",
    "\n",
    "                        'ma_max_dollar_gain': ma_max_dollar_gain,\n",
    "                        'ma_max_dollar_loss': ma_max_dollar_loss,\n",
    "                        'rsi_max_dollar_gain': rsi_max_dollar_gain,\n",
    "                        'rsi_max_dollar_loss': rsi_max_dollar_loss,\n",
    "                        'total_max_dollar_gain': total_max_dollar_gain,\n",
    "                        'total_max_dollar_loss': total_max_dollar_loss,\n",
    "\n",
    "                        'ma_max_dollar_gain_sub_comms': ma_max_dollar_gain_sub_comms,\n",
    "                        'ma_max_dollar_loss_sub_comms': ma_max_dollar_loss_sub_comms,\n",
    "                        'rsi_max_dollar_gain_sub_comms': rsi_max_dollar_gain_sub_comms,\n",
    "                        'rsi_max_dollar_loss_sub_comms': rsi_max_dollar_loss_sub_comms,\n",
    "                        'total_max_dollar_gain_sub_comms': total_max_dollar_gain_sub_comms,\n",
    "                        'total_max_dollar_loss_sub_comms': total_max_dollar_loss_sub_comms,\n",
    "\n",
    "                        'close_price_diff': close_price_diff,\n",
    "                        'point_alpha': point_alpha,\n",
    "                        'close_price_dollar_diff': close_price_dollar_diff,\n",
    "                        'dollar_alpha': dollar_alpha,\n",
    "                        'tick_size': tick_size,\n",
    "\n",
    "                        'daily_stop_loss_dollars': daily_stop_loss_dollars,\n",
    "\n",
    "                        'session_stop_loss_hit_total': stop_loss_hit_total,\n",
    "                        'session_stop_loss_hit_ma': stop_loss_hit_ma,\n",
    "                        'session_stop_loss_hit_rsi': stop_loss_hit_rsi,\n",
    "                        'loss_prevention_cost_total': loss_prevention_cost_total,\n",
    "                        'loss_prevention_cost_ma': loss_prevention_cost_ma,\n",
    "                        'loss_prevention_cost_rsi': loss_prevention_cost_rsi,                        \n",
    "                        'loss_prevention_gain_total': loss_prevention_gain_total,\n",
    "                        'loss_prevention_gain_ma': loss_prevention_gain_ma,\n",
    "                        'loss_prevention_gain_rsi': loss_prevention_gain_rsi,\n",
    "\n",
    "                        \"session_stop_loss_hit_total_sub_comms\": stop_loss_hit_total_sub_comms,\n",
    "                        \"session_stop_loss_hit_ma_sub_comms\": stop_loss_hit_ma_sub_comms,\n",
    "                        \"session_stop_loss_hit_rsi_sub_comms\": stop_loss_hit_rsi_sub_comms,\n",
    "                        \"loss_prevention_cost_total_sub_comms\": loss_prevention_cost_total_sub_comms,\n",
    "                        \"loss_prevention_cost_ma_sub_comms\": loss_prevention_cost_ma_sub_comms,\n",
    "                        \"loss_prevention_cost_rsi_sub_comms\": loss_prevention_cost_rsi_sub_comms,\n",
    "                        \"loss_prevention_gain_total_sub_comms\": loss_prevention_gain_total_sub_comms,\n",
    "                        \"loss_prevention_gain_ma_sub_comms\": loss_prevention_gain_ma_sub_comms,\n",
    "                        \"loss_prevention_gain_rsi_sub_comms\": loss_prevention_gain_rsi_sub_comms,\n",
    "\n",
    "                        'total_dollar_pnl_stop_loss_adjusted': total_dollar_pnl_stop_loss_adjusted,\n",
    "                        'ma_dollar_pnl_stop_loss_adjusted': ma_dollar_pnl_stop_loss_adjusted,\n",
    "                        'rsi_dollar_pnl_stop_loss_adjusted': rsi_dollar_pnl_stop_loss_adjusted,\n",
    "                        'total_dollar_pnl_sub_comms_stop_loss_adjusted': total_dollar_pnl_sub_comms_stop_loss_adjusted,\n",
    "                        'ma_dollar_pnl_sub_comms_stop_loss_adjusted': ma_dollar_pnl_sub_comms_stop_loss_adjusted,\n",
    "                        'rsi_dollar_pnl_sub_comms_stop_loss_adjusted': rsi_dollar_pnl_sub_comms_stop_loss_adjusted\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {ticker} - {session_name} - {compression_name}: {e}\")\n",
    "\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    return pd.DataFrame(pnl_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(compressed_sessions[ticker_view][selected_session_view][compression_view][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `generate_pnl_dataframe` to create `final_pnl_pdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define your periods\n",
    "# ma_periods = [1, 3, 5]\n",
    "# ma_combinations = [\n",
    "#     (f'wma_{wma}', f'sma_{sma}', f'rsi_{wma}')\n",
    "#     for wma in ma_periods\n",
    "#     for sma in ma_periods\n",
    "#     if wma <= sma\n",
    "# ]\n",
    "\n",
    "strategy_types = [\"trend\", \"reversion\"]\n",
    "order_types = [\"market\", \"limit\"]\n",
    "directional_biases = [\"long\"]\n",
    "\n",
    "# Generate PnL DataFrame for each full combination\n",
    "daily_stop_loss_values = [-10000, -7500, -5000, -2500, -1000, -750, -500, -250, -100, -50, -25]\n",
    "\n",
    "pnl_dataframes = []\n",
    "\n",
    "for stop_val in daily_stop_loss_values:\n",
    "    for (sig_ma, con_ma, rsi_col), strategy, order_type, directional_bias in product(\n",
    "        ma_combinations, strategy_types, order_types, directional_biases\n",
    "    ):\n",
    "        df = generate_pnl_dataframe(\n",
    "            compressed_sessions,\n",
    "            ticker_to_tick_size,\n",
    "            ticker_to_point_value,\n",
    "            ma_name1=sig_ma,\n",
    "            ma_name2=con_ma,\n",
    "            rsi_column=rsi_col,\n",
    "            strategy_type=strategy,\n",
    "            order_type=order_type,\n",
    "            directional_bias=directional_bias,\n",
    "            daily_stop_loss_dollars=stop_val\n",
    "        )\n",
    "        pnl_dataframes.append(df)\n",
    "\n",
    "# Combine into one big dataframe\n",
    "final_pnl_df = pd.concat(pnl_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions about `final_pnl_df`:\n",
    "1. What percentage of sessions have a max_dollar_gain that is above its max_dollar_loss per ticker per \n",
    "2. Print point and dollar pnl next to commissions to make sure their being subtracted- probably not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pnl_df#[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the sums of the several loss_prevention columns and how does the gain compare with the loss across all configurations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURE YOUR TARGET VALUE ===\n",
    "target_stop_loss_value = -1000\n",
    "\n",
    "# === APPLY FILTER ===\n",
    "filtered_final_pnl_df = final_pnl_df[final_pnl_df['daily_stop_loss_dollars'] == target_stop_loss_value]\n",
    "\n",
    "# === TOTAL Metrics ===\n",
    "# print(\"Unique values in 'session_stop_loss_hit_total':\", final_pnl_df['session_stop_loss_hit_total'].unique())\n",
    "# print(\"Unique values in 'loss_prevention_cost_total':\", final_pnl_df['loss_prevention_cost_total'].unique())\n",
    "# print(\"Unique values in 'loss_prevention_gain_total':\", final_pnl_df['loss_prevention_gain_total'].unique())\n",
    "print(f\"--- Metrics for daily_stop_loss_dollars = {target_stop_loss_value} ---\\n\")\n",
    "\n",
    "print(\"Total 'loss_prevention_cost_total':\", filtered_final_pnl_df['loss_prevention_cost_total'].sum())\n",
    "print(\"Total 'loss_prevention_gain_total':\", filtered_final_pnl_df['loss_prevention_gain_total'].sum())\n",
    "print()\n",
    "print(\"Total 'loss_prevention_cost_total_sub_comms':\", filtered_final_pnl_df['loss_prevention_cost_total_sub_comms'].sum())\n",
    "print(\"Total 'loss_prevention_gain_total_sub_comms':\", filtered_final_pnl_df['loss_prevention_gain_total_sub_comms'].sum())\n",
    "print()\n",
    "print(\"Total 'loss_prevention_cost_ma':\", filtered_final_pnl_df['loss_prevention_cost_ma'].sum())\n",
    "print(\"Total 'loss_prevention_gain_ma':\", filtered_final_pnl_df['loss_prevention_gain_ma'].sum())\n",
    "print()\n",
    "print(\"Total 'loss_prevention_cost_ma_sub_comms':\", filtered_final_pnl_df['loss_prevention_cost_ma_sub_comms'].sum())\n",
    "print(\"Total 'loss_prevention_gain_ma_sub_comms':\", filtered_final_pnl_df['loss_prevention_gain_ma_sub_comms'].sum())\n",
    "print()\n",
    "print(\"Total 'loss_prevention_cost_rsi':\", filtered_final_pnl_df['loss_prevention_cost_rsi'].sum())\n",
    "print(\"Total 'loss_prevention_gain_rsi':\", filtered_final_pnl_df['loss_prevention_gain_rsi'].sum())\n",
    "print()\n",
    "print(\"Total 'loss_prevention_cost_rsi_sub_comms':\", filtered_final_pnl_df['loss_prevention_cost_rsi_sub_comms'].sum())\n",
    "print(\"Total 'loss_prevention_gain_rsi_sub_comms':\", filtered_final_pnl_df['loss_prevention_gain_rsi_sub_comms'].sum())\n",
    "print()\n",
    "\n",
    "# === MA Metrics ===\n",
    "# print(\"Unique values in 'session_stop_loss_hit_ma':\", final_pnl_df['session_stop_loss_hit_ma'].unique())\n",
    "# print(\"Unique values in 'loss_prevention_cost_ma':\", final_pnl_df['loss_prevention_cost_ma'].unique())\n",
    "# print(\"Unique values in 'loss_prevention_gain_ma':\", final_pnl_df['loss_prevention_gain_ma'].unique())\n",
    "\n",
    "# === RSI Metrics ===\n",
    "# print(\"Unique values in 'session_stop_loss_hit_rsi':\", final_pnl_df['session_stop_loss_hit_rsi'].unique())\n",
    "# print(\"Unique values in 'loss_prevention_cost_rsi':\", final_pnl_df['loss_prevention_cost_rsi'].unique())\n",
    "# print(\"Unique values in 'loss_prevention_gain_rsi':\", final_pnl_df['loss_prevention_gain_rsi'].unique())\n",
    "\n",
    "# === General Stats ===\n",
    "print(\"Number of rows in filtered_final_pnl_df:\", len(filtered_final_pnl_df))\n",
    "print(\"Number of non-integers in total_dollar_pnl:\", (filtered_final_pnl_df['total_dollar_pnl'] % 1 != 0).sum())\n",
    "print()\n",
    "\n",
    "# === Preview Slice ===\n",
    "row_start = 7300\n",
    "display(final_pnl_df[[  \n",
    "    'ticker', \n",
    "    # 'session', \n",
    "    'compression_factor', 'ma_period1', 'ma_period2', 'rsi_period', 'strategy_type', 'order_type', 'directional_bias',\n",
    "    # === TOTAL ===\n",
    "    'session_stop_loss_hit_total', 'total_max_dollar_loss', 'total_dollar_pnl', 'total_dollar_pnl_stop_loss_adjusted', 'daily_stop_loss_dollars', 'loss_prevention_cost_total', 'loss_prevention_gain_total',\n",
    "    'session_stop_loss_hit_total_sub_comms', 'total_max_dollar_loss_sub_comms', 'total_dollar_pnl_sub_comms', 'total_dollar_pnl_sub_comms_stop_loss_adjusted', 'loss_prevention_cost_total_sub_comms', 'loss_prevention_gain_total_sub_comms',\n",
    "    # === MA ===\n",
    "    'session_stop_loss_hit_ma', 'ma_max_dollar_loss', 'ma_dollar_pnl', 'ma_dollar_pnl_stop_loss_adjusted', 'daily_stop_loss_dollars', 'loss_prevention_cost_ma', 'loss_prevention_gain_ma',\n",
    "    'session_stop_loss_hit_ma_sub_comms', 'ma_max_dollar_loss_sub_comms', 'ma_dollar_pnl_sub_comms', 'ma_dollar_pnl_sub_comms_stop_loss_adjusted', 'loss_prevention_cost_ma_sub_comms', 'loss_prevention_gain_ma_sub_comms',\n",
    "    # === RSI ===\n",
    "    'session_stop_loss_hit_rsi', 'rsi_max_dollar_loss', 'rsi_dollar_pnl', 'daily_stop_loss_dollars', 'loss_prevention_cost_rsi', 'loss_prevention_gain_rsi',\n",
    "    'session_stop_loss_hit_rsi_sub_comms', 'rsi_max_dollar_loss_sub_comms', 'rsi_dollar_pnl_sub_comms', 'loss_prevention_cost_rsi_sub_comms', 'loss_prevention_gain_rsi_sub_comms'\n",
    "]][row_start:row_start+60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing unique combinations of ma window length in `final_pnl_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_combinations = filtered_final_pnl_df[['ma_period1', 'ma_period2', 'rsi_period']].drop_duplicates().values.tolist()\n",
    "combo_counts = filtered_final_pnl_df.groupby(['ma_period1', 'ma_period2', 'rsi_period']).size().reset_index(name='count')\n",
    "combo_dict = combo_counts.set_index(['ma_period1', 'ma_period2', 'rsi_period'])['count'].to_dict()\n",
    "display(combo_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 style=\"color:yellow;\">Starting Analytics/Visualizations</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping by all iterables of each compression_factor of each ticker on pnl metrics to see which configs are most profitable within final_pnl_df\n",
    "- Issues: \n",
    "- I should group by session also \n",
    "- Find out if commission is being subtracted upstream. Potential problem functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options\n",
    "pd.set_option('display.max_colwidth', None)   # Allow columns to expand to full width\n",
    "pd.set_option('display.width', 0)             # Let pandas auto-detect width based on terminal\n",
    "# pd.set_option('display.max_rows', 100)        # Adjust this if you want more rows visible\n",
    "\n",
    "# Group and aggregate\n",
    "grouped = final_pnl_df.groupby([\n",
    "    'ticker', \n",
    "    # 'session', # comment out here to exclude session\n",
    "    'compression_factor', \n",
    "    'strategy_type', \n",
    "    'order_type', \n",
    "    'directional_bias',\n",
    "    'daily_stop_loss_dollars',\n",
    "    'ma_period1', \n",
    "    'ma_period2', \n",
    "    'rsi_period'\n",
    "])\n",
    "\n",
    "# Aggregate total, MA, and RSI PnL (sum and mean)\n",
    "agg_df = grouped[['total_dollar_pnl', 'total_dollar_pnl_stop_loss_adjusted', 'total_dollar_pnl_sub_comms_stop_loss_adjusted', \n",
    "                  'ma_dollar_pnl', 'ma_dollar_pnl_stop_loss_adjusted', 'ma_dollar_pnl_sub_comms_stop_loss_adjusted',\n",
    "                  'rsi_dollar_pnl', 'rsi_dollar_pnl_stop_loss_adjusted', 'rsi_dollar_pnl_sub_comms_stop_loss_adjusted',\n",
    "                  ]].agg(['sum', 'mean']).reset_index()\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "agg_df.columns = [\n",
    "    'ticker', \n",
    "    # 'session',\n",
    "    'compression_factor', 'strategy_type', 'order_type', 'directional_bias', 'daily_stop_loss_dollars',\n",
    "    'ma_period1', 'ma_period2', 'rsi_period', \n",
    "    'total_dollar_pnl_sum', 'total_dollar_pnl_mean',\n",
    "    'total_dollar_pnl_stop_loss_adjusted_sum', 'total_dollar_pnl_stop_loss_adjusted_mean',\n",
    "    'total_dollar_pnl_sub_comms_stop_loss_adjusted_sum', 'total_dollar_pnl_sub_comms_stop_loss_adjusted_mean',\n",
    "    'ma_dollar_pnl_sum', 'ma_dollar_pnl_mean',\n",
    "    'ma_dollar_pnl_stop_loss_adjusted_sum', 'ma_dollar_pnl_stop_loss_adjusted_mean',\n",
    "    'ma_dollar_pnl_sub_comms_stop_loss_adjusted_sum', 'ma_dollar_pnl_sub_comms_stop_loss_adjusted_mean',\n",
    "    'rsi_dollar_pnl_sum', 'rsi_dollar_pnl_mean',\n",
    "    'rsi_dollar_pnl_stop_loss_adjusted_sum', 'rsi_dollar_pnl_stop_loss_adjusted_mean',\n",
    "    'rsi_dollar_pnl_sub_comms_stop_loss_adjusted_sum', 'rsi_dollar_pnl_sub_comms_stop_loss_adjusted_mean'\n",
    "]\n",
    "\n",
    "# Top 10 best-performing parameter combinations\n",
    "top_combinations = agg_df.sort_values('total_dollar_pnl_sub_comms_stop_loss_adjusted_sum', ascending=False)\n",
    "print(\"If sums and means are printing the same, you are including session in the groupby. To exclude session, comment out the designated lines in the `grouped` variable and where `agg_df.columns` is created.\")\n",
    "display(top_combinations#[:25]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_final_pnl_df(df, ticker, compression_factor, ma_period1, ma_period2, rsi_period, strategy_type, order_type, directional_bias, daily_stop_loss_dollars):\n",
    "    return df[\n",
    "        (df['ticker'] == ticker) &\n",
    "        (df['compression_factor'] == compression_factor) &\n",
    "        (df['ma_period1'] == ma_period1) &\n",
    "        (df['ma_period2'] == ma_period2) &\n",
    "        (df['rsi_period'] == rsi_period) &\n",
    "        (df['strategy_type'] == strategy_type) &\n",
    "        (df['order_type'] == order_type) &\n",
    "        (df['directional_bias'] == directional_bias) &\n",
    "        (df['daily_stop_loss_dollars'] == daily_stop_loss_dollars)\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "sliced_df = slice_final_pnl_df(\n",
    "    final_pnl_df,\n",
    "    ticker='/NQ',\n",
    "    compression_factor=5,\n",
    "    ma_period1=3,\n",
    "    ma_period2=5,\n",
    "    rsi_period=3,\n",
    "    strategy_type='reversion',\n",
    "    order_type='limit',\n",
    "    directional_bias='long',\n",
    "    daily_stop_loss_dollars=-25\n",
    ")\n",
    "\n",
    "display(sliced_df)\n",
    "\n",
    "def plot_total_pnl_variants(df, \n",
    "                            dollar_pnl_col='total_dollar_pnl', \n",
    "                            dollar_pnl_sub_comms_col='total_dollar_pnl_sub_comms',\n",
    "                            dollar_pnl_stop_loss_adjusted_col='total_dollar_pnl_stop_loss_adjusted',\n",
    "                            dollar_pnl_sub_comms_stop_loss_adjusted_col='total_dollar_pnl_sub_comms_stop_loss_adjusted'):\n",
    "    # Reset matplotlib settings to default\n",
    "    plt.rcdefaults()\n",
    "    plt.figure(figsize=(18, 9))\n",
    "\n",
    "    plt.plot(df.index, df[dollar_pnl_col].cumsum(), label='Cumulative Raw PnL')\n",
    "    plt.plot(df.index, df[dollar_pnl_sub_comms_col].cumsum(), label='Cumulative Raw PnL - Commissions')\n",
    "    plt.plot(df.index, df[dollar_pnl_stop_loss_adjusted_col].cumsum(), label='Cumulative PnL + Stop Loss Adj')\n",
    "    plt.plot(df.index, df[dollar_pnl_sub_comms_stop_loss_adjusted_col].cumsum(), label='Cumulative PnL - Comms + Stop Loss Adj')\n",
    "\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Cumulative Dollar PnL')\n",
    "    plt.title(f'Cumulative Total Dollar PnL Variants {dollar_pnl_col}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_total_pnl_variants(sliced_df, \n",
    "                        dollar_pnl_col='total_dollar_pnl', \n",
    "                        dollar_pnl_sub_comms_col='total_dollar_pnl_sub_comms',\n",
    "                        dollar_pnl_stop_loss_adjusted_col='total_dollar_pnl_stop_loss_adjusted',\n",
    "                        dollar_pnl_sub_comms_stop_loss_adjusted_col='total_dollar_pnl_sub_comms_stop_loss_adjusted')\n",
    "\n",
    "plot_total_pnl_variants(sliced_df, \n",
    "                        dollar_pnl_col='ma_dollar_pnl', \n",
    "                        dollar_pnl_sub_comms_col='ma_dollar_pnl_sub_comms',\n",
    "                        dollar_pnl_stop_loss_adjusted_col='ma_dollar_pnl_stop_loss_adjusted',\n",
    "                        dollar_pnl_sub_comms_stop_loss_adjusted_col='ma_dollar_pnl_sub_comms_stop_loss_adjusted')\n",
    "\n",
    "plot_total_pnl_variants(sliced_df, \n",
    "                        dollar_pnl_col='rsi_dollar_pnl', \n",
    "                        dollar_pnl_sub_comms_col='rsi_dollar_pnl_sub_comms',\n",
    "                        dollar_pnl_stop_loss_adjusted_col='rsi_dollar_pnl_stop_loss_adjusted',\n",
    "                        dollar_pnl_sub_comms_stop_loss_adjusted_col='rsi_dollar_pnl_sub_comms_stop_loss_adjusted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pnl_by_stop_loss(df, \n",
    "                          ticker, \n",
    "                          compression_factor, \n",
    "                          ma_period1, \n",
    "                          ma_period2, \n",
    "                          rsi_period, \n",
    "                          strategy_type, \n",
    "                          order_type, \n",
    "                          directional_bias, \n",
    "                          stop_loss_values,\n",
    "                          pnl_column='total_dollar_pnl_sub_comms_stop_loss_adjusted'):\n",
    "    \"\"\"\n",
    "    Plot cumulative PnL curves for different stop loss values with rainbow color mapping.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The full final_pnl_df.\n",
    "    - ticker, compression_factor, ma_period1, ma_period2, rsi_period, strategy_type, order_type, directional_bias: Slicing parameters.\n",
    "    - stop_loss_values (list): A list of daily_stop_loss_dollars values to plot.\n",
    "    - pnl_column (str): The column to plot cumulatively.\n",
    "    \"\"\"\n",
    "    # Reset matplotlib to default\n",
    "    plt.rcdefaults()\n",
    "    plt.figure(figsize=(18, 9))\n",
    "\n",
    "    # Generate rainbow colors\n",
    "    cmap = plt.cm.rainbow\n",
    "    colors = [cmap(i / max(len(stop_loss_values)-1, 1)) for i in range(len(stop_loss_values))]\n",
    "\n",
    "    for i, stop_loss in enumerate(stop_loss_values):\n",
    "        sliced = df[\n",
    "            (df['ticker'] == ticker) &\n",
    "            (df['compression_factor'] == compression_factor) &\n",
    "            (df['ma_period1'] == ma_period1) &\n",
    "            (df['ma_period2'] == ma_period2) &\n",
    "            (df['rsi_period'] == rsi_period) &\n",
    "            (df['strategy_type'] == strategy_type) &\n",
    "            (df['order_type'] == order_type) &\n",
    "            (df['directional_bias'] == directional_bias) &\n",
    "            (df['daily_stop_loss_dollars'] == stop_loss)\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "        if not sliced.empty:\n",
    "            plt.plot(sliced.index, sliced[pnl_column].cumsum(), label=f'Session SL = {stop_loss}', color=colors[i])\n",
    "    \n",
    "    plt.xlabel('Chronological Sessions')\n",
    "    plt.ylabel('Cumulative PnL (Sub Comms + Stop Loss Adj)')\n",
    "    plt.title(f'Cumulative PnL vs Stop Loss  {ticker}, MA({ma_period1},{ma_period2}), RSI({rsi_period})')\n",
    "    plt.legend(title='Stop Loss ($)', fontsize=10)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_pnl_by_stop_loss(\n",
    "    final_pnl_df,\n",
    "    ticker='/NQ',\n",
    "    compression_factor=5,\n",
    "    ma_period1=3,\n",
    "    ma_period2=5,\n",
    "    rsi_period=3,\n",
    "    strategy_type='reversion',\n",
    "    order_type='limit',\n",
    "    directional_bias='long',\n",
    "    stop_loss_values=daily_stop_loss_values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining updated viz functions to include all iterables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trading_strategies_2(candles, \n",
    "                           ma_name1='wma_5', \n",
    "                           ma_name2='sma_5', \n",
    "                           rsi_column='rsi_5',  \n",
    "                           figsize=(40, 20), \n",
    "                           font_size=10, \n",
    "                           ma_markersize=50, \n",
    "                           signal_markersize_y=400, \n",
    "                           signal_markersize_b=250,\n",
    "                           strategy_type='trend',\n",
    "                           order_type='market',\n",
    "                           directional_bias='long'\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Plots the minute_candles DataFrame with two selected moving averages and optional RSI.\n",
    "    Also plots cumulative profit for MA and RSI strategies on a secondary axis.\n",
    "\n",
    "    Parameters:\n",
    "    - ma_name1 (str): The column name of the first moving average to plot.\n",
    "    - ma_name2 (str): The column name of the second moving average to plot.\n",
    "    - signal_column (str): The column name of the signal data (default is 'signal').\n",
    "    - figsize (tuple): The size of the plot (width, height) in inches (default is (30, 20)).\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Clean the data to ensure numeric columns are valid\n",
    "        columns_to_convert = ['open', 'high', 'low', 'close', 'volume', ma_name1, ma_name2, rsi_column] \n",
    "        candles[columns_to_convert] = candles[columns_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "        # Generate dynamic column names for PnL and signals\n",
    "        ma_entry_price = f'entry_price_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "        ma_exit_price = f'exit_price_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "        rsi_entry_price = f'entry_price_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "        rsi_exit_price = f'exit_price_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "        cum_pnl_ma_col = f'cum_pnl_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "        cum_pnl_rsi_col = f'cum_pnl_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "        cum_pnl_all_col = f'cum_pnl_all_{ma_name1}_{ma_name2}_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "        stop_loss_ma = f'stop_loss_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "        stop_loss_rsi = f'stop_loss_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "\n",
    "        # Select the columns to plot\n",
    "        plot_data = candles[['datetime', 'open', 'high', 'low', 'close', 'volume', \n",
    "                             ma_name1, ma_name2, rsi_column, \n",
    "                             ma_entry_price, ma_exit_price, rsi_entry_price, rsi_exit_price,\n",
    "                             cum_pnl_ma_col, cum_pnl_rsi_col, cum_pnl_all_col,\n",
    "                             stop_loss_ma, stop_loss_rsi]].copy() # here\n",
    "        plot_data.set_index('datetime', inplace=True)\n",
    "\n",
    "        # Create the additional plots for the moving averages and RSI, but only if they are warmed up\n",
    "        add_plots = []\n",
    "\n",
    "        # Check if the moving averages have enough valid data to plot\n",
    "        if not candles[ma_name1].isnull().all() and not candles[ma_name2].isnull().all():\n",
    "            add_plots.append(mpf.make_addplot(plot_data[ma_name1], color='yellow', type='scatter', marker='o', markersize=ma_markersize, label=f'{ma_name1}'))\n",
    "            add_plots.append(mpf.make_addplot(plot_data[ma_name1], color='yellow', linestyle='-', width=0.75))\n",
    "            add_plots.append(mpf.make_addplot(plot_data[ma_name2], color='purple', type='scatter', marker='o', markersize=ma_markersize, label=f'{ma_name2}'))\n",
    "            add_plots.append(mpf.make_addplot(plot_data[ma_name2], color='purple', linestyle='-', width=0.75))\n",
    "        else:\n",
    "            print(\"Moving averages have not warmed up yet. Plotting without them.\")\n",
    "\n",
    "        # Check if the RSI has enough valid data to plot\n",
    "        if not candles[rsi_column].isnull().all():\n",
    "            add_plots.append(mpf.make_addplot(candles[rsi_column], panel=2, color='blue', type='scatter', marker='o', markersize=ma_markersize, label='RSI'))\n",
    "            add_plots.append(mpf.make_addplot(candles[rsi_column], panel=2, color='blue', linestyle='-', width=0.75))\n",
    "            add_plots.append(mpf.make_addplot(candles['trend_indicator'], panel=2, color='white', type='scatter', marker='o', markersize=ma_markersize, label='RSI'))\n",
    "            add_plots.append(mpf.make_addplot(candles['trend_indicator'], panel=2, color='white', linestyle='-', width=0.75))\n",
    "            add_plots.append(mpf.make_addplot(candles['hundred_line'], panel=2, color='red', linestyle=':', secondary_y=False))\n",
    "            add_plots.append(mpf.make_addplot(candles['fifty_line'], panel=2, color='yellow', linestyle=':', secondary_y=False))\n",
    "            add_plots.append(mpf.make_addplot(candles['zero_line'], panel=2, color='green', linestyle=':', secondary_y=False))\n",
    "            add_plots.append(mpf.make_addplot(candles['trend_high_threshold'], panel=2, color='white', linestyle=':', secondary_y=False))\n",
    "            add_plots.append(mpf.make_addplot(candles['trend_low_threshold'], panel=2, color='white', linestyle=':', secondary_y=False))\n",
    "        else:\n",
    "            print(\"RSI has not warmed up yet. Plotting without it.\")\n",
    "\n",
    "        # Add buy, sell, and neutral markers if signal_column exists. Eliminate the if else statement to revert to working order\n",
    "        if ma_entry_price in candles.columns and ma_exit_price in candles.columns:\n",
    "            add_plots.append(mpf.make_addplot(candles[ma_entry_price], type='scatter', marker='^', markersize=signal_markersize_y, color='yellow', panel=0, secondary_y=False))\n",
    "            add_plots.append(mpf.make_addplot(candles[ma_exit_price], type='scatter', marker='o', markersize=signal_markersize_y, color='yellow', panel=0, secondary_y=False))\n",
    "        else:\n",
    "            print(\"Buy/Sell markers for MA strat have not warmed up yet. Plotting without them.\")\n",
    "\n",
    "        # Add buy, sell, and neutral markers for RSI strategy\n",
    "        if rsi_entry_price in candles.columns and rsi_exit_price in candles.columns:\n",
    "            add_plots.append(mpf.make_addplot(candles[rsi_entry_price], type='scatter', marker='^', markersize=signal_markersize_b, color='blue', panel=0, secondary_y=False))\n",
    "            add_plots.append(mpf.make_addplot(candles[rsi_exit_price], type='scatter', marker='o', markersize=signal_markersize_b, color='blue', panel=0, secondary_y=False))\n",
    "        else:\n",
    "            print(\"Buy/Sell markers for RSI strat have not warmed up yet. Plotting without them.\")\n",
    "\n",
    "        # Add cumulative profit plots on a secondary y-axis with dynamic names\n",
    "        add_plots.append(mpf.make_addplot(candles[cum_pnl_ma_col], panel=0, color='yellow', secondary_y=True, label=f'Cumulative PnL (MA: {ma_name1}_{ma_name2})', linestyle='-', width=1.25))\n",
    "        add_plots.append(mpf.make_addplot(candles[cum_pnl_rsi_col], panel=0, color='blue', secondary_y=True, label=f'Cumulative PnL (RSI: {rsi_column})', linestyle='-', width=1.25))\n",
    "        add_plots.append(mpf.make_addplot(candles[cum_pnl_all_col], panel=0, color='green', secondary_y=True, label=f'Cumulative PnL (Combined)', linestyle='-', width=1.25))\n",
    "\n",
    "        # Add stop-loss markers (x) for both MA and RSI strategies\n",
    "        # if 'stop_loss_ma' in candles.columns:\n",
    "        add_plots.append(mpf.make_addplot(candles[stop_loss_ma], type='scatter', marker='x', markersize=100, color='yellow', panel=0, secondary_y=False))\n",
    "        # else:\n",
    "        #     print(\"There are no stop loss markers for MA strat\")\n",
    "        # if 'stop_loss_rsi' in candles.columns:\n",
    "        add_plots.append(mpf.make_addplot(candles[stop_loss_rsi], type='scatter', marker='x', markersize=50, color='blue', panel=0, secondary_y=False))\n",
    "        # else:\n",
    "        #     print(\"There are no stop loss markers for RSI strat\")\n",
    "\n",
    "        # Add price action envelope as white lines\n",
    "        if 'price_action_upper' in candles.columns and 'price_action_lower' in candles.columns:\n",
    "            add_plots.append(mpf.make_addplot(candles['price_action_upper'], color='white', linestyle='-', width=0.5, label='Price Action Upper'))\n",
    "            add_plots.append(mpf.make_addplot(candles['price_action_lower'], color='white', linestyle='-', width=0.5, label='Price Action Lower'))\n",
    "            # add_plots.append(mpf.make_addplot(candles['ma_price_action_upper'], color='white', linestyle='-', width=0.5, label='Price Action Upper'))\n",
    "            # add_plots.append(mpf.make_addplot(candles['ma_price_action_lower'], color='white', linestyle='-', width=0.5, label='Price Action Lower'))\n",
    "        else:\n",
    "            print(\"Price action envelope not calculating properly\")\n",
    "\n",
    "        # Create a custom style with a black background\n",
    "        black_style = mpf.make_mpf_style(\n",
    "            base_mpf_style='charles',  # Start with the 'charles' style and modify it\n",
    "            facecolor='black',         # Set the background color to black\n",
    "            gridcolor='black',          # Set the grid line color\n",
    "            edgecolor='purple',          # Set the edge color for candles and boxes\n",
    "            figcolor='black',          # Set the figure background color to black\n",
    "            rc={'axes.labelcolor': 'yellow', \n",
    "                'xtick.color': 'yellow', \n",
    "                'ytick.color': 'yellow', \n",
    "                'axes.titlecolor': 'yellow',\n",
    "                'font.size': font_size, \n",
    "                'axes.labelsize': font_size,\n",
    "                'axes.titlesize': font_size,\n",
    "                'xtick.labelsize': font_size,\n",
    "                'ytick.labelsize': font_size,\n",
    "                'legend.fontsize': font_size}  # Set tick and label colors to white\n",
    "        )\n",
    "\n",
    "        # Plot using mplfinance\n",
    "        mpf.plot(plot_data, type='candle', style=black_style, \n",
    "                title='',\n",
    "                ylabel='Price', \n",
    "                addplot=add_plots, \n",
    "                figsize=figsize,\n",
    "                volume=True,\n",
    "                panel_ratios=(8, 2),\n",
    "                #  panel_ratios=(8, 2, 2),             \n",
    "                tight_layout=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Something wrong in the plotting_moving_averages function: {e}\")\n",
    "\n",
    "def visualize_trades_2(candles, ticker_to_tick_size, ticker_to_point_value, ma_name1='wma_5', ma_name2='sma_5',\n",
    "                        rsi_column='rsi_5', lower_slice=0, upper_slice=-1, compression_factor=1, session_key=\"\", strategy_type=\"trend\", order_type=\"market\", directional_bias='long'):\n",
    "    \"\"\"\n",
    "    Visualize trades and print summary statistics, including tick size for each ticker.\n",
    "\n",
    "    Parameters:\n",
    "    - candles (dict): Dictionary of DataFrames with candle data for each ticker.\n",
    "    - ticker_to_tick_size (dict): Dictionary mapping tickers to their respective tick sizes.\n",
    "    - ticker_to_point_value (dict): Dictionary mapping tickers to their respective point values.\n",
    "    - ma_name1, ma_name2, rsi_column: Names of MA and RSI columns.\n",
    "    - lower_slice, upper_slice: Range of rows to visualize.\n",
    "    \"\"\"\n",
    "    # Generate dynamic column names for PnL and trade metrics\n",
    "    pnl_ma_col = f'pnl_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    pnl_rsi_col = f'pnl_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    cum_pnl_ma_col = f'cum_{pnl_ma_col}'\n",
    "    cum_pnl_rsi_col = f'cum_{pnl_rsi_col}'\n",
    "    cum_pnl_all_col = f'cum_pnl_all_{ma_name1}_{ma_name2}_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    ma_entry_price = f'entry_price_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    ma_exit_price = f'exit_price_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_entry_price = f'entry_price_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_exit_price = f'exit_price_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    ma_commission_col = f'commission_cost_{ma_name1}_{ma_name2}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "    rsi_commission_col = f'commission_cost_{rsi_column}_{strategy_type}_{order_type}_{directional_bias}'\n",
    "\n",
    "    # Variable to accumulate total dollar PnL\n",
    "    total_dollar_pnl_sum = 0.0\n",
    "\n",
    "    # Iterate through the candles dictionary\n",
    "    for ticker, minute_candles_df in candles.items():\n",
    "        # Create a copy of the DataFrame for the specified slice\n",
    "        minute_candles_viz_1 = minute_candles_df[lower_slice:upper_slice].copy()\n",
    "        tick_size = ticker_to_tick_size.get(ticker, \"Unknown\")  # Retrieve tick size or default to \"Unknown\"\n",
    "        point_value = ticker_to_point_value.get(ticker, 1)  # Retrieve point value or default to 1\n",
    "\n",
    "        try:\n",
    "            # Plot moving averages\n",
    "            plot_trading_strategies_2(\n",
    "                minute_candles_viz_1,\n",
    "                ma_name1=ma_name1, ma_name2=ma_name2, rsi_column=rsi_column,\n",
    "                figsize=(40, 20), font_size=20,\n",
    "                ma_markersize=50, signal_markersize_y=450, signal_markersize_b=300, \n",
    "                strategy_type=strategy_type, order_type=order_type, directional_bias=directional_bias\n",
    "            )\n",
    "            \n",
    "            # Calculate cumulative PnL and other statistics\n",
    "            ma_pnl = round(minute_candles_df[cum_pnl_ma_col].iloc[-1], 3)\n",
    "            rsi_pnl = round(minute_candles_df[cum_pnl_rsi_col].iloc[-1], 3)\n",
    "            total_pnl = round(minute_candles_df[cum_pnl_all_col].iloc[-1], 3)\n",
    "            close_price_diff = round(minute_candles_df[\"close\"].iloc[-1] - minute_candles_df[\"close\"].iloc[0], 3)\n",
    "            point_alpha = round(total_pnl - close_price_diff, 3)\n",
    "\n",
    "            # Retrieve total commission costs\n",
    "            ma_commission_total = round(minute_candles_df[ma_commission_col].iloc[-1], 3) if ma_commission_col in minute_candles_df else 0.0\n",
    "            rsi_commission_total = round(minute_candles_df[rsi_commission_col].iloc[-1], 3) if rsi_commission_col in minute_candles_df else 0.0\n",
    "            total_commission_cost = ma_commission_total + rsi_commission_total\n",
    "\n",
    "            # Calculate total dollar PnLs\n",
    "            ma_dollar_pnl = (ma_pnl * point_value) - ma_commission_total\n",
    "            rsi_dollar_pnl = (rsi_pnl * point_value) - rsi_commission_total\n",
    "            total_dollar_pnl = (total_pnl * point_value) - total_commission_cost\n",
    "            total_dollar_pnl_sum += total_dollar_pnl\n",
    "            close_price_dollar_diff = close_price_diff * point_value\n",
    "            dollar_alpha = (point_alpha * point_value) - total_commission_cost\n",
    "\n",
    "            # Count the number of trades for MA and RSI strategies\n",
    "            ma_trades = (minute_candles_df[ma_exit_price].notna().sum() + minute_candles_df[ma_entry_price].notna().sum())/2\n",
    "            rsi_trades = (minute_candles_df[rsi_exit_price].notna().sum() + minute_candles_df[rsi_entry_price].notna().sum())/2\n",
    "            total_trades = ma_trades + rsi_trades\n",
    "\n",
    "            # Calculate max gain and max loss for MA, RSI, and total strategies\n",
    "            ma_max_gain = round(minute_candles_df[cum_pnl_ma_col].max(), 3)\n",
    "            ma_max_loss = round(minute_candles_df[cum_pnl_ma_col].min(), 3)\n",
    "            rsi_max_gain = round(minute_candles_df[cum_pnl_rsi_col].max(), 3)\n",
    "            rsi_max_loss = round(minute_candles_df[cum_pnl_rsi_col].min(), 3)\n",
    "            total_max_gain = round(minute_candles_df[cum_pnl_all_col].max(), 3)\n",
    "            total_max_loss = round(minute_candles_df[cum_pnl_all_col].min(), 3)\n",
    "            ma_max_dollar_gain = (ma_max_gain * point_value) - ma_commission_total\n",
    "            ma_max_dollar_loss = (ma_max_loss * point_value) - ma_commission_total\n",
    "            rsi_max_dollar_gain = (rsi_max_gain * point_value) - rsi_commission_total\n",
    "            rsi_max_dollar_loss = (rsi_max_loss * point_value) - rsi_commission_total\n",
    "            total_max_dollar_gain = (total_max_gain * point_value) - total_commission_cost\n",
    "            total_max_dollar_loss = (total_max_loss * point_value) - total_commission_cost\n",
    "\n",
    "            # Print detailed statistics for the ticker\n",
    "            print(f\"{ticker}: {compression_factor}-Minute Compression Factor, Strategy Type: {strategy_type}, Order Type: {order_type}, Directional Bias: {directional_bias}, Total PnL: {total_pnl:.2f}pt/${total_dollar_pnl:.2f}, {len(minute_candles_df)} rows, \"\n",
    "                f\"Session: {session_key}, {ma_name1}, {ma_name2}, {rsi_column}, \"                \n",
    "                f\"Total trades: {total_trades}, Total Commission Cost: ${total_commission_cost:.2f}, Total Max Gain: {total_max_gain:.2f}pt/${total_max_dollar_gain}, Total Max Loss: {total_max_loss:.2f}pt/${total_max_dollar_loss}, \"\n",
    "                f\"MA PnL: {ma_pnl:.2f}pt/${ma_dollar_pnl:.2f},MA trades: {ma_trades}, MA Commission Cost: ${ma_commission_total:.2f}, MA Max Gain: {ma_max_gain:.2f}pt/${ma_max_dollar_gain}, MA Max Loss: {ma_max_loss:.2f}pt/${ma_max_dollar_loss}, \"\n",
    "                f\"RSI PnL: {rsi_pnl:.2f}pt/${rsi_dollar_pnl:.2f}, RSI trades: {rsi_trades}, RSI Commission Cost: ${rsi_commission_total:.2f}, RSI Max Gain: {rsi_max_gain:.2f}pt/${rsi_max_dollar_gain}, RSI Max Loss: {rsi_max_loss:.2f}pt/${rsi_max_dollar_loss}, \"\n",
    "                f\"Close Price Difference: {close_price_diff:.2f}pt/${close_price_dollar_diff:.2f}, Alpha: {point_alpha:.2f}pt/${dollar_alpha:.2f}, Tick Size: {tick_size}, \"                \n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Handle any errors that occur during the plotting\n",
    "            print(f\"Error in visualize_trades_2 for {ticker}: {e}\")\n",
    "\n",
    "    # Return the total PnL for this ticker so it can be aggregated\n",
    "    return total_dollar_pnl_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing strategy against candles in the iteration order of `order_type`, then `strategy_type`, then `ticker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_session_index = 50  # 0 for first session, 1 for second, etc.\n",
    "chosen_compression_factor = 3  # Which compression factor to use\n",
    "chosen_compression_key = f'{chosen_compression_factor}_minute_compression' # Build the compression string dynamically\n",
    "upper_slice = -1  # Slice to the end of the DataFrame\n",
    "lower_slice = 0  # Slice from the beginning of the DataFrame\n",
    "_wma_rsi = '3'\n",
    "_sma = '3'\n",
    "wma = f'wma_{_wma_rsi}'\n",
    "sma = f'sma_{_sma}'\n",
    "rsi = f'rsi_{_wma_rsi}'\n",
    "\n",
    "# Separate tracking for trend and reversion PnL\n",
    "total_pnl_across_tickers_trend = 0.0\n",
    "total_pnl_across_tickers_reversion = 0.0\n",
    "\n",
    "for ticker, sessions in compressed_sessions.items():\n",
    "    # Get sorted session keys to create a numerical map\n",
    "    session_keys = list(sessions.keys())\n",
    "\n",
    "    if chosen_session_index >= len(session_keys):\n",
    "        print(f\"Ticker {ticker} only has {len(session_keys)} sessions. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Get the actual session key from numerical index\n",
    "    chosen_session = session_keys[chosen_session_index]\n",
    "\n",
    "    # Check if the chosen compression exists within this session\n",
    "    if chosen_compression_key not in sessions[chosen_session]:\n",
    "        print(f\"Ticker {ticker}, session {chosen_session} does not have compression {chosen_compression_key}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Grab the DataFrame for the chosen session and compression\n",
    "    df = sessions[chosen_session][chosen_compression_key]\n",
    "\n",
    "    for strategy_type in [\"trend\", \"reversion\"]: # Ensure both strategies are visualized sequentially\n",
    "        # print(f\"\\nVisualizing {strategy_type.upper()} strategy for {ticker} - Session: {chosen_session}\")\n",
    "\n",
    "        for order_type in [\"market\", \"limit\"]: # Ensure both order types are visualized sequentially\n",
    "\n",
    "            # for directional_bias in [\"long\"]:\n",
    "\n",
    "            ticker_pnl = visualize_trades_2(\n",
    "                candles={ticker: df},\n",
    "                ticker_to_tick_size=ticker_to_tick_size,\n",
    "                ticker_to_point_value=ticker_to_point_value,\n",
    "                ma_name1=wma,\n",
    "                ma_name2=sma,\n",
    "                rsi_column=rsi,\n",
    "                lower_slice=lower_slice,\n",
    "                upper_slice=upper_slice,\n",
    "                compression_factor=chosen_compression_factor,\n",
    "                session_key=chosen_session, # Pass the session key\n",
    "                strategy_type=strategy_type,\n",
    "                order_type=order_type,\n",
    "                directional_bias='long'\n",
    "            )\n",
    "\n",
    "            # Store PnL separately for each strategy\n",
    "            if strategy_type == \"trend\":\n",
    "                total_pnl_across_tickers_trend += ticker_pnl\n",
    "            else:\n",
    "                total_pnl_across_tickers_reversion += ticker_pnl\n",
    "\n",
    "    # Print final aggregated PnL for each strategy type\n",
    "    print(f\"\\nTotal Dollar PnL Across All Tickers for Trend Strategy (Session Index {chosen_session_index}): {total_pnl_across_tickers_trend:.2f}\")\n",
    "    print(f\"Total Dollar PnL Across All Tickers for Reversion Strategy (Session Index {chosen_session_index}): {total_pnl_across_tickers_reversion:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell might be kind of useless since a data frame is being generated with all of the data just below and it's much easier to read and make visualizations from. But this is calling `print_all_pnls` on every compression of every session of every ticker in the nested dictionary `compressed_sessions`. Also the function being called was not designed with this dictionary format in mind so it behaves incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ticker, sessions in compressed_sessions.items():\n",
    "#     print(f\"\\nTicker: {ticker}, Total Sessions: {len(sessions)}\")\n",
    "\n",
    "#     for session_name, compressions in sessions.items():\n",
    "#         print(f\"  Session: {session_name}, Total Compressions: {len(compressions)}\")\n",
    "\n",
    "#         for compression_name, session_df in compressions.items():\n",
    "#             # Extract the compression factor from the compression name (e.g., \"5_minute_compression\"  5)\n",
    "#             compression_factor = int(compression_name.split('_')[0])\n",
    "\n",
    "#             print(f\"\\n  {compression_factor}-Minute Compression PnL Figures for {ticker} - {session_name}\")\n",
    "\n",
    "#             # Call the print_all_pnls_1 function\n",
    "#             print_all_pnls_1(\n",
    "#                 candles={ticker: session_df},  # Pass the session's DataFrame inside a dict\n",
    "#                 compression_factor=compression_factor,  # Pass the extracted compression factor\n",
    "#                 ticker_to_tick_size=ticker_to_tick_size,  # Pass tick size mapping\n",
    "#                 ticker_to_point_value=ticker_to_point_value,  # Pass point value mapping\n",
    "#                 ma_name1='wma_5',\n",
    "#                 ma_name2='sma_5',\n",
    "#                 rsi_column='rsi_5'\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pnl_df#[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_pnl_distributions(\n",
    "#     final_pnl_df,\n",
    "#     strategy_type=\"trend\",\n",
    "#     order_type=\"market\",\n",
    "#     directional_bias=\"long\",\n",
    "#     ma_period1=None,\n",
    "#     ma_period2=None,\n",
    "#     rsi_period=None,\n",
    "#     ma_dollar_pnl = \"ma_dollar_pnl\",\n",
    "#     total_dollar_pnl = \"total_dollar_pnl\",\n",
    "#     rsi_dollar_pnl = \"rsi_dollar_pnl\"\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Plots the PnL distributions for a single ticker, strategy type, and order type.\n",
    "#     Assumes the input DataFrame is already filtered to one ticker.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Filter for the selected MA/RSI period combination if provided\n",
    "#     if ma_period1 is not None and ma_period2 is not None and rsi_period is not None:\n",
    "#         final_pnl_df = final_pnl_df[\n",
    "#             (final_pnl_df[\"ma_period1\"] == ma_period1) &\n",
    "#             (final_pnl_df[\"ma_period2\"] == ma_period2) &\n",
    "#             (final_pnl_df[\"rsi_period\"] == rsi_period)\n",
    "#         ]\n",
    "\n",
    "#     if final_pnl_df.empty:\n",
    "#         print(\"The DataFrame is empty. No data to visualize.\")\n",
    "#         return\n",
    "\n",
    "#     if \"compression_factor\" not in final_pnl_df.columns:\n",
    "#         print(\"Missing Compression_Factor column.\")\n",
    "#         return\n",
    "\n",
    "#     ticker = final_pnl_df[\"ticker\"].iloc[0]  # For title display\n",
    "\n",
    "#     # Extract unique compression factors\n",
    "#     compression_factors = sorted(final_pnl_df[\"compression_factor\"].unique())\n",
    "\n",
    "#     # Prepare data for violin plots\n",
    "#     ma_pnl_data = [final_pnl_df[final_pnl_df[\"compression_factor\"] == cf][ma_dollar_pnl].values for cf in compression_factors]\n",
    "#     total_pnl_data = [final_pnl_df[final_pnl_df[\"compression_factor\"] == cf][total_dollar_pnl].values for cf in compression_factors]\n",
    "#     rsi_pnl_data = [final_pnl_df[final_pnl_df[\"compression_factor\"] == cf][rsi_dollar_pnl].values for cf in compression_factors]\n",
    "\n",
    "#     plt.figure(figsize=(20, 12))\n",
    "\n",
    "#     # Position offsets\n",
    "#     ma_positions = [cf - 0.5 for cf in compression_factors]\n",
    "#     total_positions = compression_factors\n",
    "#     rsi_positions = [cf + 0.5 for cf in compression_factors]\n",
    "\n",
    "#     def plot_extra_stats(data, positions, color):\n",
    "#         for i, pos in enumerate(positions):\n",
    "#             if len(data[i]) == 0:\n",
    "#                 continue\n",
    "#             mean_val = np.mean(data[i])\n",
    "#             q25, q75 = np.percentile(data[i], [25, 75])\n",
    "#             p5, p95 = np.percentile(data[i], [5, 95])\n",
    "#             plt.scatter(pos, mean_val, color='black', s=80, zorder=3)\n",
    "#             plt.plot([pos, pos], [q25, q75], color=color, linewidth=4)\n",
    "#             plt.plot([pos, pos], [p5, p95], color=color, linewidth=1, linestyle='--')\n",
    "\n",
    "#     # Plot MA\n",
    "#     violin_ma = plt.violinplot(ma_pnl_data, positions=ma_positions, showmedians=True)\n",
    "#     for vp in violin_ma['bodies']:\n",
    "#         vp.set_facecolor('yellow')\n",
    "#         vp.set_edgecolor('black')\n",
    "#         vp.set_alpha(0.5)\n",
    "#     violin_ma['cmedians'].set_color('yellow')\n",
    "#     for part in ['cmins', 'cmaxes', 'cbars']:\n",
    "#         violin_ma[part].set_color('yellow')\n",
    "#     plot_extra_stats(ma_pnl_data, ma_positions, 'yellow')\n",
    "\n",
    "#     # Plot Total\n",
    "#     violin_total = plt.violinplot(total_pnl_data, positions=total_positions, showmedians=True)\n",
    "#     for vp in violin_total['bodies']:\n",
    "#         vp.set_facecolor('green')\n",
    "#         vp.set_edgecolor('black')\n",
    "#         vp.set_alpha(0.5)\n",
    "#     violin_total['cmedians'].set_color('green')\n",
    "#     for part in ['cmins', 'cmaxes', 'cbars']:\n",
    "#         violin_total[part].set_color('green')\n",
    "#     plot_extra_stats(total_pnl_data, total_positions, 'green')\n",
    "\n",
    "#     # Plot RSI\n",
    "#     violin_rsi = plt.violinplot(rsi_pnl_data, positions=rsi_positions, showmedians=True)\n",
    "#     for vp in violin_rsi['bodies']:\n",
    "#         vp.set_facecolor('blue')\n",
    "#         vp.set_edgecolor('black')\n",
    "#         vp.set_alpha(0.5)\n",
    "#     violin_rsi['cmedians'].set_color('blue')\n",
    "#     for part in ['cmins', 'cmaxes', 'cbars']:\n",
    "#         violin_rsi[part].set_color('blue')\n",
    "#     plot_extra_stats(rsi_pnl_data, rsi_positions, 'blue')\n",
    "\n",
    "#     plt.axhline(0, color='yellow', linestyle='--', linewidth=1)\n",
    "#     plt.xlabel(\"Compression Factor (Minutes)\")\n",
    "#     plt.ylabel(\"Dollar PnL\")\n",
    "#     plt.title(f\"PnL Distributions vs Compression Factor for {ticker} ({strategy_type}, {order_type})\")\n",
    "#     plt.xticks(compression_factors, labels=[str(cf) for cf in compression_factors])\n",
    "#     plt.show()\n",
    "\n",
    "#     # Summary Stats\n",
    "#     print(f\"Ticker: {ticker} - Aggregate PnL Metrics by Compression Factor ({strategy_type}, {order_type}):\\n\")\n",
    "#     for cf in compression_factors:\n",
    "#         cf_ma_pnl = final_pnl_df[final_pnl_df[\"compression_factor\"] == cf][ma_dollar_pnl]\n",
    "#         cf_total_pnl = final_pnl_df[final_pnl_df[\"compression_factor\"] == cf][total_dollar_pnl]\n",
    "#         cf_rsi_pnl = final_pnl_df[final_pnl_df[\"compression_factor\"] == cf][rsi_dollar_pnl]\n",
    "#         print(f\"  Compression Factor {cf}:\")\n",
    "#         print(f\"    MA PnL    -> Sum: {cf_ma_pnl.sum():.2f}, Mean: {cf_ma_pnl.mean():.2f}\")\n",
    "#         print(f\"    Total PnL -> Sum: {cf_total_pnl.sum():.2f}, Mean: {cf_total_pnl.mean():.2f}\")\n",
    "#         print(f\"    RSI PnL   -> Sum: {cf_rsi_pnl.sum():.2f}, Mean: {cf_rsi_pnl.mean():.2f}\")\n",
    "#         print(\"-\" * 50)\n",
    "#     print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "# def plot_all_pnl_distributions(final_pnl_df, \n",
    "#                                ma_period1=None, \n",
    "#                                ma_period2=None, \n",
    "#                                rsi_period=None,\n",
    "#                                ma_dollar_pnl='ma_dollar_pnl',\n",
    "#                                total_dollar_pnl='total_dollar_pnl',\n",
    "#                                rsi_dollar_pnl='rsi_dollar_pnl'):\n",
    "#     \"\"\"\n",
    "#     Plots PnL distributions for each ticker, strategy, order type, and selected MA/RSI period combination.\n",
    "#     \"\"\"\n",
    "#     if final_pnl_df.empty:\n",
    "#         print(\"The DataFrame is empty. Nothing to plot.\")\n",
    "#         return\n",
    "\n",
    "#     required_columns = {\"ticker\", \"strategy_type\", \"order_type\", \"compression_factor\"}\n",
    "#     if not required_columns.issubset(final_pnl_df.columns):\n",
    "#         print(f\"Missing one or more required columns: {required_columns}\")\n",
    "#         return\n",
    "\n",
    "#     unique_tickers = final_pnl_df[\"ticker\"].unique()\n",
    "\n",
    "#     for ticker in unique_tickers:\n",
    "#         ticker_df = final_pnl_df[final_pnl_df[\"ticker\"] == ticker]\n",
    "\n",
    "#         for strategy_type in [\"trend\", \"reversion\"]:\n",
    "#             for order_type in [\"market\", \"limit\"]:\n",
    "#                 subset = ticker_df[\n",
    "#                     (ticker_df[\"strategy_type\"] == strategy_type) &\n",
    "#                     (ticker_df[\"order_type\"] == order_type) &\n",
    "#                     (ticker_df[\"ma_period1\"] == ma_period1) &\n",
    "#                     (ticker_df[\"ma_period2\"] == ma_period2) &\n",
    "#                     (ticker_df[\"rsi_period\"] == rsi_period)\n",
    "#                 ]\n",
    "#                 if not subset.empty:\n",
    "#                     plot_pnl_distributions(\n",
    "#                         subset,\n",
    "#                         strategy_type=strategy_type,\n",
    "#                         order_type=order_type,\n",
    "#                         ma_period1=ma_period1,\n",
    "#                         ma_period2=ma_period2,\n",
    "#                         rsi_period=rsi_period,\n",
    "#                         ma_dollar_pnl=ma_dollar_pnl,\n",
    "#                         total_dollar_pnl=total_dollar_pnl,\n",
    "#                         rsi_dollar_pnl=rsi_dollar_pnl\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pnl_distributions(\n",
    "    final_pnl_df,\n",
    "    strategy_type=\"trend\",\n",
    "    order_type=\"market\",\n",
    "    directional_bias=\"long\",\n",
    "    ma_period1=None,\n",
    "    ma_period2=None,\n",
    "    rsi_period=None,\n",
    "    daily_stop_loss_dollars=None,\n",
    "    ma_dollar_pnl=\"ma_dollar_pnl\",\n",
    "    total_dollar_pnl=\"total_dollar_pnl\",\n",
    "    rsi_dollar_pnl=\"rsi_dollar_pnl\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the PnL distributions for a single ticker, strategy type, and order type.\n",
    "    Assumes the input DataFrame is already filtered to one ticker.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter for the selected MA/RSI period combination if provided\n",
    "    if ma_period1 is not None and ma_period2 is not None and rsi_period is not None:\n",
    "        final_pnl_df = final_pnl_df[\n",
    "            (final_pnl_df[\"ma_period1\"] == ma_period1) &\n",
    "            (final_pnl_df[\"ma_period2\"] == ma_period2) &\n",
    "            (final_pnl_df[\"rsi_period\"] == rsi_period)\n",
    "        ]\n",
    "\n",
    "    # Filter by daily_stop_loss_dollars if specified\n",
    "    if daily_stop_loss_dollars is not None:\n",
    "        final_pnl_df = final_pnl_df[\n",
    "            final_pnl_df[\"daily_stop_loss_dollars\"] == daily_stop_loss_dollars\n",
    "        ]\n",
    "\n",
    "    if final_pnl_df.empty:\n",
    "        print(\"The DataFrame is empty. No data to visualize.\")\n",
    "        return\n",
    "\n",
    "    if \"compression_factor\" not in final_pnl_df.columns:\n",
    "        print(\"Missing Compression_Factor column.\")\n",
    "        return\n",
    "\n",
    "    ticker = final_pnl_df[\"ticker\"].iloc[0]  # For title display\n",
    "    compression_factors = sorted(final_pnl_df[\"compression_factor\"].unique())\n",
    "\n",
    "    ma_pnl_data = [final_pnl_df[final_pnl_df[\"compression_factor\"] == cf][ma_dollar_pnl].values for cf in compression_factors]\n",
    "    total_pnl_data = [final_pnl_df[final_pnl_df[\"compression_factor\"] == cf][total_dollar_pnl].values for cf in compression_factors]\n",
    "    rsi_pnl_data = [final_pnl_df[final_pnl_df[\"compression_factor\"] == cf][rsi_dollar_pnl].values for cf in compression_factors]\n",
    "\n",
    "    plt.figure(figsize=(20, 12))\n",
    "\n",
    "    ma_positions = [cf - 0.5 for cf in compression_factors]\n",
    "    total_positions = compression_factors\n",
    "    rsi_positions = [cf + 0.5 for cf in compression_factors]\n",
    "\n",
    "    def plot_extra_stats(data, positions, color):\n",
    "        for i, pos in enumerate(positions):\n",
    "            if len(data[i]) == 0:\n",
    "                continue\n",
    "            mean_val = np.mean(data[i])\n",
    "            q25, q75 = np.percentile(data[i], [25, 75])\n",
    "            p5, p95 = np.percentile(data[i], [5, 95])\n",
    "            plt.scatter(pos, mean_val, color='black', s=80, zorder=3)\n",
    "            plt.plot([pos, pos], [q25, q75], color=color, linewidth=4)\n",
    "            plt.plot([pos, pos], [p5, p95], color=color, linewidth=1, linestyle='--')\n",
    "\n",
    "    # Plot MA\n",
    "    violin_ma = plt.violinplot(ma_pnl_data, positions=ma_positions, showmedians=True)\n",
    "    for vp in violin_ma['bodies']:\n",
    "        vp.set_facecolor('yellow')\n",
    "        vp.set_edgecolor('black')\n",
    "        vp.set_alpha(0.5)\n",
    "    violin_ma['cmedians'].set_color('yellow')\n",
    "    for part in ['cmins', 'cmaxes', 'cbars']:\n",
    "        violin_ma[part].set_color('yellow')\n",
    "    plot_extra_stats(ma_pnl_data, ma_positions, 'yellow')\n",
    "\n",
    "    # Plot Total\n",
    "    violin_total = plt.violinplot(total_pnl_data, positions=total_positions, showmedians=True)\n",
    "    for vp in violin_total['bodies']:\n",
    "        vp.set_facecolor('green')\n",
    "        vp.set_edgecolor('black')\n",
    "        vp.set_alpha(0.5)\n",
    "    violin_total['cmedians'].set_color('green')\n",
    "    for part in ['cmins', 'cmaxes', 'cbars']:\n",
    "        violin_total[part].set_color('green')\n",
    "    plot_extra_stats(total_pnl_data, total_positions, 'green')\n",
    "\n",
    "    # Plot RSI\n",
    "    violin_rsi = plt.violinplot(rsi_pnl_data, positions=rsi_positions, showmedians=True)\n",
    "    for vp in violin_rsi['bodies']:\n",
    "        vp.set_facecolor('blue')\n",
    "        vp.set_edgecolor('black')\n",
    "        vp.set_alpha(0.5)\n",
    "    violin_rsi['cmedians'].set_color('blue')\n",
    "    for part in ['cmins', 'cmaxes', 'cbars']:\n",
    "        violin_rsi[part].set_color('blue')\n",
    "    plot_extra_stats(rsi_pnl_data, rsi_positions, 'blue')\n",
    "\n",
    "    plt.axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "    plt.xlabel(\"Compression Factor (Minutes)\")\n",
    "    plt.ylabel(\"Dollar PnL\")\n",
    "    title = f\"PnL Distributions vs Compression Factor for {ticker} ({strategy_type}, {order_type}, SL={daily_stop_loss_dollars})\"\n",
    "    plt.title(title)\n",
    "    plt.xticks(compression_factors, labels=[str(cf) for cf in compression_factors])\n",
    "    plt.show()\n",
    "\n",
    "    # Summary Stats\n",
    "    print(f\"Ticker: {ticker} - Aggregate PnL Metrics by Compression Factor ({strategy_type}, {order_type}, SL={daily_stop_loss_dollars}):\\n\")\n",
    "    for cf in compression_factors:\n",
    "        cf_ma_pnl = final_pnl_df[final_pnl_df[\"compression_factor\"] == cf][ma_dollar_pnl]\n",
    "        cf_total_pnl = final_pnl_df[final_pnl_df[\"compression_factor\"] == cf][total_dollar_pnl]\n",
    "        cf_rsi_pnl = final_pnl_df[final_pnl_df[\"compression_factor\"] == cf][rsi_dollar_pnl]\n",
    "        print(f\"  Compression Factor {cf}:\")\n",
    "        print(f\"    MA PnL    -> Sum: {cf_ma_pnl.sum():.2f}, Mean: {cf_ma_pnl.mean():.2f}\")\n",
    "        print(f\"    Total PnL -> Sum: {cf_total_pnl.sum():.2f}, Mean: {cf_total_pnl.mean():.2f}\")\n",
    "        print(f\"    RSI PnL   -> Sum: {cf_rsi_pnl.sum():.2f}, Mean: {cf_rsi_pnl.mean():.2f}\")\n",
    "        print(\"-\" * 50)\n",
    "    print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "def plot_all_pnl_distributions(final_pnl_df, \n",
    "                               ma_period1=None, \n",
    "                               ma_period2=None, \n",
    "                               rsi_period=None,\n",
    "                               daily_stop_loss_dollars=None,\n",
    "                               ma_dollar_pnl='ma_dollar_pnl',\n",
    "                               total_dollar_pnl='total_dollar_pnl',\n",
    "                               rsi_dollar_pnl='rsi_dollar_pnl'):\n",
    "    \"\"\"\n",
    "    Plots PnL distributions for each ticker, strategy, order type, and selected MA/RSI period combination,\n",
    "    filtered by daily_stop_loss_dollars.\n",
    "    \"\"\"\n",
    "    if final_pnl_df.empty:\n",
    "        print(\"The DataFrame is empty. Nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    required_columns = {\"ticker\", \"strategy_type\", \"order_type\", \"compression_factor\", \"daily_stop_loss_dollars\"}\n",
    "    if not required_columns.issubset(final_pnl_df.columns):\n",
    "        print(f\"Missing one or more required columns: {required_columns}\")\n",
    "        return\n",
    "\n",
    "    unique_tickers = final_pnl_df[\"ticker\"].unique()\n",
    "\n",
    "    for ticker in unique_tickers:\n",
    "        ticker_df = final_pnl_df[final_pnl_df[\"ticker\"] == ticker]\n",
    "\n",
    "        if daily_stop_loss_dollars is not None:\n",
    "            ticker_df = ticker_df[ticker_df[\"daily_stop_loss_dollars\"] == daily_stop_loss_dollars]\n",
    "\n",
    "        for strategy_type in [\"trend\", \"reversion\"]:\n",
    "            for order_type in [\"market\", \"limit\"]:\n",
    "                subset = ticker_df[\n",
    "                    (ticker_df[\"strategy_type\"] == strategy_type) &\n",
    "                    (ticker_df[\"order_type\"] == order_type) &\n",
    "                    (ticker_df[\"ma_period1\"] == ma_period1) &\n",
    "                    (ticker_df[\"ma_period2\"] == ma_period2) &\n",
    "                    (ticker_df[\"rsi_period\"] == rsi_period)\n",
    "                ]\n",
    "                if not subset.empty:\n",
    "                    plot_pnl_distributions(\n",
    "                        subset,\n",
    "                        strategy_type=strategy_type,\n",
    "                        order_type=order_type,\n",
    "                        ma_period1=ma_period1,\n",
    "                        ma_period2=ma_period2,\n",
    "                        rsi_period=rsi_period,\n",
    "                        daily_stop_loss_dollars=daily_stop_loss_dollars,\n",
    "                        ma_dollar_pnl=ma_dollar_pnl,\n",
    "                        total_dollar_pnl=total_dollar_pnl,\n",
    "                        rsi_dollar_pnl=rsi_dollar_pnl\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_stop_loss_dollars = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_pnl_distributions(final_pnl_df, \n",
    "                           ma_period1=3, \n",
    "                           ma_period2=3, \n",
    "                           rsi_period=3,\n",
    "                           daily_stop_loss_dollars = daily_stop_loss_dollars,\n",
    "                           ma_dollar_pnl='ma_dollar_pnl',\n",
    "                           total_dollar_pnl='total_dollar_pnl',\n",
    "                           rsi_dollar_pnl='rsi_dollar_pnl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_pnl_distributions(final_pnl_df, \n",
    "                           ma_period1=3, \n",
    "                           ma_period2=3, \n",
    "                           rsi_period=3,\n",
    "                           daily_stop_loss_dollars = daily_stop_loss_dollars,\n",
    "                           ma_dollar_pnl='ma_dollar_pnl_stop_loss_adjusted',\n",
    "                           total_dollar_pnl='total_dollar_pnl_stop_loss_adjusted',\n",
    "                           rsi_dollar_pnl='rsi_dollar_pnl_stop_loss_adjusted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_pnl_distributions(final_pnl_df, \n",
    "                           ma_period1=3, \n",
    "                           ma_period2=3, \n",
    "                           rsi_period=3,\n",
    "                           daily_stop_loss_dollars = daily_stop_loss_dollars,\n",
    "                           ma_dollar_pnl='ma_dollar_pnl_sub_comms_stop_loss_adjusted',\n",
    "                           total_dollar_pnl='total_dollar_pnl_sub_comms_stop_loss_adjusted',\n",
    "                           rsi_dollar_pnl='rsi_dollar_pnl_sub_comms_stop_loss_adjusted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"color:red;\"><b>End of Research Branch<b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 style=\"color:yellow;\">Clean a copy of `minute_candles` for current day</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a copy of `minute_candles` to experiment with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minute_candles_0 = minute_candles_nmhr.copy() # Use this line if working with all accrued data from csv stock\n",
    "minute_candles_0 = minute_candles.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minute_candles_1 = minute_candles_0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create copy and reduce data frame to only columns from `columns_to_save` so the subsequent calculations are guaranteed fresh and define the candle `compression_factor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_save = ['datetime', 'ticker', 'open', 'high', 'low', 'close', 'accumulative_volume']\n",
    "minute_candles_1 = filter_columns_in_dict(minute_candles_1, columns_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some light but necessary data cleaning: removing duplicate rows and rows with zero price (ghost streamer data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply `remove_zero_close` to all DataFrames in the dictionary\n",
    "# for key in minute_candles_1.keys():\n",
    "#     minute_candles_1[key] = remove_zero_close(minute_candles_1[key])\n",
    "\n",
    "# # Apply `remove_duplicates` to all DataFrames in the dictionary\n",
    "# for key in minute_candles_1.keys():\n",
    "#     minute_candles_1[key] = remove_duplicates(minute_candles_1[key])\n",
    "\n",
    "# # Apply `remove_duplicates` to all DataFrames in the dictionary\n",
    "# for key in minute_candles_1.keys():\n",
    "#     minute_candles_1[key] = remove_data_between_5pm_and_6pm(minute_candles_1[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(minute_candles_1['/ES'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 style=\"color:yellow;\">The \"Data Handler\" for this Simulation Iteration</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compress the one minute candles if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_candles = {}\n",
    "\n",
    "# Define the compression factors\n",
    "compression_factors = range(1, 16, 2)\n",
    "\n",
    "# Iterate through the compression factors and compress each ticker's data separately\n",
    "for compression_factor in compression_factors:\n",
    "    # Generate the dynamic name for the compression level\n",
    "    dynamic_name = f\"minute_candles_{compression_factor}\"\n",
    "\n",
    "    # Apply compression to each ticker's DataFrame and store the results\n",
    "    compressed_candles[dynamic_name] = {\n",
    "        ticker: compress_candles(df, compression_factor) for ticker, df in minute_candles_1.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Access dynamically named dictionaries of DataFrames\n",
    "display(compressed_candles['minute_candles_3']['/NQ'])  # Access the dictionary for compression factor 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_periods = [5, 10, 20]\n",
    "sma_periods = ma_periods\n",
    "wma_periods = ma_periods\n",
    "rsi_periods = ma_periods\n",
    "\n",
    "# Dynamically create ma_combinations\n",
    "ma_combinations = [\n",
    "    (f'wma_{period}', f'sma_{period}', f'rsi_{period}') for period in ma_periods\n",
    "]\n",
    "\n",
    "# Print the result to verify\n",
    "print(\"MA Periods:\", ma_periods)\n",
    "print(\"MA Combinations:\", ma_combinations)\n",
    "\n",
    "# Iterate through all the dictionaries in compressed_candles\n",
    "for compression_name, ticker_dict in compressed_candles.items():\n",
    "    print(f\"Processing compression: {compression_name}\")\n",
    "    \n",
    "    # Iterate through all the tickers in the current dictionary\n",
    "    for ticker, df in ticker_dict.items():\n",
    "        print(f\"Processing ticker: {ticker} in {compression_name}\")\n",
    "        \n",
    "        # Calculate moving averages and indicators for each DataFrame\n",
    "        ticker_dict[ticker] = calculate_indicators(\n",
    "            df, \n",
    "            price_col='close', \n",
    "            acc_vol_col='accumulative_volume', \n",
    "            sma_periods=sma_periods,\n",
    "            wma_periods=wma_periods,\n",
    "            rsi_periods=rsi_periods,\n",
    "            candle_window=5\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examination_compression_factor = 3\n",
    "display(compressed_candles[f'minute_candles_{examination_compression_factor}']['/NQ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the trading strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for compression_name, ticker_dict in compressed_candles.items():\n",
    "    print(f\"Processing compression: {compression_name}\")\n",
    "    \n",
    "    # Iterate through all the tickers in the current dictionary\n",
    "    for ticker, df in ticker_dict.items():\n",
    "        print(f\"Processing ticker: {ticker} in {compression_name}\")\n",
    "        \n",
    "        # Iterate through all the ma_combinations\n",
    "        for sig_ma, con_ma, rsi_col in ma_combinations:\n",
    "            # Generate trading signals\n",
    "            ticker_dict[ticker] = generate_trading_signals_1(\n",
    "                df,\n",
    "                ma_name1=sig_ma,\n",
    "                ma_name2=con_ma,\n",
    "                rsi_column=rsi_col\n",
    "            )\n",
    "\n",
    "            # Update position_open columns to be 1:1 verbal boolean with the signal\n",
    "            ticker_dict[ticker] = update_position_open_1(\n",
    "                df,\n",
    "                ma_name1=sig_ma,\n",
    "                ma_name2=con_ma,\n",
    "                rsi_column=rsi_col\n",
    "            )\n",
    "\n",
    "            # Determine entry prices for each ticker\n",
    "            ticker_dict[ticker] = determine_entry_prices_1(\n",
    "                df,\n",
    "                ma_name1=sig_ma,\n",
    "                ma_name2=con_ma,\n",
    "                rsi_column=rsi_col,\n",
    "                ticker_to_tick_size=ticker_to_tick_size,\n",
    "                ticker=ticker\n",
    "            )\n",
    "\n",
    "            # Determine exit prices for each ticker\n",
    "            ticker_dict[ticker] = determine_exit_prices_1(\n",
    "                df,\n",
    "                ma_name1=sig_ma,\n",
    "                ma_name2=con_ma,\n",
    "                rsi_column=rsi_col,\n",
    "                ticker_to_tick_size=ticker_to_tick_size,\n",
    "                ticker=ticker\n",
    "            )\n",
    "\n",
    "            # Stop loss calculation\n",
    "            ticker_dict[ticker] = calculate_stop_losses_1(\n",
    "                df,\n",
    "                ma_name1=sig_ma,\n",
    "                ma_name2=con_ma,\n",
    "                rsi_column=rsi_col\n",
    "            )\n",
    "\n",
    "            # Track stop loss hits\n",
    "            ticker_dict[ticker] = track_stop_loss_hits_1(\n",
    "                df,\n",
    "                ma_name1=sig_ma,\n",
    "                ma_name2=con_ma,\n",
    "                rsi_column=rsi_col,\n",
    "                ticker_to_tick_size=ticker_to_tick_size,\n",
    "                ticker=ticker\n",
    "            )\n",
    "\n",
    "            # Adjust signals from stop loss hits\n",
    "            ticker_dict[ticker] = adjust_signals_for_stop_loss_1(\n",
    "                df,\n",
    "                ma_name1=sig_ma,\n",
    "                ma_name2=con_ma,\n",
    "                rsi_column=rsi_col\n",
    "            )\n",
    "\n",
    "            # Re-update position_open column after stop loss hits\n",
    "            ticker_dict[ticker] = update_position_open_1(\n",
    "                df,\n",
    "                ma_name1=sig_ma,\n",
    "                ma_name2=con_ma,\n",
    "                rsi_column=rsi_col\n",
    "            )\n",
    "\n",
    "            # Re-determine entry prices after stop loss hits\n",
    "            ticker_dict[ticker] = determine_entry_prices_1(\n",
    "                df,\n",
    "                ma_name1=sig_ma,\n",
    "                ma_name2=con_ma,\n",
    "                rsi_column=rsi_col,\n",
    "                ticker_to_tick_size=ticker_to_tick_size,\n",
    "                ticker=ticker\n",
    "            )\n",
    "\n",
    "            # Re-determine exit prices after stop loss hits\n",
    "            ticker_dict[ticker] = determine_exit_prices_1(\n",
    "                df,\n",
    "                ma_name1=sig_ma,\n",
    "                ma_name2=con_ma,\n",
    "                rsi_column=rsi_col,\n",
    "                ticker_to_tick_size=ticker_to_tick_size,\n",
    "                ticker=ticker\n",
    "            )\n",
    "\n",
    "            # Update stop loss levels after stop loss hits\n",
    "            ticker_dict[ticker] = update_stop_loss_1(\n",
    "                df,\n",
    "                ma_name1=sig_ma,\n",
    "                ma_name2=con_ma,\n",
    "                rsi_column=rsi_col\n",
    "            )\n",
    "\n",
    "            # Calculate profit/loss for each ticker's DataFrame\n",
    "            ticker_dict[ticker] = calculate_profit_loss_1(\n",
    "                df,\n",
    "                contract_multiplier=1,\n",
    "                ma_name1=sig_ma,\n",
    "                ma_name2=con_ma,\n",
    "                rsi_column=rsi_col\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 style=\"color:yellow;\">Inspection of Strategy Outputs</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a selection of columns from the new data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: f'{x:.4f}')\n",
    "pd.options.display.float_format = '{:,.4f}'.format # Format numerical output to have certain number of decimals\n",
    "# pd.options.display.float_format = None # Reset to default numerical output formatting\n",
    "pd.set_option('display.width', 2000) # Set the display width to a large number\n",
    "pd.set_option('display.max_colwidth', 1000) # Set max column width to a large number\n",
    "\n",
    "pd.set_option('display.max_columns', None) # Displays all columns\n",
    "# pd.set_option('display.max_rows', None) # Displays all rows                             \n",
    "# pd.reset_option('display.max_columns') # Display default abbreviated columns\n",
    "# pd.reset_option('display.max_rows') # Display default abbreviated rows\n",
    "\n",
    "# key = 0\n",
    "\n",
    "tickers_list = list(ticker_tables.keys())\n",
    "# display(tickers_list)\n",
    "# display(minute_candles_1[tickers_list[key]][['close', \n",
    "#                                            'candle_span_max', \n",
    "#                                            'stop_loss_ma', \n",
    "#                                            'wma_5', 'sma_5', \n",
    "#                                            'signal_wma_5_sma_5',\n",
    "#                                            'ma_position_open',\n",
    "#                                            'ma_entry_price',\n",
    "#                                            'ma_exit_price',\n",
    "#                                            'ma_stop_loss_hit',\n",
    "#                                            'close',\n",
    "#                                            'stop_loss_rsi', \n",
    "#                                            'rsi_5', \n",
    "#                                            'signal_rsi_5',\n",
    "#                                            'rsi_position_open',\n",
    "#                                            'rsi_entry_price',\n",
    "#                                            'rsi_exit_price',\n",
    "#                                            'rsi_stop_loss_hit']])\n",
    "# display(minute_candles_1[tickers_list[key]])\n",
    "\n",
    "key = 1\n",
    "ma_name1 = 'wma_5'\n",
    "ma_name2 = 'sma_5'\n",
    "rsi_column = 'rsi_5'\n",
    "\n",
    "display(compressed_candles[f'minute_candles_{examination_compression_factor}'][tickers_list[key]][['close', \n",
    "                                                                'candle_span_max', \n",
    "                                                                f'stop_loss_{ma_name1}_{ma_name2}', \n",
    "                                                                ma_name1, ma_name2, \n",
    "                                                                f'signal_{ma_name1}_{ma_name2}',\n",
    "                                                                f'position_open_{ma_name1}_{ma_name2}',\n",
    "                                                                f'entry_price_{ma_name1}_{ma_name2}',\n",
    "                                                                f'exit_price_{ma_name1}_{ma_name2}',\n",
    "                                                                f'stop_loss_hit_{ma_name1}_{ma_name2}',\n",
    "                                                                'close',\n",
    "                                                                f'stop_loss_{rsi_column}', \n",
    "                                                                rsi_column, \n",
    "                                                                f'signal_{rsi_column}',\n",
    "                                                                f'position_open_{rsi_column}',\n",
    "                                                                f'entry_price_{rsi_column}',\n",
    "                                                                f'exit_price_{rsi_column}',\n",
    "                                                                f'stop_loss_hit_{rsi_column}']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(compressed_candles[f'minute_candles_{examination_compression_factor}']['/NQ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker, tick_size in ticker_to_tick_size.items():\n",
    "    print(f\"Ticker: {ticker}, Tick Size: {tick_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the streamed and stored data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "\n",
    "sig_ma = f'wma_{window_size}'\n",
    "con_ma = f'sma_{window_size}'\n",
    "rsi_col = f'rsi_{window_size}'\n",
    "\n",
    "visualize_trades_1(\n",
    "    candles=compressed_candles[f'minute_candles_{examination_compression_factor}'], \n",
    "    ticker_to_tick_size=ticker_to_tick_size,\n",
    "    ticker_to_point_value=ticker_to_point_value,    \n",
    "    ma_name1=sig_ma, \n",
    "    ma_name2=con_ma, \n",
    "    rsi_column=rsi_col, \n",
    "    lower_slice=0, \n",
    "    upper_slice=-1,\n",
    "    compression_factor=examination_compression_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add metrics for difference between last and first close price, and difference between that and total pnl, and a boolean to determine whether the algorithm had alpha (better performance than buy/hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dynamic_name, ticker_dict in compressed_candles.items():\n",
    "    # Extract the compression factor from the dynamic name\n",
    "    compression_factor = int(dynamic_name.split('_')[-1])  # Extract the number from 'minute_candles_X'\n",
    "\n",
    "    print()\n",
    "    print(f\"{compression_factor}-Minute Compression PnL Figures\")\n",
    "    \n",
    "    # Call the updated print_all_pnls function\n",
    "    print_all_pnls_1(\n",
    "        candles=ticker_dict,  # Pass the dictionary of DataFrames\n",
    "        compression_factor=compression_factor,  # Pass the extracted compression factor\n",
    "        ticker_to_tick_size=ticker_to_tick_size,  # Pass tick size mapping\n",
    "        ticker_to_point_value=ticker_to_point_value,  # Pass point value mapping\n",
    "        ma_name1='wma_5',\n",
    "        ma_name2='sma_5',\n",
    "        rsi_column='rsi_5'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider rewriting this function given the new print_all_pnls function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pnl_dataframe(candles, compression_factor, ticker_to_tick_size, ticker_to_point_value, ma_name1='wma_5', ma_name2='sma_5', rsi_column='rsi_5'):\n",
    "    \"\"\"\n",
    "    Generates a DataFrame containing detailed PnL, trade statistics, and max gain/loss for all tickers.\n",
    "\n",
    "    Parameters:\n",
    "    - candles: Dictionary of DataFrames containing trading data for multiple tickers.\n",
    "    - compression_factor: The compression factor associated with the current dataset.\n",
    "    - ticker_to_tick_size: Dictionary mapping tickers to tick sizes.\n",
    "    - ticker_to_point_value: Dictionary mapping tickers to point values.\n",
    "    - ma_name1, ma_name2: Moving average column names.\n",
    "    - rsi_column: RSI column name.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with PnL, trade statistics, and max gain/loss for all tickers.\n",
    "    \"\"\"\n",
    "    # Generate dynamic column names for PnL and trade metrics\n",
    "    pnl_ma_col = f'pnl_{ma_name1}_{ma_name2}'\n",
    "    pnl_rsi_col = f'pnl_{rsi_column}'\n",
    "    cum_pnl_ma_col = f'cum_{pnl_ma_col}'\n",
    "    cum_pnl_rsi_col = f'cum_{pnl_rsi_col}'\n",
    "    cum_pnl_all_col = f'cum_pnl_all_{ma_name1}_{ma_name2}_{rsi_column}'\n",
    "    ma_entry_price = f'entry_price_{ma_name1}_{ma_name2}'\n",
    "    ma_exit_price = f'exit_price_{ma_name1}_{ma_name2}'\n",
    "    rsi_entry_price = f'entry_price_{rsi_column}'\n",
    "    rsi_exit_price = f'exit_price_{rsi_column}'\n",
    "    ma_commission_col = f'commission_cost_{ma_name1}_{ma_name2}'\n",
    "    rsi_commission_col = f'commission_cost_{rsi_column}'\n",
    "\n",
    "    # Create a list to hold rows of the DataFrame\n",
    "    pnl_rows = []\n",
    "\n",
    "    for ticker, minute_candles_df in candles.items():\n",
    "        try:\n",
    "            # Retrieve tick size and point value for the ticker\n",
    "            tick_size = ticker_to_tick_size.get(ticker, \"Unknown\")\n",
    "            point_value = ticker_to_point_value.get(ticker, 1)\n",
    "\n",
    "            # Calculate cumulative PnL and other statistics\n",
    "            ma_pnl = round(minute_candles_df[cum_pnl_ma_col].iloc[-1], 3)\n",
    "            rsi_pnl = round(minute_candles_df[cum_pnl_rsi_col].iloc[-1], 3)\n",
    "            total_pnl = round(minute_candles_df[cum_pnl_all_col].iloc[-1], 3)\n",
    "            close_price_diff = round(minute_candles_df[\"close\"].iloc[-1] - minute_candles_df[\"close\"].iloc[0], 3)\n",
    "            point_alpha = round(total_pnl - close_price_diff, 3)\n",
    "\n",
    "            # Retrieve total commission costs\n",
    "            ma_commission_total = round(minute_candles_df[ma_commission_col].iloc[-1], 3) if ma_commission_col in minute_candles_df else 0.0\n",
    "            rsi_commission_total = round(minute_candles_df[rsi_commission_col].iloc[-1], 3) if rsi_commission_col in minute_candles_df else 0.0\n",
    "            total_commission_cost = ma_commission_total + rsi_commission_total\n",
    "\n",
    "            # Calculate total dollar PnLs\n",
    "            ma_dollar_pnl = ma_pnl * point_value\n",
    "            rsi_dollar_pnl = rsi_pnl * point_value\n",
    "            total_dollar_pnl = total_pnl * point_value\n",
    "            close_price_dollar_diff = close_price_diff * point_value\n",
    "            dollar_alpha = point_alpha * point_value\n",
    "\n",
    "            # Count the number of trades for MA and RSI strategies\n",
    "            ma_trades = (minute_candles_df[ma_exit_price].notna().sum() + minute_candles_df[ma_entry_price].notna().sum()) / 2\n",
    "            rsi_trades = (minute_candles_df[rsi_exit_price].notna().sum() + minute_candles_df[rsi_entry_price].notna().sum()) / 2\n",
    "\n",
    "            # Calculate max gain and max loss for MA, RSI, and total strategies\n",
    "            ma_max_gain = round(minute_candles_df[cum_pnl_ma_col].max(), 3)\n",
    "            ma_max_loss = round(minute_candles_df[cum_pnl_ma_col].min(), 3)\n",
    "            rsi_max_gain = round(minute_candles_df[cum_pnl_rsi_col].max(), 3)\n",
    "            rsi_max_loss = round(minute_candles_df[cum_pnl_rsi_col].min(), 3)\n",
    "            total_max_gain = round(minute_candles_df[cum_pnl_all_col].max(), 3)\n",
    "            total_max_loss = round(minute_candles_df[cum_pnl_all_col].min(), 3)\n",
    "            ma_max_dollar_gain = ma_max_gain * point_value\n",
    "            ma_max_dollar_loss = ma_max_loss * point_value\n",
    "            rsi_max_dollar_gain = rsi_max_gain * point_value\n",
    "            rsi_max_dollar_loss = rsi_max_loss * point_value\n",
    "            total_max_dollar_gain = total_max_gain * point_value\n",
    "            total_max_dollar_loss = total_max_loss * point_value\n",
    "\n",
    "            # Create a row with all the calculated metrics\n",
    "            pnl_rows.append({\n",
    "                'Ticker': ticker,\n",
    "                'Compression_Factor': compression_factor,\n",
    "                'MA_Trades': ma_trades,\n",
    "                'RSI_Trades': rsi_trades,\n",
    "                'Total_PnL': total_pnl,\n",
    "                'MA_PnL': ma_pnl,\n",
    "                'RSI_PnL': rsi_pnl,\n",
    "                'Total_Dollar_PnL': total_dollar_pnl,\n",
    "                'MA_Dollar_PnL': ma_dollar_pnl,\n",
    "                'RSI_Dollar_PnL': rsi_dollar_pnl,\n",
    "                'MA_Max_Gain': ma_max_gain,\n",
    "                'MA_Max_Loss': ma_max_loss,\n",
    "                'RSI_Max_Gain': rsi_max_gain,\n",
    "                'RSI_Max_Loss': rsi_max_loss,\n",
    "                'Total_Max_Gain': total_max_gain,\n",
    "                'Total_Max_Loss': total_max_loss,\n",
    "                'MA_Dollar_Max_Gain': ma_max_dollar_gain,\n",
    "                'MA_Dollar_Max_Loss': ma_max_dollar_loss,\n",
    "                'RSI_Dollar_Max_Gain': rsi_max_dollar_gain,\n",
    "                'RSI_Dollar_Max_Loss': rsi_max_dollar_loss,\n",
    "                'Total_Dollar_Max_Gain': total_max_dollar_gain,\n",
    "                'Total_Dollar_Max_Loss': total_max_dollar_loss,\n",
    "                'MA_Commission_Cost': ma_commission_total,\n",
    "                'RSI_Commission_Cost': rsi_commission_total,\n",
    "                'Total_Commission_Cost': total_commission_cost,\n",
    "                'Close_Price_Diff': close_price_diff,\n",
    "                'Point_Alpha': point_alpha,\n",
    "                'Close_Price_Dollar_Diff': close_price_dollar_diff,\n",
    "                'Dollar_Alpha': dollar_alpha,\n",
    "                'Tick_Size': tick_size\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing ticker {ticker}: {e}\")\n",
    "\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    return pd.DataFrame(pnl_rows)\n",
    "\n",
    "# Generate a DataFrame for each compression factor and concatenate them\n",
    "all_pnl_dfs = []\n",
    "\n",
    "for dynamic_name, ticker_dict in compressed_candles.items():\n",
    "    compression_factor = int(dynamic_name.split('_')[-1])  # Extract the compression factor\n",
    "    pnl_df = generate_pnl_dataframe(\n",
    "        candles=ticker_dict,\n",
    "        compression_factor=compression_factor,\n",
    "        ticker_to_tick_size=ticker_to_tick_size,\n",
    "        ticker_to_point_value=ticker_to_point_value,\n",
    "        ma_name1='wma_5',\n",
    "        ma_name2='sma_5',\n",
    "        rsi_column='rsi_5'\n",
    "    )\n",
    "    all_pnl_dfs.append(pnl_df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "final_pnl_df = pd.concat(all_pnl_dfs, ignore_index=True)\n",
    "\n",
    "# Display the final DataFrame\n",
    "display(final_pnl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pnl_distributions(final_pnl_df, strategy_type=\"trend\", order_type=\"market\"):\n",
    "    \"\"\"\n",
    "    Plots the PnL distributions for a specified strategy type (\"trend\" or \"reversion\") \n",
    "    for each ticker against compression factors using violin plots.\n",
    "\n",
    "    Parameters:\n",
    "    - final_pnl_df (pd.DataFrame): DataFrame containing PnL metrics for all tickers and strategies.\n",
    "    - strategy_type (str): Either \"trend\" or \"reversion\", specifying which strategy's PnL to visualize.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure there is data to visualize\n",
    "    if final_pnl_df.empty:\n",
    "        print(\"The DataFrame is empty. No data to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Ensure the strategy type column exists\n",
    "    if \"Strategy_Type\" not in final_pnl_df.columns:\n",
    "        print(\"Error: The DataFrame does not contain a 'Strategy_Type' column.\")\n",
    "        return\n",
    "\n",
    "    # Filter for the selected strategy type\n",
    "    strategy_data = final_pnl_df[\n",
    "        (final_pnl_df[\"Strategy_Type\"] == strategy_type) &\n",
    "        (final_pnl_df[\"Order_Type\"] == order_type)\n",
    "    ]\n",
    "\n",
    "    if strategy_data.empty:\n",
    "        print(f\"No data available for strategy type: {strategy_type}\")\n",
    "        return\n",
    "\n",
    "    # Get the unique tickers in the filtered dataframe\n",
    "    unique_tickers = strategy_data[\"Ticker\"].unique()\n",
    "\n",
    "    # Create a violin plot for each ticker\n",
    "    for ticker in unique_tickers:\n",
    "        # Filter data for the specific ticker\n",
    "        ticker_data = strategy_data[strategy_data[\"Ticker\"] == ticker]\n",
    "\n",
    "        # Extract unique compression factors and sort them\n",
    "        compression_factors = sorted(ticker_data[\"Compression_Factor\"].unique())\n",
    "\n",
    "        # Prepare data for violin plots (three metrics)\n",
    "        ma_pnl_data = [ticker_data[ticker_data[\"Compression_Factor\"] == cf][\"MA_Dollar_PnL\"].values for cf in compression_factors]\n",
    "        total_pnl_data = [ticker_data[ticker_data[\"Compression_Factor\"] == cf][\"Total_Dollar_PnL\"].values for cf in compression_factors]\n",
    "        rsi_pnl_data = [ticker_data[ticker_data[\"Compression_Factor\"] == cf][\"RSI_Dollar_PnL\"].values for cf in compression_factors]\n",
    "\n",
    "        # Create the violin plot\n",
    "        plt.figure(figsize=(20, 12))\n",
    "\n",
    "        # Shift positions for separation\n",
    "        ma_positions = [cf - 0.5 for cf in compression_factors]  # Move left\n",
    "        total_positions = compression_factors  # Stay at original position (middle)\n",
    "        rsi_positions = [cf + 0.5 for cf in compression_factors]  # Move right\n",
    "\n",
    "        # Function to plot extra statistics\n",
    "        def plot_extra_stats(data, positions, color):\n",
    "            for i, pos in enumerate(positions):\n",
    "                if len(data[i]) == 0:\n",
    "                    continue  # Skip empty data\n",
    "                mean_val = np.mean(data[i])\n",
    "                q25, q75 = np.percentile(data[i], [25, 75])\n",
    "                p5, p95 = np.percentile(data[i], [5, 95])\n",
    "\n",
    "                # Plot mean as a black dot\n",
    "                plt.scatter(pos, mean_val, color='black', s=80, zorder=3)\n",
    "\n",
    "                # Plot interquartile range (thicker line)\n",
    "                plt.plot([pos, pos], [q25, q75], color=color, linewidth=4)\n",
    "\n",
    "                # Plot 5th and 95th percentiles (thin whisker line)\n",
    "                plt.plot([pos, pos], [p5, p95], color=color, linewidth=1, linestyle='--')\n",
    "\n",
    "        # Plot MA Dollar PnL (Yellow, Left)\n",
    "        violin_ma = plt.violinplot(ma_pnl_data, positions=ma_positions, showmedians=True)\n",
    "        for vp in violin_ma['bodies']:\n",
    "            vp.set_facecolor('yellow')  # Yellow for MA_PnL\n",
    "            vp.set_edgecolor('black')\n",
    "            vp.set_alpha(0.5)\n",
    "        violin_ma['cmedians'].set_color('yellow')  # Set median color\n",
    "        for part in ['cmins', 'cmaxes', 'cbars']:  # Whiskers & caps\n",
    "            violin_ma[part].set_color('yellow')\n",
    "\n",
    "        # Plot extra statistics for MA PnL\n",
    "        plot_extra_stats(ma_pnl_data, ma_positions, 'yellow')\n",
    "\n",
    "        # Plot Total Dollar PnL (Green, Middle)\n",
    "        violin_total = plt.violinplot(total_pnl_data, positions=total_positions, showmedians=True)\n",
    "        for vp in violin_total['bodies']:\n",
    "            vp.set_facecolor('green')  # Green for Total_PnL\n",
    "            vp.set_edgecolor('black')\n",
    "            vp.set_alpha(0.5)\n",
    "        violin_total['cmedians'].set_color('green')  # Set median color\n",
    "        for part in ['cmins', 'cmaxes', 'cbars']:  # Whiskers & caps\n",
    "            violin_total[part].set_color('green')\n",
    "\n",
    "        # Plot extra statistics for Total PnL\n",
    "        plot_extra_stats(total_pnl_data, total_positions, 'green')\n",
    "\n",
    "        # Plot RSI Dollar PnL (Blue, Right)\n",
    "        violin_rsi = plt.violinplot(rsi_pnl_data, positions=rsi_positions, showmedians=True)\n",
    "        for vp in violin_rsi['bodies']:\n",
    "            vp.set_facecolor('blue')  # Blue for RSI_PnL\n",
    "            vp.set_edgecolor('black')\n",
    "            vp.set_alpha(0.5)\n",
    "        violin_rsi['cmedians'].set_color('blue')  # Set median color\n",
    "        for part in ['cmins', 'cmaxes', 'cbars']:  # Whiskers & caps\n",
    "            violin_rsi[part].set_color('blue')\n",
    "\n",
    "        # Plot extra statistics for RSI PnL\n",
    "        plot_extra_stats(rsi_pnl_data, rsi_positions, 'blue')\n",
    "\n",
    "        # Add a horizontal zero line\n",
    "        plt.axhline(0, color='yellow', linestyle='--', linewidth=1)\n",
    "\n",
    "        # Labels and title\n",
    "        plt.xlabel(\"Compression Factor (Minutes)\")\n",
    "        plt.ylabel(\"Dollar PnL\")\n",
    "        plt.title(f\"PnL Distributions vs Compression Factor for {ticker} ({strategy_type}, {order_type})\")\n",
    "\n",
    "        # Adjust x-ticks to avoid overlapping\n",
    "        plt.xticks(compression_factors, labels=[str(cf) for cf in compression_factors])\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "        # Print sum and mean of all PnL metrics **grouped by compression factor**\n",
    "        print(f\"Ticker: {ticker} - Aggregate PnL Metrics by Compression Factor ({strategy_type} Strategy):\\n\")\n",
    "        for cf in compression_factors:\n",
    "            cf_ma_pnl = ticker_data[ticker_data[\"Compression_Factor\"] == cf][\"MA_Dollar_PnL\"]\n",
    "            cf_total_pnl = ticker_data[ticker_data[\"Compression_Factor\"] == cf][\"Total_Dollar_PnL\"]\n",
    "            cf_rsi_pnl = ticker_data[ticker_data[\"Compression_Factor\"] == cf][\"RSI_Dollar_PnL\"]\n",
    "\n",
    "            print(f\"  Compression Factor {cf}:\")\n",
    "            print(f\"    MA PnL    -> Sum: {cf_ma_pnl.sum():.2f}, Mean: {cf_ma_pnl.mean():.2f}\")\n",
    "            print(f\"    Total PnL -> Sum: {cf_total_pnl.sum():.2f}, Mean: {cf_total_pnl.mean():.2f}\")\n",
    "            print(f\"    RSI PnL   -> Sum: {cf_rsi_pnl.sum():.2f}, Mean: {cf_rsi_pnl.mean():.2f}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60 + \"\\n\")  # Separator for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strategy_type in [\"trend\", \"reversion\"]:\n",
    "    for order_type in [\"market\", \"limit\"]:\n",
    "        plot_pnl_distributions(final_pnl_df, strategy_type=strategy_type, order_type=order_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open Interest (OI)** is a term used in derivatives markets (such as futures and options) to describe the total number of outstanding (or open) contracts that have not yet been settled or closed.\n",
    "\n",
    "### Key Points:\n",
    "1. **Open Interest vs Volume**:\n",
    "   - **Volume** refers to the number of contracts traded in a specific period (typically one day). Each time a buyer and seller trade a contract, it counts toward the volume.\n",
    "   - **Open Interest** refers to the total number of contracts that remain active (open) at the end of a trading day. It only changes when a new contract is created or an existing contract is closed (through settlement or expiration).\n",
    "\n",
    "2. **How Open Interest Changes**:\n",
    "   - **Increases**: Open interest increases when new contracts are opened, meaning a buyer and a seller create a new contract. This happens when one trader opens a new long position (buy) and another trader opens a new short position (sell).\n",
    "   - **Decreases**: Open interest decreases when contracts are closed. This happens when one of the traders involved in the contract closes their position (either through offsetting trades or contract expiration).\n",
    "\n",
    "   **Example of Changes in Open Interest**:\n",
    "   - Two traders open a new contract: Open interest increases by 1.\n",
    "   - A trader who holds a position sells it to another trader (who did not previously have a position): Open interest does not change.\n",
    "   - A trader closes their position by selling it back to the market, and the other party in the contract does the same: Open interest decreases by 1.\n",
    "\n",
    "3. **Why Open Interest is Important**:\n",
    "   - **Liquidity**: A higher open interest generally indicates more liquidity in the market, meaning it's easier for traders to enter and exit positions. Markets with high open interest tend to have tighter spreads and less price slippage.\n",
    "   - **Market Sentiment**: Increasing open interest in a rising market can indicate that more traders are entering new positions and expecting the price to continue moving up (bullish sentiment). Conversely, increasing open interest in a falling market can indicate bearish sentiment.\n",
    "   - **Potential Reversals**: If open interest starts to decline while price trends continue, it can suggest that traders are closing their positions and the current trend may lose momentum, potentially leading to a reversal.\n",
    "\n",
    "4. **Open Interest in Different Markets**:\n",
    "   - **Futures**: Open interest is closely watched in futures markets. For example, in commodity markets like oil or gold, OI can indicate the level of trader engagement in a particular contract.\n",
    "   - **Options**: In options markets, open interest shows the number of outstanding option contracts for a specific strike price and expiration date. It's useful for understanding market positioning and potential areas of support/resistance.\n",
    "\n",
    "### Practical Use of Open Interest:\n",
    "- **Trend Confirmation**: Traders often use open interest in combination with price movements to confirm trends. For example:\n",
    "   - **Rising prices + increasing open interest**: Confirms that new traders are entering the market, supporting the continuation of the trend.\n",
    "   - **Rising prices + decreasing open interest**: Suggests that traders are closing positions, indicating that the trend may be weakening.\n",
    "   \n",
    "- **Reversals**: A decrease in open interest during a trending market can signal that the current trend is running out of steam, as participants are exiting positions.\n",
    "\n",
    "### Example of Open Interest in a Futures Contract:\n",
    "Suppose youre trading a futures contract for crude oil. Initially, 100 contracts are open in the market (Open Interest = 100). During the trading day:\n",
    "- Trader A buys 10 contracts from Trader B, and both open new positions. Open interest increases by 10 (OI = 110).\n",
    "- Trader C sells 5 contracts, closing their position. Open interest decreases by 5 (OI = 105).\n",
    "\n",
    "In this example, open interest fluctuates based on the creation and closure of new contracts but doesn't change based on intra-day volume alone.\n",
    "\n",
    "### Summary:\n",
    "- **Open Interest** reflects the total number of open (unsettled) derivative contracts in the market.\n",
    "- It increases with new contracts and decreases when contracts are closed.\n",
    "- Open interest can be a key indicator of market sentiment, liquidity, and potential trend strength or weakness.\n",
    "\n",
    "It is primarily used in futures and options trading to give insights into the current market condition and participant behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"color:red;\">Creating a dictionary containing order details.</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will get the hashValue of the first linked account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_accounts = client.account_linked().json()\n",
    "linked_accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get account number and hashes for linked accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_hash = linked_accounts[0].get('hashValue')\n",
    "print(account_hash)\n",
    "# sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_order = {\"orderType\": \"MARKET\",\n",
    "        \"session\": \"NORMAL\",\n",
    "        \"duration\": \"DAY\",\n",
    "        \"orderStrategyType\": \"SINGLE\",\n",
    "        \"orderLegCollection\": [\n",
    "            {\n",
    "                \"instruction\": \"BUY\",\n",
    "                \"quantity\": 1,\n",
    "                \"instrument\": {\n",
    "                    \"symbol\": \"ENVX\",\n",
    "                    \"assetType\": \"EQUITY\"\n",
    "                }\n",
    "            }\n",
    "            ]\n",
    "        }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.order_place(account_hash, buy_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placing the order created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resp = client.order_place(account_hash, order)\n",
    "# print(f\"Response code: {resp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the order ID. If the order is immediately filled, its id might not be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order_id = resp.headers.get('location', '/').split('/')[-1] \n",
    "# print(f\"Order id: {order_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get specific order details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.order_details(account_hash, order_id).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cancel specific order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(client.order_cancel(account_hash, order_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"session\": \"NORMAL\",                             # Indicates that the trading session is the normal market session\n",
    "  \"duration\": \"DAY\",                               # Specifies that the order is valid only for the trading day\n",
    "  \"orderType\": \"MARKET\",                           # Sets the order type to 'MARKET', meaning it will be executed at the current market price\n",
    "  \"cancelTime\": \"2024-09-17T02:32:09.008Z\",        # Time when the order will be canceled if not executed\n",
    "  \"complexOrderStrategyType\": \"NONE\",              # No complex order strategy is applied\n",
    "  \"quantity\": 0,                                   # Number of shares/contracts to be ordered (0 in this case, placeholder)\n",
    "  \"filledQuantity\": 0,                             # Number of shares/contracts that have been filled (none in this case)\n",
    "  \"remainingQuantity\": 0,                          # Number of shares/contracts remaining to be filled (none in this case)\n",
    "  \"destinationLinkName\": \"string\",                 # Placeholder for the routing destination of the order\n",
    "  \"releaseTime\": \"2024-09-17T02:32:09.008Z\",       # Time when the order will be released for execution\n",
    "  \"stopPrice\": 0,                                  # Stop price for stop orders (0 since it's not a stop order)\n",
    "  \"stopPriceLinkBasis\": \"MANUAL\",                  # Manual basis for setting the stop price\n",
    "  \"stopPriceLinkType\": \"VALUE\",                    # Stop price is set to a specific value\n",
    "  \"stopPriceOffset\": 0,                            # Offset value for the stop price (0 since no offset is defined)\n",
    "  \"stopType\": \"STANDARD\",                          # Type of stop order (STANDARD type is specified)\n",
    "  \"priceLinkBasis\": \"MANUAL\",                      # Manual basis for setting the order price\n",
    "  \"priceLinkType\": \"VALUE\",                        # Price is set to a specific value\n",
    "  \"price\": 0,                                      # Price of the order (0 since this is a market order)\n",
    "  \"taxLotMethod\": \"FIFO\",                          # First-In, First-Out (FIFO) method for calculating the tax lot\n",
    "  \"orderLegCollection\": [                          # Collection of order legs, each representing a separate part of the order\n",
    "    {\n",
    "      \"orderLegType\": \"EQUITY\",                    # Indicates that this order leg is for an equity (stock)\n",
    "      \"legId\": 0,                                  # Unique identifier for the order leg\n",
    "      \"instrument\": {\n",
    "        \"cusip\": \"string\",                         # CUSIP (placeholder) for identifying the instrument\n",
    "        \"symbol\": \"string\",                        # Stock symbol (placeholder)\n",
    "        \"description\": \"string\",                   # Description of the instrument (placeholder)\n",
    "        \"instrumentId\": 0,                         # Unique instrument ID (placeholder)\n",
    "        \"netChange\": 0,                            # Net price change of the instrument (0 for placeholder)\n",
    "        \"type\": \"SWEEP_VEHICLE\"                    # Type of the instrument (sweep vehicle is used here)\n",
    "      },\n",
    "      \"instruction\": \"BUY\",                        # Instruction for the leg, in this case to 'BUY'\n",
    "      \"positionEffect\": \"OPENING\",                 # Indicates that this order is opening a new position\n",
    "      \"quantity\": 0,                               # Number of shares/contracts for this order leg (0 as placeholder)\n",
    "      \"quantityType\": \"ALL_SHARES\",                # Specifies the quantity type for the order (all shares to be included)\n",
    "      \"divCapGains\": \"REINVEST\",                   # Dividend or capital gains strategy, reinvest dividends\n",
    "      \"toSymbol\": \"string\"                         # Placeholder for symbol conversion or destination\n",
    "    }\n",
    "  ],\n",
    "  \"activationPrice\": 0,                            # Activation price for the order (not applicable here, so it's 0)\n",
    "  \"specialInstruction\": \"ALL_OR_NONE\",             # Special instruction for the order, meaning it should execute fully or not at all\n",
    "  \"orderStrategyType\": \"SINGLE\",                   # Single-order strategy (not part of a larger order strategy)\n",
    "  \"orderId\": 0,                                    # Unique identifier for the order (0 as a placeholder)\n",
    "  \"cancelable\": false,                             # Indicates whether the order can be canceled (false here)\n",
    "  \"editable\": false,                               # Indicates whether the order can be edited (false here)\n",
    "  \"status\": \"AWAITING_PARENT_ORDER\",               # Status of the order (waiting for a parent order)\n",
    "  \"enteredTime\": \"2024-09-17T02:32:09.008Z\",       # Time the order was entered\n",
    "  \"closeTime\": \"2024-09-17T02:32:09.008Z\",         # Time the order was closed\n",
    "  \"accountNumber\": 0,                              # Account number associated with the order (placeholder value)\n",
    "  \"orderActivityCollection\": [                     # Collection of activities related to the order (e.g., executions)\n",
    "    {\n",
    "      \"activityType\": \"EXECUTION\",                 # Type of activity (execution of the order)\n",
    "      \"executionType\": \"FILL\",                     # Execution type, indicating that the order has been filled\n",
    "      \"quantity\": 0,                               # Number of shares/contracts executed (0 as placeholder)\n",
    "      \"orderRemainingQuantity\": 0,                 # Remaining quantity of the order (0 since none is left)\n",
    "      \"executionLegs\": [\n",
    "        {\n",
    "          \"legId\": 0,                              # ID for the execution leg\n",
    "          \"price\": 0,                              # Price at which the order was executed\n",
    "          \"quantity\": 0,                           # Quantity executed in this leg\n",
    "          \"mismarkedQuantity\": 0,                  # Quantity marked incorrectly, if any\n",
    "          \"instrumentId\": 0,                       # Instrument ID for this execution leg\n",
    "          \"time\": \"2024-09-17T02:32:09.008Z\"       # Time of the execution leg\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"replacingOrderCollection\": [                    # Collection of orders that are replacing this one (empty in this case)\n",
    "    \"string\"\n",
    "  ],\n",
    "  \"childOrderStrategies\": [                        # Collection of child order strategies (empty in this case)\n",
    "    \"string\"\n",
    "  ],\n",
    "  \"statusDescription\": \"string\"                    # Description of the current status of the order (placeholder)\n",
    "}\n",
    "\n",
    "{\n",
    "  \"session\": \"NORMAL\",                             # The trading session. Can take values: \"NORMAL\", \"AM\", \"PM\", \"SEAMLESS\".\n",
    "  \"duration\": \"DAY\",                               # Duration of the order. Can take values: \"DAY\", \"GTC\" (Good Till Canceled), \"FOK\" (Fill or Kill), \"IOC\" (Immediate or Cancel).\n",
    "  \"orderType\": \"MARKET\",                           # Type of order. Can take values: \"MARKET\", \"LIMIT\", \"STOP\", \"STOP_LIMIT\".\n",
    "  \"cancelTime\": \"2024-09-17T02:32:09.008Z\",        # Time when the order will be canceled if not executed. Format: ISO 8601.\n",
    "  \"complexOrderStrategyType\": \"NONE\",              # Strategy type for complex orders. Can take values: \"NONE\", \"COVERED\", \"VERTICAL\", \"CALENDAR\", etc.\n",
    "  \"quantity\": 0,                                   # Number of shares or contracts. Can be any positive integer.\n",
    "  \"filledQuantity\": 0,                             # Number of shares/contracts filled. Can be any non-negative integer.\n",
    "  \"remainingQuantity\": 0,                          # Number of shares/contracts remaining to be filled. Can be any non-negative integer.\n",
    "  \"destinationLinkName\": \"string\",                 # Routing destination of the order. Can take values like \"AUTO\" (automatically routed) or specific exchanges.\n",
    "  \"releaseTime\": \"2024-09-17T02:32:09.008Z\",       # Time the order will be released for execution. Format: ISO 8601.\n",
    "  \"stopPrice\": 0,                                  # The price at which a stop order is triggered. Can be any non-negative number.\n",
    "  \"stopPriceLinkBasis\": \"MANUAL\",                  # The basis for setting the stop price. Can take values: \"MANUAL\", \"LAST\", \"BID\", \"ASK\".\n",
    "  \"stopPriceLinkType\": \"VALUE\",                    # The method used to link the stop price. Can take values: \"VALUE\", \"PERCENT\", \"TICKS\".\n",
    "  \"stopPriceOffset\": 0,                            # Offset from the stop price. Can be any non-negative number.\n",
    "  \"stopType\": \"STANDARD\",                          # Type of stop order. Can take values: \"STANDARD\", \"TRAILING_STOP\".\n",
    "  \"priceLinkBasis\": \"MANUAL\",                      # Basis for the price of the order. Can take values: \"MANUAL\", \"LAST\", \"BID\", \"ASK\".\n",
    "  \"priceLinkType\": \"VALUE\",                        # Method used to link the price. Can take values: \"VALUE\", \"PERCENT\", \"TICKS\".\n",
    "  \"price\": 0,                                      # Order price (for limit orders). Can be any positive number.\n",
    "  \"taxLotMethod\": \"FIFO\",                          # Tax lot method. Can take values: \"FIFO\", \"LIFO\", \"HIFO\", \"SPECIFIC_LOT\".\n",
    "  \"orderLegCollection\": [                          # Collection of legs for multi-leg orders (e.g., options).\n",
    "    {\n",
    "      \"orderLegType\": \"EQUITY\",                    # Type of asset in the order leg. Can take values: \"EQUITY\", \"OPTION\", \"FUTURE\", \"BOND\".\n",
    "      \"legId\": 0,                                  # Unique identifier for the leg. Can be any non-negative integer.\n",
    "      \"instrument\": {\n",
    "        \"cusip\": \"string\",                         # CUSIP number for the security. This is a unique 9-character identifier for securities.\n",
    "        \"symbol\": \"string\",                        # Ticker symbol for the instrument (e.g., \"AAPL\").\n",
    "        \"description\": \"string\",                   # Text description of the instrument (e.g., \"Apple Inc.\").\n",
    "        \"instrumentId\": 0,                         # Unique identifier for the instrument. Can be any non-negative integer.\n",
    "        \"netChange\": 0,                            # Net price change of the instrument. Can be any number (positive or negative).\n",
    "        \"type\": \"SWEEP_VEHICLE\"                    # Type of instrument. Can take values: \"EQUITY\", \"OPTION\", \"FUTURE\", \"SWEEP_VEHICLE\".\n",
    "      },\n",
    "      \"instruction\": \"BUY\",                        # Instruction for this leg. Can take values: \"BUY\", \"SELL\", \"BUY_TO_COVER\", \"SELL_SHORT\".\n",
    "      \"positionEffect\": \"OPENING\",                 # Indicates if the leg is opening or closing a position. Can take values: \"OPENING\", \"CLOSING\".\n",
    "      \"quantity\": 0,                               # Number of shares/contracts in this leg. Can be any positive integer.\n",
    "      \"quantityType\": \"ALL_SHARES\",                # Specifies the type of quantity. Can take values: \"ALL_SHARES\", \"DOLLARS\".\n",
    "      \"divCapGains\": \"REINVEST\",                   # Dividend/capital gains instructions. Can take values: \"REINVEST\", \"CASH\".\n",
    "      \"toSymbol\": \"string\"                         # Conversion symbol, if applicable (e.g., stock split).\n",
    "    }\n",
    "  ],\n",
    "  \"activationPrice\": 0,                            # Price at which the order becomes active (for stop orders). Can be any positive number.\n",
    "  \"specialInstruction\": \"ALL_OR_NONE\",             # Special instruction for the order. Can take values: \"ALL_OR_NONE\", \"NONE\", \"DO_NOT_REDUCE\".\n",
    "  \"orderStrategyType\": \"SINGLE\",                   # Strategy type for the order. Can take values: \"SINGLE\", \"OCO\" (One Cancels Other), \"OTO\" (One Triggers Other).\n",
    "  \"orderId\": 0,                                    # Unique identifier for the order. Can be any non-negative integer.\n",
    "  \"cancelable\": false,                             # Indicates whether the order can be canceled. Boolean: true/false.\n",
    "  \"editable\": false,                               # Indicates whether the order can be edited. Boolean: true/false.\n",
    "  \"status\": \"AWAITING_PARENT_ORDER\",               # Status of the order. Can take values: \"AWAITING_PARENT_ORDER\", \"FILLED\", \"CANCELED\", etc.\n",
    "  \"enteredTime\": \"2024-09-17T02:32:09.008Z\",       # Time when the order was entered. Format: ISO 8601.\n",
    "  \"closeTime\": \"2024-09-17T02:32:09.008Z\",         # Time when the order was closed (if applicable). Format: ISO 8601.\n",
    "  \"accountNumber\": 0,                              # Account number associated with the order. Can be any valid account number.\n",
    "  \"orderActivityCollection\": [                     # Collection of activities related to the order (e.g., executions).\n",
    "    {\n",
    "      \"activityType\": \"EXECUTION\",                 # Type of activity. Can take values: \"EXECUTION\", \"CANCEL\", etc.\n",
    "      \"executionType\": \"FILL\",                     # Execution type, meaning the order was filled. Can also be \"PARTIAL_FILL\".\n",
    "      \"quantity\": 0,                               # Quantity executed. Can be any positive integer.\n",
    "      \"orderRemainingQuantity\": 0,                 # Remaining quantity after this execution. Can be any positive integer.\n",
    "      \"executionLegs\": [\n",
    "        {\n",
    "          \"legId\": 0,                              # ID of the execution leg. Can be any non-negative integer.\n",
    "          \"price\": 0,                              # Price at which the execution took place. Can be any positive number.\n",
    "          \"quantity\": 0,                           # Quantity executed in this leg. Can be any positive integer.\n",
    "          \"mismarkedQuantity\": 0,                  # Quantity that was incorrectly marked (if applicable). Can be any positive integer.\n",
    "          \"instrumentId\": 0,                       # Unique identifier for the instrument executed. Can be any non-negative integer.\n",
    "          \"time\": \"2024-09-17T02:32:09.008Z\"       # Time the execution occurred. Format: ISO 8601.\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"replacingOrderCollection\": [                    # Collection of orders that are replacing this one (if applicable). Empty in this case.\n",
    "    \"string\"\n",
    "  ],\n",
    "  \"childOrderStrategies\": [                        # Collection of child order strategies (if applicable). Empty in this case.\n",
    "    \"string\"\n",
    "  ],\n",
    "  \"statusDescription\": \"string\"                    # Description of the current order status. Text string.\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
