{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"color:red;\"><b>John's Swing-Trading Code (Python Translation)<b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h3 style=\"color:yellow;\">Importing Modules and Packages</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "# from datetime import datetime\n",
    "from scipy.stats import linregress\n",
    "import time\n",
    "import pandas_market_calendars as mcal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import sys\n",
    "from tqdm import tqdm  # optional: for a progress bar\n",
    "import warnings\n",
    "import schwabdev\n",
    "from dotenv import load_dotenv\n",
    "warnings.filterwarnings(\"ignore\")  # silence any pandas SettingWithCopy warnings\n",
    "import requests\n",
    "from io import StringIO\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.drawing.image import Image as XLImage\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 style=\"color:purple;\">Authentication</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h3 style=\"color:yellow;\">Authentication for Schwab</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load environment variables from .env file for authentification purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the display options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display options\n",
    "# Set the global float format to 4 decimal places\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.4f}')\n",
    "pd.options.display.float_format = '{:,.4f}'.format # Format numerical output to have certain number of decimals\n",
    "# pd.options.display.float_format = None # Reset to default numerical output formatting\n",
    "pd.set_option('display.width', 2000) # Set the display width to a large number\n",
    "pd.set_option('display.max_colwidth', 1000) # Set max column width to a large number\n",
    "\n",
    "pd.set_option('display.max_columns', None) # Displays all columns\n",
    "# pd.set_option('display.max_rows', None) # Displays all rows                             \n",
    "# pd.reset_option('display.max_columns') # Display default abbreviated columns\n",
    "pd.reset_option('display.max_rows') # Display default abbreviated rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the client object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "App Key: 2C7Jh6zt5QdlSb0N5RnhWpaiEzKFr150\n",
      "App Secret: OQGxH0GkD5bAEMA4\n",
      "Callback URL: https://127.0.0.1\n",
      "Client initialized!\n"
     ]
    }
   ],
   "source": [
    "app_key = os.getenv('app_key')\n",
    "app_secret = os.getenv('app_secret')\n",
    "callback_url = os.getenv('callback_url')\n",
    "\n",
    "# Print them to verify (avoid printing sensitive info in production)\n",
    "print(f\"App Key: {app_key}\")\n",
    "print(f\"App Secret: {app_secret}\")\n",
    "print(f\"Callback URL: {callback_url}\")\n",
    "\n",
    "# Now proceed to initialize the client\n",
    "client = schwabdev.Client(app_key, app_secret, callback_url)\n",
    "print('Client initialized!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this when in need of updating the refresh token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.update_tokens(force=True)\n",
    "# client = schwabdev.Client(app_key, app_secret, callback_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h3 style=\"color:yellow;\">Authentication for GBQ</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines `config()` to return the service account key path for a given user (`reese` or `ben`); raises an error for others. Used for BigQuery authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_proj_id = 'stock-chipper-87578' \n",
    "\n",
    "def config(username=None):\n",
    "    if username == 'reese':\n",
    "        file_dir = \"C:/Users/rsmcd/OneDrive/Desktop/Trade Review/stock-chipper-research/\"\n",
    "        credential_file = 'stock-chipper-87578-ec8b427fca6a.json'\n",
    "    elif username == 'ben':\n",
    "        file_dir = \"C:/Users/benwo/Documents/repos/stock-chipper-app/creds/\"\n",
    "        credential_file = \"stock-chipper-87578-b17ad3f7e6e1.json\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognized or missing username: {username}\")\n",
    "    \n",
    "    return file_dir + credential_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selects user credentials based on context: uses CLI argument if run as `__main__`, defaults to `\"reese\"` otherwise. Falls back to `\"reese\"` on error. Result (`private_key`) is used for BigQuery authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falling back to default username (reese)\n"
     ]
    }
   ],
   "source": [
    "# Use default when in Jupyter or if __main__ but no args\n",
    "try:\n",
    "    if __name__ == \"__main__\":\n",
    "        if len(sys.argv) <= 1:\n",
    "            raise ValueError(\"No username provided via CLI args\")\n",
    "        private_key = config(sys.argv[1])\n",
    "    else:\n",
    "        private_key = config(\"reese\")  # Default for notebook\n",
    "except Exception as e:\n",
    "    print(\"Falling back to default username (reese)\")\n",
    "    private_key = config(\"reese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 style=\"color:purple;\">Defining Functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticker_data(ticker, date_pull_begin=None, local=False, local_folder=\"local-ticker-data-schwab\", credentials_path=None, project_id=None):\n",
    "    \"\"\"\n",
    "    Retrieves historical daily OHLC data for a given ticker, either from a local file or from BigQuery.\n",
    "\n",
    "    If `local` is True, the function reads the data from a local TSV file under the specified folder.\n",
    "    Otherwise, it queries the `stocks_candle_1day` table in BigQuery.\n",
    "\n",
    "    Args:\n",
    "        ticker (str): The stock ticker symbol to retrieve data for.\n",
    "        date_pull_begin (datetime-like, optional): The earliest date to pull data from.\n",
    "            Defaults to two years before today if not provided.\n",
    "        local (bool): Whether to read from a local file instead of querying BigQuery.\n",
    "        local_folder (str): Name of the subfolder under 'data/' containing local files.\n",
    "            Options include \"local-ticker-data-gbq\" or \"local-ticker-data-schwab\".\n",
    "        credentials_path (str, optional): Full path to the GCP service account JSON key.\n",
    "            Required if `local` is False.\n",
    "        project_id (str, optional): GCP project ID. Required if `local` is False.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing columns ['ticker', 'date_time', 'high', 'low'].\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `credentials_path` or `project_id` is missing when `local` is False.\n",
    "    \"\"\"\n",
    "    if local:\n",
    "        file_path = Path(\"strategies/swing-trading/data\") / local_folder / f\"{ticker}.txt\"\n",
    "        return pd.read_csv(file_path, sep=\"\\t\", parse_dates=[\"date_time\"])\n",
    "\n",
    "    if date_pull_begin is None:\n",
    "        date_pull_begin = pd.Timestamp.today() - pd.DateOffset(years=2)                             #### hardcoding !!!\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        SELECT ticker, date_time, open, high, low, close, volume\n",
    "        FROM `stock-chipper-87578.first_rate.stocks_candle_1day`\n",
    "        WHERE date_time > '{date_pull_begin:%Y-%m-%d}' AND ticker = '{ticker}'\n",
    "        ORDER BY date_time ASC\n",
    "    \"\"\"\n",
    "\n",
    "    return run_sql_query(sql, project_id=project_id, credentials_path=credentials_path)\n",
    "\n",
    "def evaluate_recent_performance(data, month_lookback=3):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of recent evaluation-period returns, giving more\n",
    "    weight to the most recent months.\n",
    "\n",
    "    This is used to score the recent performance of a trading strategy over\n",
    "    the past `month_lookback` months. Each evaluation period is weighted by\n",
    "    recency: newer periods receive higher weight using the formula\n",
    "    `weight = 1 / (months_ago + 1)`.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Must contain 'evaluation_period_end' (datetime-like) and\n",
    "            'evaluation_return' (float, can include NaNs).\n",
    "        month_lookback (int): Number of months to look back from the latest evaluation period.\n",
    "\n",
    "    Returns:\n",
    "        float: Weighted average of evaluation returns over the lookback window.\n",
    "            Returns 0.0 if no qualifying periods exist.\n",
    "    \"\"\"\n",
    "    data = data.copy()                                                                                  # Work on a copy to avoid modifying original\n",
    "    data[\"evaluation_period_end\"] = pd.to_datetime(data[\"evaluation_period_end\"])                       # Ensure datetime format for filtering\n",
    "\n",
    "    last_period = data[\"evaluation_period_end\"].max()                                                   # Get the most recent evaluation period\n",
    "    cutoff = last_period - pd.DateOffset(months=month_lookback)                                         # Calculate earliest period to include in lookback\n",
    "\n",
    "    recent = data[                                                                                      # Filter to only rows within the lookback window\n",
    "        (data[\"evaluation_period_end\"] >= cutoff) &\n",
    "        (data[\"evaluation_period_end\"] <= last_period)\n",
    "    ][[\"evaluation_period_end\", \"evaluation_return\"]].copy()                                            # Keep only relevant columns\n",
    "\n",
    "    if recent.empty:                                                                                   # Return early if no data in lookback window\n",
    "        return 0.0\n",
    "\n",
    "    recent[\"evaluation_return\"] = recent[\"evaluation_return\"].fillna(0.0)                               # Replace missing returns with 0\n",
    "\n",
    "    recent[\"months_ago\"] = ((last_period - recent[\"evaluation_period_end\"]).dt.days / 30) + 1          # Estimate recency in months (approx.)\n",
    "    recent[\"wt\"] = 1 / recent[\"months_ago\"]                                                             # Assign higher weight to more recent periods\n",
    "    recent[\"wt\"] = recent[\"wt\"] / recent[\"wt\"].sum()                                                    # Normalize weights to sum to 1\n",
    "\n",
    "    recent[\"wtd_returns\"] = recent[\"wt\"] * recent[\"evaluation_return\"]                                  # Compute weighted return for each row\n",
    "\n",
    "    return recent[\"wtd_returns\"].sum()                                                                  # Return the total weighted average\n",
    "\n",
    "def run_sql_query(sql, project_id, credentials_path=None):\n",
    "    \"\"\"\n",
    "    Executes a SQL query against Google BigQuery using a service account.\n",
    "\n",
    "    This function creates an authenticated BigQuery client using a local\n",
    "    service account JSON credentials file, submits the SQL query, waits\n",
    "    for it to complete, and returns the results as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        sql (str): The SQL query string to be executed.\n",
    "        project_id (str): The GCP project ID associated with the BigQuery dataset.\n",
    "        credentials_path (str): Full path to the service account JSON credentials file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The query results as a DataFrame. Each column corresponds to a \n",
    "                      field in the SELECT statement of the query.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `credentials_path` is not provided.\n",
    "    \"\"\"\n",
    "    if credentials_path is None:\n",
    "        raise ValueError(\"credentials_path must be provided\")\n",
    "\n",
    "    credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "    client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "    query_job = client.query(sql)\n",
    "    result = query_job.result()\n",
    "    return result.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 style=\"color:purple;\">Retrieve Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h3 style=\"color:yellow;\">Getting data from Schwab</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fetching US Equity Tickers from NASDAQ Trader**\n",
    "\n",
    "The `get_all_tickers()` function retrieves publicly traded US equity tickers.\n",
    "\n",
    "1. **Data Sources**:\n",
    "\n",
    "   * NASDAQ-listed: `nasdaqlisted.txt`\n",
    "   * Other exchanges (NYSE, AMEX, etc.): `otherlisted.txt`\n",
    "\n",
    "2. **Parsing**:\n",
    "\n",
    "   * NASDAQ symbols are read and labeled as `\"NASDAQ\"`.\n",
    "   * Other symbols are mapped to full exchange names via a code dictionary.\n",
    "\n",
    "ðŸ“Œ *Uncomment the concat line and comment the line below it to include all exchanges as originally intended.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 4977 tickers.\n",
      "                                 ticker exchange\n",
      "0                                  AACB   NASDAQ\n",
      "1                                 AACBR   NASDAQ\n",
      "2                                 AACBU   NASDAQ\n",
      "3                                  AACG   NASDAQ\n",
      "4                                  AACI   NASDAQ\n",
      "...                                 ...      ...\n",
      "4973                               ZYBT   NASDAQ\n",
      "4974                               ZYME   NASDAQ\n",
      "4975                               ZYXI   NASDAQ\n",
      "4976                                ZZZ   NASDAQ\n",
      "4977  File Creation Time: 0716202514:01   NASDAQ\n",
      "\n",
      "[4977 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_all_tickers():\n",
    "    \"\"\"\n",
    "    Fetches all listed US equity tickers from NASDAQ Trader and returns a DataFrame\n",
    "    with their associated full exchange name.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns ['ticker', 'exchange']\n",
    "    \"\"\"\n",
    "    # URLs to fetch data\n",
    "    nasdaq_url = 'https://www.nasdaqtrader.com/dynamic/SymDir/nasdaqlisted.txt'\n",
    "    other_url = 'https://www.nasdaqtrader.com/dynamic/SymDir/otherlisted.txt'\n",
    "\n",
    "    # Download NASDAQ-listed tickers\n",
    "    nasdaq_response = requests.get(nasdaq_url)\n",
    "    nasdaq_data = pd.read_csv(StringIO(nasdaq_response.text), sep='|')\n",
    "    nasdaq_df = nasdaq_data[['Symbol']].dropna().copy()\n",
    "    nasdaq_df.columns = ['ticker']\n",
    "    nasdaq_df['exchange'] = 'NASDAQ'\n",
    "\n",
    "    # Download other-listed tickers (NYSE, AMEX, etc.)\n",
    "    other_response = requests.get(other_url)\n",
    "    other_data = pd.read_csv(StringIO(other_response.text), sep='|')\n",
    "    other_df = other_data[['ACT Symbol', 'Exchange']].dropna().copy()\n",
    "    other_df.columns = ['ticker', 'exchange']\n",
    "\n",
    "    # Map exchange abbreviations to full names\n",
    "    exchange_map = {\n",
    "        'A': 'NYSE American',   # Formerly AMEX\n",
    "        'N': 'NYSE',\n",
    "        'P': 'NYSE Arca',\n",
    "        'Z': 'BATS',\n",
    "        'V': 'IEX'\n",
    "    }\n",
    "    other_df['exchange'] = other_df['exchange'].map(exchange_map).fillna('Other')\n",
    "\n",
    "    # Combine both sources\n",
    "    # all_tickers_df = pd.concat([nasdaq_df, other_df], ignore_index=True)\n",
    "\n",
    "    # Combine both sources\n",
    "    all_tickers_df = nasdaq_df\n",
    "\n",
    "    return all_tickers_df\n",
    "\n",
    "all_tickers_schwab = get_all_tickers()\n",
    "print(f\"Retrieved {len(all_tickers_schwab)} tickers.\")\n",
    "print(all_tickers_schwab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block defines and calls a function that randomly selects up to `n` tickers from a given DataFrame of tickers and exchanges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected tickers:\n",
      "   ticker exchange\n",
      "0    SERV   NASDAQ\n",
      "1    PABU   NASDAQ\n",
      "2    RPAY   NASDAQ\n",
      "3   CDTTW   NASDAQ\n",
      "4    QRVO   NASDAQ\n",
      "5    RDVY   NASDAQ\n",
      "6     SLP   NASDAQ\n",
      "7    DLPN   NASDAQ\n",
      "8    SLDP   NASDAQ\n",
      "9    HURN   NASDAQ\n",
      "10   LASR   NASDAQ\n",
      "11   SLQD   NASDAQ\n",
      "12  MRNOW   NASDAQ\n",
      "13   MESO   NASDAQ\n",
      "14   KMLI   NASDAQ\n",
      "15  PMTRW   NASDAQ\n",
      "16   DOMH   NASDAQ\n",
      "17  ESLAW   NASDAQ\n",
      "18  DGICB   NASDAQ\n",
      "19   EVCM   NASDAQ\n"
     ]
    }
   ],
   "source": [
    "def select_random_tickers(ticker_df, n=5):\n",
    "    \"\"\"\n",
    "    Randomly selects n tickers from a DataFrame of tickers and exchanges.\n",
    "\n",
    "    Args:\n",
    "        ticker_df (pd.DataFrame): DataFrame with columns ['ticker', 'exchange'].\n",
    "        n (int): Number of tickers to select.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with n randomly selected tickers.\n",
    "    \"\"\"\n",
    "    return ticker_df.sample(n=min(n, len(ticker_df))).reset_index(drop=True)\n",
    "\n",
    "schwab_ticker_num = len(all_tickers_schwab)  # Use the total number of tickers available\n",
    "# schwab_ticker_num = 200 # Limit to 100 tickers for testing purposes\n",
    "random_tickers = select_random_tickers(all_tickers_schwab, n=schwab_ticker_num)\n",
    "print(\"Randomly selected tickers:\")\n",
    "print(random_tickers[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine randomly selected and major tickers into one DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_ticker_list = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\"]\n",
    "big_tickers = pd.DataFrame({\n",
    "    'ticker': big_ticker_list,\n",
    "    'exchange': ['NASDAQ'] * 5\n",
    "})\n",
    "\n",
    "# Combine random sample with manually selected big tickers\n",
    "test_tickers_df = pd.concat([random_tickers, big_tickers], ignore_index=True)\n",
    "\n",
    "# Deduplicate by 'ticker', keeping the first occurrence\n",
    "test_tickers_df = test_tickers_df.drop_duplicates(subset='ticker').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schwab_data_for_last_year(symbol, exchange=None):\n",
    "    \"\"\"\n",
    "    Fetch 1 year of daily price data from the Schwab API.\n",
    "\n",
    "    Args:\n",
    "        symbol (str): Ticker symbol to fetch.\n",
    "        exchange (str, optional): Exchange name to attach to result.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: List of candle dictionaries with ticker and exchange info.\n",
    "\n",
    "    Schwab API - Valid Frequency Parameters for `client.price_history()`:\n",
    "        frequencyType='minute':  frequency = 1, 5, 10, 15, 30\n",
    "        frequencyType='daily':   frequency = 1\n",
    "        frequencyType='weekly':  frequency = 1\n",
    "        frequencyType='monthly': frequency = 1\n",
    "\n",
    "    Notes:\n",
    "        - startDate and endDate should be datetime objects. The client handles timestamp conversion.\n",
    "        - Only minute-level data supports multiple frequency values.\n",
    "    \"\"\"\n",
    "    end = datetime.now()\n",
    "    start = end - timedelta(days=365)\n",
    "\n",
    "    all_candles = []\n",
    "\n",
    "    while start < end:\n",
    "        window_end = min(start + timedelta(days=365), end)\n",
    "        start = pd.to_datetime(start)\n",
    "        window_end = pd.to_datetime(window_end)\n",
    "\n",
    "        try:\n",
    "            response = client.price_history(\n",
    "                symbol=symbol,\n",
    "                periodType='year',             # Comment out to get intraday candles\n",
    "                period=1,                      # Comment out to get intraday candles\n",
    "                frequencyType='daily',         # Changed from 'minute', Change back to get intraday candles\n",
    "                frequency=1,                   # Changed from 30, Change back to get intraday candles\n",
    "                # startDate=start,             # Uncomment to get intraday candles\n",
    "                # endDate=window_end,          # Uncomment to get intraday candles\n",
    "                needExtendedHoursData=False\n",
    "            )\n",
    "            json_data = response.json()\n",
    "\n",
    "            if 'candles' in json_data and json_data['candles']:\n",
    "                for candle in json_data['candles']:\n",
    "                    candle[\"ticker\"] = symbol\n",
    "                    candle[\"exchange\"] = exchange\n",
    "                    all_candles.append(candle)\n",
    "            else:\n",
    "                print(f\"No candles returned for {symbol} in this chunk.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {symbol}: {e}\")\n",
    "\n",
    "        time.sleep(1)\n",
    "        start = window_end + timedelta(days=1)\n",
    "\n",
    "    return all_candles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for reference to the prior function just up here ^."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # :param periodType: period type (\"day\"|\"month\"|\"year\"|\"ytd\")\n",
    "        # :type periodType: str\n",
    "        # :param period: period\n",
    "        # :type period: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull all data and include exchange info from Schwab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-16 13:21:52\n",
      "4977\n",
      "[1/4977] SERV: NASDAQ: Retrieved 251 candles\n",
      "[2/4977] PABU: NASDAQ: Retrieved 251 candles\n",
      "[3/4977] RPAY: NASDAQ: Retrieved 251 candles\n",
      "[4/4977] CDTTW: NASDAQ: Retrieved 167 candles\n",
      "[5/4977] QRVO: NASDAQ: Retrieved 251 candles\n",
      "[6/4977] RDVY: NASDAQ: Retrieved 251 candles\n",
      "[7/4977] SLP: NASDAQ: Retrieved 251 candles\n",
      "[8/4977] DLPN: NASDAQ: Retrieved 251 candles\n",
      "[9/4977] SLDP: NASDAQ: Retrieved 251 candles\n",
      "[10/4977] HURN: NASDAQ: Retrieved 251 candles\n",
      "[11/4977] LASR: NASDAQ: Retrieved 251 candles\n",
      "[12/4977] SLQD: NASDAQ: Retrieved 251 candles\n",
      "[13/4977] MRNOW: NASDAQ: Retrieved 194 candles\n",
      "[14/4977] MESO: NASDAQ: Retrieved 249 candles\n",
      "[15/4977] KMLI: NASDAQ: Retrieved 22 candles\n",
      "[16/4977] PMTRW: NASDAQ: Retrieved 13 candles\n",
      "[17/4977] DOMH: NASDAQ: Retrieved 251 candles\n",
      "[18/4977] ESLAW: NASDAQ: Retrieved 103 candles\n",
      "[19/4977] DGICB: NASDAQ: Retrieved 155 candles\n",
      "[20/4977] EVCM: NASDAQ: Retrieved 251 candles\n",
      "[21/4977] GROW: NASDAQ: Retrieved 251 candles\n",
      "[22/4977] FORM: NASDAQ: Retrieved 251 candles\n",
      "[23/4977] SXTC: NASDAQ: Retrieved 251 candles\n",
      "[24/4977] UFIV: NASDAQ: Retrieved 251 candles\n",
      "[25/4977] CXAI: NASDAQ: Retrieved 251 candles\n",
      "[26/4977] ESTA: NASDAQ: Retrieved 251 candles\n",
      "[27/4977] IMRX: NASDAQ: Retrieved 251 candles\n",
      "[28/4977] CUPR: NASDAQ: Retrieved 65 candles\n",
      "[29/4977] NOTV: NASDAQ: Retrieved 251 candles\n",
      "[30/4977] PFI: NASDAQ: Retrieved 251 candles\n",
      "[31/4977] FTAI: NASDAQ: Retrieved 251 candles\n",
      "[32/4977] PNQI: NASDAQ: Retrieved 251 candles\n",
      "[33/4977] DXR: NASDAQ: Retrieved 237 candles\n",
      "[34/4977] BTGD: NASDAQ: Retrieved 185 candles\n",
      "[35/4977] HPKEW: NASDAQ: Retrieved 187 candles\n",
      "[36/4977] EFSC: NASDAQ: Retrieved 251 candles\n",
      "[37/4977] RTXG: NASDAQ: Retrieved 26 candles\n",
      "[38/4977] ODD: NASDAQ: Retrieved 251 candles\n",
      "[39/4977] AUUD: NASDAQ: Retrieved 251 candles\n",
      "[40/4977] LUNG: NASDAQ: Retrieved 251 candles\n",
      "[41/4977] QHDG: NASDAQ: Retrieved 225 candles\n",
      "[42/4977] CRNX: NASDAQ: Retrieved 251 candles\n",
      "[43/4977] FUNC: NASDAQ: Retrieved 251 candles\n",
      "[44/4977] SPKL: NASDAQ: Retrieved 213 candles\n",
      "[45/4977] GSUN: NASDAQ: Retrieved 242 candles\n",
      "[46/4977] IPOD: NASDAQ: Retrieved 19 candles\n",
      "[47/4977] MSPR: NASDAQ: Retrieved 251 candles\n",
      "[48/4977] SDHIR: NASDAQ: Retrieved 13 candles\n",
      "[49/4977] NTGR: NASDAQ: Retrieved 251 candles\n",
      "[50/4977] RAA: NASDAQ: Retrieved 96 candles\n",
      "[51/4977] IPODU: NASDAQ: Retrieved 41 candles\n",
      "[52/4977] LPAAW: NASDAQ: Retrieved 145 candles\n",
      "[53/4977] OPAL: NASDAQ: Retrieved 251 candles\n",
      "[54/4977] IMXI: NASDAQ: Retrieved 251 candles\n",
      "[55/4977] EEFT: NASDAQ: Retrieved 251 candles\n",
      "[56/4977] SJCP: NASDAQ: Retrieved 207 candles\n",
      "[57/4977] SOGP: NASDAQ: Retrieved 251 candles\n",
      "[58/4977] RMSGW: NASDAQ: Retrieved 134 candles\n",
      "[59/4977] BRKR: NASDAQ: Retrieved 251 candles\n",
      "[60/4977] GLOW: NASDAQ: Retrieved 249 candles\n",
      "[61/4977] BHFAO: NASDAQ: Retrieved 251 candles\n",
      "[62/4977] OUSTW: NASDAQ: Retrieved 233 candles\n",
      "[63/4977] LANDO: NASDAQ: Retrieved 251 candles\n",
      "[64/4977] AMD: NASDAQ: Retrieved 251 candles\n",
      "[65/4977] SOFI: NASDAQ: Retrieved 251 candles\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "all_candles_combined = []\n",
    "num_tickers = len(test_tickers_df)\n",
    "print(num_tickers)\n",
    "\n",
    "for i, (_, row) in enumerate(test_tickers_df.iterrows(), start=1):\n",
    "    ticker = row['ticker']\n",
    "    exchange = row['exchange']\n",
    "\n",
    "    # print(f\"[{i}/{num_tickers}] Fetching {ticker} ({exchange})...\")\n",
    "    \n",
    "    try:\n",
    "        candles = get_schwab_data_for_last_year(ticker, exchange)\n",
    "        all_candles_combined.extend(candles)\n",
    "        print(f\"[{i}/{num_tickers}] {ticker}: {exchange}: Retrieved {len(candles)} candles\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve {ticker}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the combined candle list into a DataFrame, parses timestamps, sorts the data, and then splits it into a dictionary of DataFramesâ€”one per tickerâ€”for easy access and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_format = True\n",
    "\n",
    "# Convert to DataFrame\n",
    "candles_df = pd.DataFrame(all_candles_combined)\n",
    "if daily_format:\n",
    "    candles_df['date_time'] = pd.to_datetime(candles_df['datetime'], unit='ms').dt.normalize()\n",
    "else:\n",
    "    candles_df['date_time'] = pd.to_datetime(candles_df['datetime'], unit='ms')\n",
    "    \n",
    "candles_df = candles_df.sort_values(['ticker', 'date_time']).reset_index(drop=True)\n",
    "\n",
    "# Split candles_df into a dictionary of DataFrames by ticker\n",
    "candles_dict_schwab = {\n",
    "    ticker: df.reset_index(drop=True)\n",
    "    for ticker, df in candles_df.groupby(\"ticker\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(candles_df)\n",
    "display(candles_dict_schwab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `daily_format` is `False`, this filters out future timestamps from `candles_df` based on the current time. This avoids certain oddities that were causing problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not daily_format:\n",
    "    now = pd.Timestamp.now().replace(tzinfo=None)\n",
    "    candles_df = candles_df[candles_df[\"date_time\"] <= now]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_candle_summary(candles_df, daily_format=True):\n",
    "    \"\"\"\n",
    "    Builds a summary DataFrame with information per ticker.\n",
    "\n",
    "    Args:\n",
    "        candles_df (pd.DataFrame): Full candle data with 'date_time' and other fields.\n",
    "        daily_format (bool): Whether 'date_time' is a date (no time component).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with summary statistics per ticker.\n",
    "    \"\"\"\n",
    "    summary = (\n",
    "        candles_df\n",
    "        .groupby('ticker')\n",
    "        .agg(\n",
    "            exchange=('exchange', 'first'),\n",
    "            candle_count=('date_time', 'count'),\n",
    "            first_candle=('date_time', 'min'),\n",
    "            last_candle=('date_time', 'max'),\n",
    "            average_close_price=('close', 'mean'),\n",
    "            average_volume=('volume', 'mean')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Estimate average market cap\n",
    "    summary['liquidity_proxy'] = summary['average_close_price'] * summary['average_volume']\n",
    "\n",
    "    # Calculate density relative to max candle count\n",
    "    max_candles = summary['candle_count'].max()\n",
    "    summary['candle_density'] = summary['candle_count'] / max_candles\n",
    "\n",
    "    # Convert to datetime if date-only format is used\n",
    "    if daily_format:\n",
    "        summary['first_candle'] = pd.to_datetime(summary['first_candle'])\n",
    "        summary['last_candle'] = pd.to_datetime(summary['last_candle'])\n",
    "    else:\n",
    "        # Ensure datetime is clean and not timezone-aware\n",
    "        summary['first_candle'] = pd.to_datetime(summary['first_candle']).dt.tz_localize(None)\n",
    "        summary['last_candle'] = pd.to_datetime(summary['last_candle']).dt.tz_localize(None)\n",
    "\n",
    "    # Compute duration metrics\n",
    "    duration = summary['last_candle'] - summary['first_candle']\n",
    "    summary['total_hours'] = duration.dt.total_seconds() / 3600\n",
    "    summary['total_days'] = duration.dt.total_seconds() / (3600 * 24)\n",
    "    summary['total_weeks'] = duration.dt.total_seconds() / (3600 * 24 * 7)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a summary of the candle data by calling `build_candle_summary` on `candles_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None) # Displays all rows\n",
    "pd.reset_option('display.max_rows') # Display default abbreviated rows\n",
    "\n",
    "summary_candles_df = build_candle_summary(candles_df, daily_format=daily_format)\n",
    "print(len(summary_candles_df), \"tickers with data\")\n",
    "display(summary_candles_df.sort_values(\"average_volume\", ascending=False))\n",
    "\n",
    "pd.reset_option('display.max_rows') # Display default abbreviated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(data, column, bins=100, log_scale=False):\n",
    "    \"\"\"\n",
    "    Plots a histogram for a specified column in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame containing the data.\n",
    "        column (str): Column name to plot.\n",
    "        bins (int): Number of histogram bins.\n",
    "        log_scale (bool): Whether to use log scale for the x-axis.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(data[column].dropna(), bins=bins, edgecolor='black')\n",
    "    plt.title(f\"Histogram of {column}\")\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    if log_scale:\n",
    "        plt.xscale(\"log\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(summary_candles_df, \"average_close_price\")\n",
    "plot_histogram(summary_candles_df, \"average_volume\")\n",
    "plot_histogram(summary_candles_df, \"liquidity_proxy\")\n",
    "plot_histogram(summary_candles_df, \"candle_density\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters `summary_candles_df` to include only tickers with `candle_density` â‰¥ `candle_density_threshold`, removes duplicates, and converts the result to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candle_density_threshold = 0.5\n",
    "average_close_price_threshold = 1_000\n",
    "average_volume_threshold = 100_000\n",
    "liquidity_proxy_threshold = 2_500_000\n",
    "\n",
    "filtered_summary_candles_df = summary_candles_df[\n",
    "    (summary_candles_df[\"candle_density\"] >= candle_density_threshold) &\n",
    "    (summary_candles_df[\"average_close_price\"] < average_close_price_threshold) &\n",
    "    (summary_candles_df[\"average_volume\"] > average_volume_threshold) &\n",
    "    (summary_candles_df[\"liquidity_proxy\"] > liquidity_proxy_threshold)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "tickers_to_test_schwab = filtered_summary_candles_df[\"ticker\"].drop_duplicates().tolist()\n",
    "\n",
    "display(filtered_summary_candles_df.sort_values(\"liquidity_proxy\", ascending=False))\n",
    "print(len(tickers_to_test_schwab), \"tickers to test\")\n",
    "display(tickers_to_test_schwab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h3 style=\"color:yellow;\">Getting data from GBQ</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration and Directory Setup\n",
    "\n",
    "Defines key flags controlling the behavior of the script, such as whether to rebuild results, use local or remote data, and how many tickers to analyze. It also sets up directory paths for saving data, and ensures the necessary folders exist before proceeding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config Flags\n",
    "get_ticker_list_from_gbq = True\n",
    "use_local_data_gbq = False\n",
    "build_results_gbq = True\n",
    "fresh_start_gbq = True\n",
    "num_to_test_gbq = 10 #schwab_ticker_num - 1  # If <0, use all\n",
    "output_stub_gbq = \"20240101_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path setup\n",
    "DATA_DIR = Path(\"strategies/swing-trading/data\")\n",
    "WALK_FORWARD_DIR_SCHWAB = DATA_DIR / \"walk-forward-schwab\"                              ### Consider adding later (or sooner and do the data dirs at the same time) !!!\n",
    "WALK_FORWARD_DIR_GBQ = DATA_DIR / \"walk-forward-gbq\"\n",
    "MINUTE_TICKERS_FILE = DATA_DIR / \"tickers_random_index_list.txt\"\n",
    "\n",
    "# Delete the entire DATA_DIR folder and all its contents\n",
    "if DATA_DIR.exists() and DATA_DIR.is_dir():\n",
    "    shutil.rmtree(DATA_DIR)\n",
    "    print(f\"Deleted: {DATA_DIR}\")\n",
    "else:\n",
    "    print(f\"Directory does not exist: {DATA_DIR}\")\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "WALK_FORWARD_DIR_SCHWAB.mkdir(parents=True, exist_ok=True)\n",
    "WALK_FORWARD_DIR_GBQ.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#MARKER# Make data storage directories here and move this block upstream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Ticker List\n",
    "\n",
    "Retrieves the list of tickers to analyze. If `get_ticker_list_from_gbq` is `True`, it queries BigQuery for tickers used in a recent backtesting window and adds a randomized index for sampling. Otherwise, it loads the list from a previously saved local file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull tickers from DB or local file\n",
    "if get_ticker_list_from_gbq:\n",
    "    # You'll need to implement this function to return a DataFrame\n",
    "    sql = '''\n",
    "        SELECT DISTINCT ticker \n",
    "        FROM main.tf_stocks_for_backtesting(\"2023-08-01\",\"2024-01-01\")\n",
    "    '''\n",
    "    all_tickers_gbq = run_sql_query(sql, project_id=gbq_proj_id, credentials_path=private_key) # placeholder\n",
    "    all_tickers_gbq[\"random_index\"] = random.sample(range(len(all_tickers_gbq)), len(all_tickers_gbq))\n",
    "    all_tickers_gbq.to_csv(MINUTE_TICKERS_FILE, sep=\"\\t\", index=False)\n",
    "else:\n",
    "    all_tickers_gbq = pd.read_csv(MINUTE_TICKERS_FILE, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(all_tickers_gbq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter Tickers to Test\n",
    "\n",
    "Determines which tickers to include in the analysis. If `num_to_test` is greater than 0, it selects only those tickers with a `random_index` below the threshold. Otherwise, it includes all available tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter tickers\n",
    "if num_to_test_gbq > 0:\n",
    "    tickers_to_test_gbq = all_tickers_gbq[all_tickers_gbq[\"random_index\"] <= num_to_test_gbq][\"ticker\"].tolist() + big_ticker_list\n",
    "else:\n",
    "    tickers_to_test_gbq = all_tickers_gbq[\"ticker\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tickers_to_test_gbq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_recent_trading_day(current_date=None):\n",
    "    \"\"\"\n",
    "    Returns the most recent valid NYSE trading day before (or on) the given date.\n",
    "\n",
    "    This function checks the NYSE trading calendar for the last 30 days leading up to \n",
    "    `current_date` (or today, if not specified), and iterates backward to find the most \n",
    "    recent trading day. It accounts for weekends and market holidays.\n",
    "\n",
    "    Args:\n",
    "        current_date (datetime.date, optional): The reference date. Defaults to today.\n",
    "\n",
    "    Returns:\n",
    "        datetime.date: The most recent valid trading day prior to `current_date`.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no trading day is found within the past 20 calendar days.\n",
    "    \"\"\"\n",
    "    if current_date is None:                                                                 # Use today's date if none is provided\n",
    "        current_date = date.today()                                                          # Set current_date to today\n",
    "\n",
    "    nyse = mcal.get_calendar('NYSE')                                                         # Load NYSE trading calendar\n",
    "    schedule = nyse.valid_days(start_date=(current_date - timedelta(days=30)).isoformat(),   # Get list of valid trading days\n",
    "                               end_date=current_date.isoformat())                             # within the past 30 days\n",
    "    valid_days = [d.date() for d in schedule]                                                 # Convert schedule to list of date objects\n",
    "\n",
    "    for i in range(1, 21):                                                                    # Check the last 20 calendar days\n",
    "        candidate = current_date - timedelta(days=i)                                          # Go back one day at a time\n",
    "        if candidate in valid_days:                                                           # Return if the day is a valid trading day\n",
    "            return candidate                                                                  # Return most recent valid trading day\n",
    "    raise ValueError(\"Could not find recent trading day.\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up walkforward analysis parameters for GBQ data, defining the number of periods, analysis/evaluation window lengths, and overall date range. It also handles fresh vs. resumed runs by optionally deleting existing result files and filtering tickers accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fresh_start logic\n",
    "if build_results_gbq:\n",
    "    overall_finish_gbq = datetime.strptime('2024-03-15', '%Y-%m-%d') #get_most_recent_trading_day()\n",
    "    print(\"GBQ Overall finish date:\", overall_finish_gbq)\n",
    "\n",
    "    num_periods_gbq = 3\n",
    "    print(\"GBQ Number of periods:\", num_periods_gbq)\n",
    "\n",
    "    analysis_period_gbq = timedelta(days=6*7)\n",
    "    print(\"GBQ Analysis period:\", analysis_period_gbq)\n",
    "\n",
    "    evaluation_period_gbq = timedelta(days=4*7)\n",
    "    print(\"GBQ Evaluation period:\", evaluation_period_gbq)\n",
    "\n",
    "    end_of_last_analysis_period_gbq = overall_finish_gbq - evaluation_period_gbq\n",
    "    print(\"GBQ End of last analysis period:\", end_of_last_analysis_period_gbq)\n",
    "\n",
    "    overall_start_gbq = end_of_last_analysis_period_gbq - num_periods_gbq * analysis_period_gbq\n",
    "    print(\"GBQ Overall start date:\", overall_start_gbq)\n",
    "    \n",
    "    expected_min_analysis_days_gbq = 6 * 4\n",
    "    print(\"GBQ Expected min analysis days:\", expected_min_analysis_days_gbq)\n",
    "\n",
    "    if fresh_start_gbq:                                                                 # If a full reset is requested...\n",
    "        for file in WALK_FORWARD_DIR_GBQ.glob(\"*\"):                                    # Iterate over all files in the walk-forward directory\n",
    "            file.unlink()                                                          # Delete each file (clears previous results)\n",
    "    else:                                                                          # Otherwise, resume from where you left off\n",
    "        existing_files = [f.stem.replace(output_stub_gbq, \"\")                          # Get list of tickers that already have result files\n",
    "                        for f in WALK_FORWARD_DIR_GBQ.glob(\"*.txt\")]                   # Only look for .txt files matching previous outputs\n",
    "        tickers_to_test_gbq = [t for t in tickers_to_test_gbq if t not in existing_files]  # Remove tickers that already have results from the test list\n",
    "\n",
    "#MARKER# Transfer this to Schwab like logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch daily data for each ticker from GBQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "candles_dict_gbq = {}\n",
    "num_tickers = len(tickers_to_test_gbq)\n",
    "\n",
    "for i, ticker in enumerate(tickers_to_test_gbq, start=1):\n",
    "    print(f\"({i}/{num_tickers}) Fetching data for {ticker}\")\n",
    "    candles_dict_gbq[ticker] = get_ticker_data(\n",
    "        ticker,\n",
    "        overall_start_gbq,\n",
    "        local=use_local_data_gbq,\n",
    "        credentials_path=private_key,\n",
    "        project_id=gbq_proj_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(candles_dict_gbq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h3 style=\"color:yellow;\">Save Schwab and GBQ data to disk</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ticker_data(data_dict, end_path_component):\n",
    "    \"\"\"\n",
    "    Saves each DataFrame in a dictionary to a separate tab-delimited .txt file under a specified subdirectory.\n",
    "\n",
    "    Args:\n",
    "        data_dict (dict): Dictionary where keys are ticker symbols and values are pandas DataFrames.\n",
    "        end_path_component (str): Subfolder name under DATA_DIR where files will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None. Files are saved to disk, and progress is printed to the console.\n",
    "    \"\"\"\n",
    "    DAILY_TICKER_DATA_DIR = DATA_DIR / end_path_component\n",
    "    DAILY_TICKER_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    num_tickers = len(data_dict)\n",
    "    for i, (ticker, df) in enumerate(data_dict.items(), start=1):\n",
    "        if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "            print(f\"({i}/{num_tickers}) Skipping {ticker} â€” no data available\")\n",
    "            continue\n",
    "\n",
    "        output_file = DAILY_TICKER_DATA_DIR / f\"{ticker}.txt\"\n",
    "        try:\n",
    "            df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "            print(f\"({i}/{num_tickers}) Saved {ticker} data to {output_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"({i}/{num_tickers}) Failed to save {ticker}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save both Schwab and GBQ data to their respective folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "save_ticker_data(candles_dict_gbq, \"local-ticker-data-gbq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "save_ticker_data(candles_dict_schwab, \"local-ticker-data-schwab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 style=\"color:purple;\">Begin Back-Testing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h3 style=\"color:yellow;\">Running Analysis on Schwab Data</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set configuration flags for Schwab data sourcing, result building, and output naming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config Flags\n",
    "get_ticker_list_from_schwab = False\n",
    "use_local_data_schwab = True\n",
    "build_results_schwab = True\n",
    "fresh_start_schwab = True\n",
    "output_stub_schwab = pd.to_datetime(summary_candles_df[\"last_candle\"].max()).strftime(\"%Y%m%d_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating some time metrics for use in subsequent calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(summary_candles_df.sort_values(\"liquidity_proxy\", ascending=False)[[\"ticker\", \"total_hours\", \"total_days\", \"total_weeks\"]][0:1])\n",
    "max_weeks = summary_candles_df.sort_values(\"liquidity_proxy\", ascending=False).iloc[0][\"total_weeks\"]\n",
    "max_days = summary_candles_df.sort_values(\"liquidity_proxy\", ascending=False).iloc[0][\"total_days\"]\n",
    "max_hours = summary_candles_df.sort_values(\"liquidity_proxy\", ascending=False).iloc[0][\"total_hours\"]\n",
    "\n",
    "display(max_weeks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define walk-forward analysis period ranges and manage existing Schwab result files based on the `fresh_start_schwab` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fresh_start logic weeks/older\n",
    "if build_results_schwab:    \n",
    "\n",
    "    overall_finish_schwab              = pd.to_datetime(summary_candles_df[\"last_candle\"].max())         # Most recent timestamp available in the dataset\n",
    "\n",
    "    num_periods_schwab                 = 1                                                                # Number of walk-forward cycles to simulate\n",
    "    cycle_length_weeks_schwab         = round(max_weeks / num_periods_schwab, ndigits=0)                                   # Total weeks per analysis + evaluation cycle\n",
    "    cycle_length_days_schwab          = round(max_days / num_periods_schwab, ndigits=0)                                    # Total days per analysis + evaluation cycle\n",
    "    analysis_ratio_schwab             = 1                                                              # Portion of each cycle allocated to analysis\n",
    "    evaluation_ratio_schwab           = 1 - analysis_ratio_schwab                                        # Remaining portion allocated to evaluation\n",
    "\n",
    "    analysis_weeks_schwab             = cycle_length_weeks_schwab * analysis_ratio_schwab               # Total weeks in each analysis window\n",
    "    evaluation_weeks_schwab           = cycle_length_weeks_schwab * evaluation_ratio_schwab             # Total weeks in each evaluation window\n",
    "\n",
    "    days_in_week_schwab               = 7                                                                # Used to convert weeks to days\n",
    "    analysis_period_schwab            = timedelta(days=analysis_weeks_schwab*days_in_week_schwab)     # Total duration of an analysis window, expressed in days\n",
    "    evaluation_period_schwab          = timedelta(days=evaluation_weeks_schwab*days_in_week_schwab)   # Total duration of an evaluation window, expressed in days\n",
    "    \n",
    "    end_of_last_analysis_period_schwab = overall_finish_schwab - evaluation_period_schwab                   # End date of final analysis window\n",
    "    overall_start_schwab = end_of_last_analysis_period_schwab - num_periods_schwab * analysis_period_schwab  # Earliest date included in analysis\n",
    "    expected_min_analysis_days_schwab = 1 #(analysis_weeks_schwab + evaluation_weeks_schwab)*days_in_week_schwab           # Heuristic: expected number of days required for a valid analysis\n",
    "\n",
    "    if fresh_start_schwab:                                                                              # If a full reset is requested...\n",
    "        for file in WALK_FORWARD_DIR_SCHWAB.glob(\"*\"):                                                  # Iterate over all files in the walk-forward directory\n",
    "            file.unlink()                                                                                # Delete each file (clears previous results)\n",
    "    else:                                                                                                 # Otherwise, resume from where you left off\n",
    "        existing_files = [f.stem.replace(output_stub_schwab, \"\")                                          # Get list of tickers that already have result files\n",
    "                        for f in WALK_FORWARD_DIR_SCHWAB.glob(\"*.txt\")]                                   # Only look for .txt files matching previous outputs\n",
    "        tickers_to_test_schwab = [t for t in tickers_to_test_schwab if t not in existing_files]           # Remove tickers that already have results from the test list\n",
    "\n",
    "    print(\"SCHWAB Overall finish date:\", overall_finish_schwab)\n",
    "    print(\"SCHWAB Number of periods:\", num_periods_schwab)\n",
    "    print(\"SCHWAB Analysis plus evaluation total weeks:\", cycle_length_weeks_schwab)\n",
    "    print(\"SCHWAB Analysis ratio:\", analysis_ratio_schwab)\n",
    "    print(\"SCHWAB Evaluation ratio:\", evaluation_ratio_schwab)\n",
    "    print(\"SCHWAB Analysis weeks:\", analysis_weeks_schwab)\n",
    "    print(\"SCHWAB Evaluation weeks:\", evaluation_weeks_schwab)\n",
    "    print(\"SCHWAB Analysis period:\", analysis_period_schwab)\n",
    "    print(\"SCHWAB Evaluation period:\", evaluation_period_schwab)\n",
    "    print(\"SCHWAB End of last analysis period:\", end_of_last_analysis_period_schwab)\n",
    "    print(\"SCHWAB Overall start date:\", overall_start_schwab)\n",
    "    print(\"SCHWAB Expected min analysis days:\", expected_min_analysis_days_schwab)\n",
    "\n",
    "#MARKER# Using this logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_analysis_structure(ticker, end_of_last_analysis_period, analysis_period, evaluation_period, num_periods):\n",
    "    \"\"\"\n",
    "    Constructs a DataFrame defining rolling analysis and evaluation periods for a given ticker.\n",
    "\n",
    "    Args:\n",
    "        ticker (str): Ticker symbol to assign to the generated rows.\n",
    "        end_of_last_analysis_period (datetime): The most recent analysis period end date.\n",
    "        analysis_period (timedelta): Duration of each analysis period.\n",
    "        evaluation_period (timedelta): Duration of each evaluation period following the analysis.\n",
    "        num_periods (int): Number of rolling periods to generate.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with `num_periods` rows and the columns at the end of the function:\n",
    "    \"\"\"\n",
    "    analysis_period_starts = [                                                                  # Generate list of period start dates\n",
    "        end_of_last_analysis_period - (analysis_period + evaluation_period) * i                 # Each start is offset by i * total cycle length\n",
    "        for i in range(1, num_periods + 1)                                                      # For the last `num_periods` analysis windows\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame({                                                                         # Create base DataFrame for the analysis structure\n",
    "        \"ticker\": ticker,                                                                       # Set the ticker label\n",
    "        \"analysis_period_start\": pd.to_datetime(analysis_period_starts),                        # Assign start dates for each analysis period\n",
    "    })\n",
    "\n",
    "    df[\"analysis_period_end\"] = df[\"analysis_period_start\"] + analysis_period                   # Calculate the end of each analysis period\n",
    "    df[\"analysis_buy\"] = 0.0                                                                    # Initialize buy threshold column\n",
    "    df[\"analysis_sell\"] = 0.0                                                                   # Initialize sell threshold column\n",
    "    df[\"analysis_return\"] = 0.0                                                                 # Initialize return column for analysis period\n",
    "    df[\"analysis_trades\"] = 0                                                                   # Initialize number of trades in analysis period\n",
    "    df[\"analysis_eval_metric\"] = 0.0                                                            # Initialize penalized evaluation metric column\n",
    "    df[\"evaluation_period_start\"] = df[\"analysis_period_end\"] + timedelta(days=1)               # Evaluation starts the day after analysis ends\n",
    "    df[\"evaluation_period_end\"] = df[\"evaluation_period_start\"] + evaluation_period             # Evaluation end is offset from its start\n",
    "    df[\"evaluation_return\"] = 0.0                                                               # Initialize evaluation return column\n",
    "    df[\"evaluation_trades\"] = 0                                                                 # Initialize evaluation trade count\n",
    "    df[\"evaluation_data_good\"] = False                                                          # Flag whether evaluation data exists\n",
    "\n",
    "    return df                                                                                   # Return the prepared DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Builds the base results structure for each Schwab ticker by generating rolling analysis and evaluation windows using `prepare_analysis_structure()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build the base results structure\n",
    "results_structure_dict_schwab = {}\n",
    "num_tickers_schwab = len(tickers_to_test_schwab)\n",
    "\n",
    "# Prepare the analysis structure for each ticker\n",
    "for i, ticker in enumerate(tickers_to_test_schwab, start=1):\n",
    "    print(f\"({i}/{num_tickers_schwab}) Preparing analysis structure for {ticker}\")\n",
    "    results_structure_dict_schwab[ticker] = prepare_analysis_structure(\n",
    "        ticker,\n",
    "        end_of_last_analysis_period_schwab,\n",
    "        analysis_period_schwab,\n",
    "        evaluation_period_schwab,\n",
    "        num_periods_schwab\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results_structure_dict_schwab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trades(data, upper_bound, lower_bound, time_start):\n",
    "    \"\"\"\n",
    "    Simulates a basic swing trading strategy using high/low breakouts.\n",
    "\n",
    "    Iterates over a DataFrame of daily OHLC data to identify buy and sell trades:\n",
    "    - A **buy** occurs when the low of the day falls below or equals the `lower_bound`.\n",
    "    - A **sell** occurs when the high of the day rises above or equals the `upper_bound`.\n",
    "    - Only one position can be held at a time.\n",
    "    - If the end of the data is reached while a position is open, it is force-closed \n",
    "      at the midpoint of the final day's high and low.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Daily OHLC data with columns ['date_time', 'high', 'low'].\n",
    "        upper_bound (float): The price level that triggers a sell.\n",
    "        lower_bound (float): The price level that triggers a buy.\n",
    "        time_start (datetime-like): Trades are only evaluated for rows on or after this timestamp.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame of executed trades with columns:\n",
    "            ['date', 'type', 'daily_high', 'daily_low', 'trade_price']\n",
    "    \"\"\"\n",
    "    time_start = pd.to_datetime(time_start)  # Ensure compatible type for comparison\n",
    "    data = data.copy()\n",
    "    data[\"date_time\"] = pd.to_datetime(data[\"date_time\"])    \n",
    "    state = 0                                                                                     # 0 = not in position, 1 = in position\n",
    "    trades = []                                                                                   # List to store executed trades\n",
    "\n",
    "    for i, row in data.iterrows():                                                                # Iterate over each row in the data\n",
    "        if row[\"date_time\"] < time_start:                                                         # Skip rows before the time_start threshold\n",
    "            continue                                                                              # Move to the next row\n",
    "\n",
    "        if state == 0 and row[\"low\"] <= lower_bound:                                              # Entry condition: not in position and price hits or drops below lower bound\n",
    "            trades.append({                                                                       # Record a buy trade\n",
    "                \"date\": row[\"date_time\"],                                                         # Trade date\n",
    "                \"type\": \"buy\",                                                                    # Trade type\n",
    "                \"daily_high\": row[\"high\"],                                                        # High of the day\n",
    "                \"daily_low\": row[\"low\"],                                                          # Low of the day\n",
    "                \"trade_price\": lower_bound                                                        # Executed price at lower bound\n",
    "            })\n",
    "            state = 1                                                                             # Update state to indicate we are now in a position\n",
    "        elif state == 1 and row[\"high\"] >= upper_bound:                                           # Exit condition: in position and price rises above upper bound\n",
    "            trades.append({                                                                       # Record a sell trade\n",
    "                \"date\": row[\"date_time\"],                                                         # Trade date\n",
    "                \"type\": \"sell\",                                                                   # Trade type\n",
    "                \"daily_high\": row[\"high\"],                                                        # High of the day\n",
    "                \"daily_low\": row[\"low\"],                                                          # Low of the day\n",
    "                \"trade_price\": upper_bound                                                        # Executed price at upper bound\n",
    "            })\n",
    "            state = 0                                                                             # Update state to indicate we're out of position\n",
    "\n",
    "    if state == 1:                                                                                # If still in position at the end, force close\n",
    "        last = data.iloc[-1]                                                                      # Get the last row in the data\n",
    "        trades.append({                                                                           # Record a forced sell trade\n",
    "            \"date\": last[\"date_time\"],                                                            # Trade date\n",
    "            \"type\": \"sell\",                                                                       # Trade type\n",
    "            \"daily_high\": last[\"high\"],                                                           # High of the day\n",
    "            \"daily_low\": last[\"low\"],                                                             # Low of the day\n",
    "            \"trade_price\": 0.5 * (last[\"high\"] + last[\"low\"])                                     # Forced close price is midpoint between high and low\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(trades)                                                                   # Convert list of trades to a DataFrame and return\n",
    "\n",
    "def get_returns(data, upper_bound, lower_bound, time_start, starting_cash=10000):\n",
    "    \"\"\"\n",
    "    Simulates a swing trading strategy and calculates the annualized return.\n",
    "\n",
    "    Executes trades based on breakout conditions using `get_trades()`:\n",
    "    - Buys when price hits `lower_bound`\n",
    "    - Sells when price hits `upper_bound`\n",
    "    - Assumes full portfolio allocation on each trade (no partial positions)\n",
    "    - Closes the final position at the end of the data if still open\n",
    "\n",
    "    Computes:\n",
    "    - The total number of buy-side trades\n",
    "    - The annualized return over the trading period\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Daily OHLC data with columns ['date_time', 'high', 'low'].\n",
    "        upper_bound (float): Price level that triggers a sell.\n",
    "        lower_bound (float): Price level that triggers a buy.\n",
    "        time_start (datetime-like): The start date for evaluating trades.\n",
    "        starting_cash (float): Initial portfolio cash. Defaults to 10,000.\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"annualized_return\" (float or None): Annualized percentage return,\n",
    "                or None if no trades were executed or time span is 0.\n",
    "            \"num_trades\" (int): Number of completed buy trades.\n",
    "        }\n",
    "    \"\"\"\n",
    "    trades = get_trades(data, upper_bound, lower_bound, time_start)                             # Run the trade simulation using breakout rules\n",
    "\n",
    "    if trades.empty:                                                                            # If no trades occurred, return early\n",
    "        return {\"annualized_return\": None, \"num_trades\": 0}                                     # Return None and 0 trades if no signals\n",
    "\n",
    "    shares = 0                                                                                  # Initialize position size\n",
    "    cash = starting_cash                                                                        # Start with the full cash amount\n",
    "\n",
    "    for _, trade in trades.iterrows():                                                          # Loop through the trades chronologically\n",
    "        if trade[\"type\"] == \"buy\":                                                              # If it's a buy trade\n",
    "            shares = cash / trade[\"trade_price\"]                                                # Allocate entire portfolio into shares\n",
    "            cash = 0                                                                            # Cash is now fully deployed\n",
    "        elif trade[\"type\"] == \"sell\":                                                           # If it's a sell trade\n",
    "            cash = shares * trade[\"trade_price\"]                                                # Liquidate shares to get cash\n",
    "            shares = 0                                                                          # No position remains\n",
    "\n",
    "    last_day = pd.to_datetime(data[\"date_time\"].max())  \n",
    "    time_start = pd.to_datetime(time_start)                                                          # Last available date in the data\n",
    "    period_years = (last_day - time_start).days / 365.25                        # Duration of trading period in years\n",
    "\n",
    "    if period_years == 0:                                                                       # Edge case: zero duration (e.g., same-day trades)\n",
    "        return {\"annualized_return\": None, \"num_trades\": trades['type'].eq(\"buy\").sum()}        # Avoid divide-by-zero; return None safely\n",
    "\n",
    "    total_return = cash / starting_cash - 1                                                     # Compute simple return (final / initial - 1)\n",
    "    annualized = (cash / starting_cash) ** (1 / period_years) - 1                               # Convert total return to annualized return\n",
    "\n",
    "    return {\"total_return\": total_return, \"annualized_return\": annualized, \"num_trades\": trades['type'].eq(\"buy\").sum()}      # Return final results as a dictionary\n",
    "\n",
    "def analyze_ticker_data(data, grid_size=20, num_pse=1.5):\n",
    "    \"\"\"\n",
    "    Performs a grid search to optimize buy and sell thresholds for a swing trading strategy.\n",
    "\n",
    "    This function evaluates a range of lower-bound (buy) and upper-bound (sell) thresholds\n",
    "    to determine which combination yields the best penalized return, where penalization is\n",
    "    based on the pseudo-standard error of the return.\n",
    "\n",
    "    The performance of each threshold pair is evaluated using:\n",
    "        - Annualized return\n",
    "        - Number of trades\n",
    "        - Pseudo-standard error: |return| / sqrt(trades)\n",
    "        - Penalized return: return - (num_pse * pseudo_se)\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Historical OHLC data with at least 'low', 'high', and 'date_time' columns.\n",
    "        grid_size (int): Number of evenly spaced values to test for both lb and ub thresholds.\n",
    "        num_pse (float): Penalty multiplier applied to the pseudo-standard error.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A single-row DataFrame containing the best-performing parameters:\n",
    "            - lb (float): Optimal lower bound (buy threshold)\n",
    "            - ub (float): Optimal upper bound (sell threshold)\n",
    "            - spread (float): ub - lb\n",
    "            - return (float): Annualized return for that configuration\n",
    "            - trades (int): Number of buy trades\n",
    "            - pseudo_se (float): Pseudo-standard error of the return\n",
    "            - return_lb (float): Penalized return used for selection\n",
    "            - time_start (datetime): Start time of the evaluation period\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no valid parameter combinations are found (e.g., no trades or all return_lb are NaN).\n",
    "    \"\"\"\n",
    "    lb_start = data['low'].quantile(0.01)\n",
    "    lb_end = data['low'].quantile(0.75)\n",
    "    ub_start = data['high'].quantile(0.10)\n",
    "    ub_end = data['high'].quantile(0.99)\n",
    "    time_start = data['date_time'].min()\n",
    "\n",
    "    lb_values = np.linspace(lb_start, lb_end, grid_size)\n",
    "    ub_values = np.linspace(ub_start, ub_end, grid_size)\n",
    "\n",
    "    experiments = []\n",
    "\n",
    "    for lb in lb_values:\n",
    "        for ub in ub_values:\n",
    "            if lb >= ub:\n",
    "                continue\n",
    "\n",
    "            result = get_returns(data, upper_bound=ub, lower_bound=lb, time_start=time_start)\n",
    "            num_trades = result.get(\"num_trades\", 0)\n",
    "\n",
    "            if num_trades == 0:\n",
    "                continue  # Skip configurations with no trades\n",
    "\n",
    "            annualized_return = result.get(\"annualized_return\") or 0.0\n",
    "            spread = ub - lb\n",
    "            pseudo_se = abs(annualized_return) / np.sqrt(num_trades)\n",
    "            return_lb = annualized_return - num_pse * pseudo_se\n",
    "\n",
    "            experiments.append({\n",
    "                \"lb\": lb,\n",
    "                \"ub\": ub,\n",
    "                \"spread\": spread,\n",
    "                \"return\": annualized_return,\n",
    "                \"trades\": num_trades,\n",
    "                \"pseudo_se\": pseudo_se,\n",
    "                \"return_lb\": return_lb,\n",
    "                \"time_start\": time_start\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(experiments)\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No valid parameter combinations found.\")\n",
    "\n",
    "    max_return_lb = df[\"return_lb\"].max()\n",
    "    best = df[df[\"return_lb\"] == max_return_lb]\n",
    "    best = best.sort_values([\"trades\", \"spread\"], ascending=[False, True]).tail(1)\n",
    "\n",
    "    return best.reset_index(drop=True)\n",
    "\n",
    "def run_analysis_loop(ticker_results, daily_data, expected_min_analysis_days):                          # Perform optimization and evaluation for each analysis/evaluation period\n",
    "    \"\"\"\n",
    "    Performs optimization and evaluation for each row in the walk-forward results DataFrame.\n",
    "\n",
    "    Args:\n",
    "        ticker_results (pd.DataFrame): Table with period metadata and signal slots for a single ticker.\n",
    "        daily_data (pd.DataFrame): OHLCV data with a 'date_time' column.\n",
    "        expected_min_analysis_days (int): Minimum number of days required to consider an analysis window valid.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated ticker_results with optimized thresholds and evaluation performance filled in.\n",
    "    \"\"\"\n",
    "    for idx, row in ticker_results.iterrows():                                                           # Iterate through each row (period) of the ticker results table\n",
    "        analysis_data = daily_data[                                                                      # Slice the data for the current analysis window\n",
    "            (daily_data[\"date_time\"] >= row[\"analysis_period_start\"]) &                                  # Include data on or after the analysis start\n",
    "            (daily_data[\"date_time\"] <= row[\"analysis_period_end\"])                                      # And on or before the analysis end\n",
    "        ]\n",
    "\n",
    "        if len(analysis_data) < expected_min_analysis_days:                                              # Skip this window if not enough trading days\n",
    "            continue\n",
    "\n",
    "        evaluation_data = daily_data[                                                                    # Slice the data for the evaluation window\n",
    "            (daily_data[\"date_time\"] >= row[\"evaluation_period_start\"]) &                                # Include data on or after the evaluation start\n",
    "            (daily_data[\"date_time\"] <= row[\"evaluation_period_end\"])                                    # And on or before the evaluation end\n",
    "        ]\n",
    "\n",
    "        if not analysis_data.empty:                                                                       # Proceed if there's valid analysis data\n",
    "            period_results = analyze_ticker_data(analysis_data)                                           # Run optimization for buy/sell thresholds\n",
    "\n",
    "            ticker_results.at[idx, \"analysis_buy\"] = period_results[\"lb\"][0]                              # Save optimized lower bound (buy threshold)\n",
    "            ticker_results.at[idx, \"analysis_sell\"] = period_results[\"ub\"][0]                             # Save optimized upper bound (sell threshold)\n",
    "            ticker_results.at[idx, \"analysis_return\"] = period_results[\"return\"][0]                       # Save annualized return for this config\n",
    "            ticker_results.at[idx, \"analysis_trades\"] = period_results[\"trades\"][0]                       # Save number of trades\n",
    "            ticker_results.at[idx, \"analysis_eval_metric\"] = period_results[\"return_lb\"][0]              # Save penalized return metric\n",
    "        else:\n",
    "            ticker_results.at[idx, \"analysis_return\"] = np.nan                                            # If no data, store NaN as placeholder\n",
    "\n",
    "        if not evaluation_data.empty:                                                                     # Proceed if there's evaluation data\n",
    "            ticker_results.at[idx, \"evaluation_data_good\"] = True                                         # Mark the data as usable\n",
    "\n",
    "            eval_results = get_returns(\n",
    "                evaluation_data,\n",
    "                upper_bound=ticker_results.at[idx, \"analysis_sell\"],\n",
    "                lower_bound=ticker_results.at[idx, \"analysis_buy\"],\n",
    "                time_start=ticker_results.at[idx, \"evaluation_period_start\"]\n",
    "            )\n",
    "\n",
    "            ticker_results.at[idx, \"evaluation_trades\"] = eval_results[\"num_trades\"]                      # Store number of trades during evaluation\n",
    "            ticker_results.at[idx, \"evaluation_return\"] = eval_results[\"annualized_return\"]               # Store annualized return during evaluation\n",
    "\n",
    "    return ticker_results                                                                                 # Return the updated DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs the walk-forward optimization and evaluation loop for each Schwab ticker and stores the updated results in `final_results_dict_schwab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "final_results_dict_schwab = {}\n",
    "num_tickers_schwab = len(tickers_to_test_schwab)\n",
    "\n",
    "# 2. Run analysis and evaluation loop\n",
    "for i, ticker in enumerate(tickers_to_test_schwab, start=1):\n",
    "    print(f\"({i}/{num_tickers_schwab}) Running analysis loop for {ticker} from Schwab data\")\n",
    "    final_results_dict_schwab[ticker] = run_analysis_loop(\n",
    "        results_structure_dict_schwab[ticker],\n",
    "        candles_dict_schwab[ticker],\n",
    "        expected_min_analysis_days_schwab\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at `final_results_dict_schwab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dict items to a list to enable index-based access\n",
    "final_results_list_schwab = list(final_results_dict_schwab.items())\n",
    "\n",
    "# Example: display the DataFrame at index 0\n",
    "ticker, df = final_results_list_schwab[0]\n",
    "print(f\"Ticker: {ticker}\")\n",
    "display(df)\n",
    "display(final_results_dict_schwab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_analysis_metrics(results_dict, price_data_dict):\n",
    "    \"\"\"\n",
    "    Adds analytics columns to each DataFrame in results_dict using the associated OHLCV data.\n",
    "\n",
    "    Parameters:\n",
    "    - results_dict (dict): Maps tickers to DataFrames of walk-forward analysis results.\n",
    "    - price_data_dict (dict): Maps tickers to their full OHLCV DataFrames with 'date_time', 'high', 'low', 'close'.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Updated results_dict with new columns added.\n",
    "    \"\"\"\n",
    "    updated_results = {}\n",
    "\n",
    "    for ticker, df in results_dict.items():\n",
    "        prices = price_data_dict[ticker].copy()\n",
    "        prices[\"date_time\"] = pd.to_datetime(prices[\"date_time\"])\n",
    "        last_close = prices[\"close\"].iloc[-1]\n",
    "\n",
    "        df = df.copy()\n",
    "        df[\"current_price_below_lb\"] = df[\"analysis_buy\"].apply(\n",
    "            lambda lb: last_close < lb if pd.notnull(lb) else np.nan\n",
    "        )\n",
    "        df[\"percent_below_lb\"] = df[\"analysis_buy\"].apply(\n",
    "            lambda lb: (lb - last_close) / lb if pd.notnull(lb) else np.nan\n",
    "        )\n",
    "        df[\"current_price_below_ub\"] = df[\"analysis_sell\"].apply(\n",
    "            lambda ub: last_close < ub if pd.notnull(ub) else np.nan\n",
    "        )\n",
    "        df[\"percent_below_ub\"] = df[\"analysis_sell\"].apply(\n",
    "            lambda ub: (ub - last_close) / ub if pd.notnull(ub) else np.nan\n",
    "        )\n",
    "        df[\"current_price_between_bounds\"] = df.apply(\n",
    "            lambda row: (\n",
    "                row[\"analysis_buy\"] < last_close < row[\"analysis_sell\"]\n",
    "                if pd.notnull(row[\"analysis_buy\"]) and pd.notnull(row[\"analysis_sell\"])\n",
    "                else np.nan\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        num_days_lb_list = []\n",
    "        num_days_ub_list = []\n",
    "        trend_slope_list = []\n",
    "        norm_trend_slope_list = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            lb = row[\"analysis_buy\"]\n",
    "            ub = row[\"analysis_sell\"]\n",
    "            start_date = pd.to_datetime(row[\"analysis_period_start\"])\n",
    "            end_date = pd.to_datetime(row[\"analysis_period_end\"])\n",
    "\n",
    "            analysis_period = prices[\n",
    "                (prices[\"date_time\"] >= start_date) &\n",
    "                (prices[\"date_time\"] <= end_date)\n",
    "            ].copy()\n",
    "\n",
    "            analysis_period = analysis_period.sort_values(\"date_time\", ascending=False)\n",
    "\n",
    "            # Days below lb\n",
    "            if pd.notnull(lb):\n",
    "                count_lb = 0\n",
    "                for _, candle in analysis_period.iterrows():\n",
    "                    if candle[\"high\"] < lb and candle[\"low\"] < lb:\n",
    "                        count_lb += 1\n",
    "                    else:\n",
    "                        break\n",
    "                num_days_lb_list.append(count_lb)\n",
    "            else:\n",
    "                num_days_lb_list.append(np.nan)\n",
    "\n",
    "            # Days below ub\n",
    "            if pd.notnull(ub):\n",
    "                count_ub = 0\n",
    "                for _, candle in analysis_period.iterrows():\n",
    "                    if candle[\"high\"] < ub and candle[\"low\"] < ub:\n",
    "                        count_ub += 1\n",
    "                    else:\n",
    "                        break\n",
    "                num_days_ub_list.append(count_ub)\n",
    "            else:\n",
    "                num_days_ub_list.append(np.nan)\n",
    "\n",
    "            # Trend slope (raw and normalized) using real time\n",
    "            if len(analysis_period) >= 2:\n",
    "                closes = analysis_period[\"close\"].values\n",
    "                x = mdates.date2num(analysis_period[\"date_time\"])  # use real time axis\n",
    "                slope, _, _, _, _ = linregress(x, closes)\n",
    "                trend_slope_list.append(slope)\n",
    "                norm_slope = slope / closes.mean() if closes.mean() != 0 else 0\n",
    "                norm_trend_slope_list.append(norm_slope)\n",
    "            else:\n",
    "                trend_slope_list.append(np.nan)\n",
    "                norm_trend_slope_list.append(np.nan)\n",
    "\n",
    "        df[\"num_days_below_lb\"] = num_days_lb_list\n",
    "        df[\"num_days_below_ub\"] = num_days_ub_list\n",
    "        df[\"trend_slope\"] = trend_slope_list\n",
    "        df[\"norm_trend_slope\"] = norm_trend_slope_list  # For scoring\n",
    "\n",
    "        # Cyclicality measure\n",
    "        full_prices = prices.set_index(\"date_time\")\n",
    "        full_span = full_prices[\"high\"].max() - full_prices[\"low\"].min()\n",
    "        avg_day_range = (full_prices[\"high\"] - full_prices[\"low\"]).mean()\n",
    "        cyclicality_ratio = avg_day_range / full_span if full_span != 0 else np.nan\n",
    "        df[\"cyclicality\"] = cyclicality_ratio\n",
    "\n",
    "        updated_results[ticker] = df\n",
    "\n",
    "    return updated_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_periods_schwab == 1 and analysis_ratio_schwab == 1:\n",
    "    final_results_dict_schwab = add_analysis_metrics(final_results_dict_schwab, candles_dict_schwab)\n",
    "    display(final_results_dict_schwab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_profit_probability(results_dict, weights=None, bound_reference=\"lower\"):\n",
    "    \"\"\"\n",
    "    Adds a profit probability score, ranking, and swing probability category to each DataFrame in a results dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - results_dict (dict): Dictionary of {ticker: pd.DataFrame} with analysis results.\n",
    "    - weights (dict, optional): Weights for each scoring component.\n",
    "    - bound_reference (str): Either \"lower\" or \"upper\" to indicate which bound to use for scoring.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Updated results_dict with 'profit_score', 'profit_rank', and 'swing_probability' columns.\n",
    "    \"\"\"\n",
    "    if bound_reference not in {\"lower\", \"upper\"}:\n",
    "        raise ValueError(\"bound_reference must be either 'lower' or 'upper'\")\n",
    "\n",
    "    suffix = \"lb\" if bound_reference == \"lower\" else \"ub\"\n",
    "\n",
    "    if weights is None:\n",
    "        weights = {\n",
    "            f\"current_price_below_{suffix}\": 1.0,\n",
    "            f\"percent_below_{suffix}\": 0.0,\n",
    "            f\"num_days_below_{suffix}\": 0.0,\n",
    "            \"cyclicality\": 0.75,\n",
    "            \"norm_trend_slope\": 1.75  # âœ… New metric\n",
    "        }\n",
    "\n",
    "    combined_df = pd.concat(results_dict.values(), ignore_index=True)\n",
    "    combined_df[f\"current_price_below_{suffix}\"] = combined_df[f\"current_price_below_{suffix}\"].astype(float)\n",
    "\n",
    "    # Rank components\n",
    "    combined_df[\"_rank_percent\"] = combined_df[f\"percent_below_{suffix}\"].rank(pct=True)\n",
    "    combined_df[\"_rank_days\"] = combined_df[f\"num_days_below_{suffix}\"].rank(pct=True)\n",
    "    combined_df[\"_rank_cyclicality\"] = combined_df[\"cyclicality\"].rank(pct=True)\n",
    "    combined_df[\"_rank_trend\"] = combined_df[\"norm_trend_slope\"].rank(pct=True)  # âœ… New ranking\n",
    "\n",
    "    current_mask = combined_df[f\"current_price_below_{suffix}\"]\n",
    "\n",
    "    # Final score\n",
    "    combined_df[\"profit_score\"] = current_mask * (\n",
    "        weights.get(f\"current_price_below_{suffix}\", 0) +\n",
    "        combined_df[\"_rank_percent\"] * weights.get(f\"percent_below_{suffix}\", 0) +\n",
    "        combined_df[\"_rank_days\"] * weights.get(f\"num_days_below_{suffix}\", 0) +\n",
    "        combined_df[\"_rank_cyclicality\"] * weights.get(\"cyclicality\", 0) +\n",
    "        combined_df[\"_rank_trend\"] * weights.get(\"norm_trend_slope\", 0)  # âœ… New score component\n",
    "    )\n",
    "\n",
    "    combined_df[\"profit_rank\"] = combined_df[\"profit_score\"].rank(ascending=False)\n",
    "\n",
    "    # Swing category\n",
    "    swing_probs = pd.Series(index=combined_df.index, dtype=\"object\")\n",
    "    nonzero_scores = combined_df[combined_df[\"profit_score\"] > 0]\n",
    "    percentiles = nonzero_scores[\"profit_score\"].rank(pct=True)\n",
    "\n",
    "    for idx, p in percentiles.items():\n",
    "        if p <= 0.2:\n",
    "            swing_probs[idx] = \"very_low\"\n",
    "        elif p <= 0.4:\n",
    "            swing_probs[idx] = \"low\"\n",
    "        elif p <= 0.6:\n",
    "            swing_probs[idx] = \"medium\"\n",
    "        elif p <= 0.8:\n",
    "            swing_probs[idx] = \"high\"\n",
    "        else:\n",
    "            swing_probs[idx] = \"very_high\"\n",
    "\n",
    "    swing_probs[combined_df[\"profit_score\"] == 0] = \"zero\"\n",
    "    combined_df[\"swing_probability\"] = swing_probs\n",
    "\n",
    "    # Clean up\n",
    "    combined_df.drop(columns=[\"_rank_percent\", \"_rank_days\", \"_rank_cyclicality\", \"_rank_trend\"], inplace=True)\n",
    "\n",
    "    # Return results\n",
    "    row_counter = 0\n",
    "    for ticker, df in results_dict.items():\n",
    "        num_rows = len(df)\n",
    "        updated_chunk = combined_df.iloc[row_counter:row_counter + num_rows]\n",
    "        df[\"profit_score\"] = updated_chunk[\"profit_score\"].values\n",
    "        df[\"profit_rank\"] = updated_chunk[\"profit_rank\"].values\n",
    "        df[\"swing_probability\"] = updated_chunk[\"swing_probability\"].values\n",
    "        row_counter += num_rows\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calls `score_profit_probability` and passes Schwab results to get profit scoring if the parameters are in trade recommendation configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_periods_schwab == 1 and analysis_ratio_schwab == 1:\n",
    "    final_results_dict_schwab = score_profit_probability(final_results_dict_schwab, weights=None, bound_reference=\"lower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Schwab results to disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ticker in enumerate(tickers_to_test_schwab, start=1):\n",
    "    output_file = f\"{output_stub_schwab}{ticker}.txt\"\n",
    "    output_path = WALK_FORWARD_DIR_SCHWAB / output_file\n",
    "    final_results_dict_schwab[ticker].to_csv(output_path, sep=\"\\t\", index=False)\n",
    "\n",
    "    print(f\"({i}/{num_tickers_schwab}) Completed {ticker}\")\n",
    "    recent_eval = evaluate_recent_performance(final_results_dict_schwab[ticker])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in trade recommendation parameter configuration, take the single row from each value in the dict and concatenate into a dataframe. This won't work with multiple analysis/eval cycles. Hence the if."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows') # Display default abbreviated rows\n",
    "# pd.set_option('display.max_rows', None) # Displays all rows\n",
    "\n",
    "if num_periods_schwab == 1 and analysis_ratio_schwab == 1:\n",
    "    final_results_df_schwab = pd.concat(final_results_dict_schwab.values(), ignore_index=True)\n",
    "    print(\"Concatenated results for single period analysis.\")\n",
    "    final_results_df_schwab.to_csv(WALK_FORWARD_DIR_SCHWAB / \"final_results_df_schwab.csv\", sep=\"\\t\", index=False)\n",
    "    display(final_results_df_schwab.sort_values(\"trend_slope\", ascending=True))\n",
    "\n",
    "else: print(\"Multiple periods detected, skipping concatenation.\")\n",
    "\n",
    "pd.reset_option('display.max_rows') # Display default abbreviated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h3 style=\"color:yellow;\">Running Analysis on GBQ Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_analysis_structure_gbq(ticker, end_of_last_analysis_period, analysis_period, evaluation_period, num_periods):\n",
    "    \"\"\"\n",
    "    Constructs a DataFrame defining rolling analysis and evaluation periods for a given ticker.\n",
    "\n",
    "    Args:\n",
    "        ticker (str): Ticker symbol to assign to the generated rows.\n",
    "        end_of_last_analysis_period (datetime): The most recent analysis period end date.\n",
    "        analysis_period (timedelta): Duration of each analysis period.\n",
    "        evaluation_period (timedelta): Duration of each evaluation period following the analysis.\n",
    "        num_periods (int): Number of rolling periods to generate.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with `num_periods` rows and the columns at the end of the function:\n",
    "    \"\"\"\n",
    "    analysis_period_starts = [                                                                 # Generate list of period start dates\n",
    "        end_of_last_analysis_period - analysis_period * i                                      # Each start is offset by i * analysis_period\n",
    "        for i in range(1, num_periods + 1)                                                      # For the last `num_periods` analysis windows\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame({                                                                         # Create base DataFrame for the analysis structure\n",
    "        \"ticker\": ticker,                                                                       # Set the ticker label\n",
    "        \"analysis_period_start\": pd.to_datetime(analysis_period_starts),                        # Assign start dates for each analysis period\n",
    "    })\n",
    "\n",
    "    df[\"analysis_period_end\"] = df[\"analysis_period_start\"] + analysis_period                   # Calculate the end of each analysis period\n",
    "    df[\"analysis_buy\"] = 0.0                                                                    # Initialize buy threshold column\n",
    "    df[\"analysis_sell\"] = 0.0                                                                   # Initialize sell threshold column\n",
    "    df[\"analysis_return\"] = 0.0                                                                 # Initialize return column for analysis period\n",
    "    df[\"analysis_trades\"] = 0                                                                   # Initialize number of trades in analysis period\n",
    "    df[\"analysis_eval_metric\"] = 0.0                                                            # Initialize penalized evaluation metric column\n",
    "    df[\"evaluation_period_start\"] = df[\"analysis_period_end\"] + timedelta(days=1)              # Evaluation starts the day after analysis ends\n",
    "    df[\"evaluation_period_end\"] = df[\"evaluation_period_start\"] + evaluation_period             # Evaluation end is offset from its start\n",
    "    df[\"evaluation_return\"] = 0.0                                                               # Initialize evaluation return column\n",
    "    df[\"evaluation_trades\"] = 0                                                                 # Initialize evaluation trade count\n",
    "    df[\"evaluation_data_good\"] = False                                                          # Flag whether evaluation data exists\n",
    "\n",
    "    return df                                                                                   # Return the prepared DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Builds the base results structure for each GBQ ticker by generating rolling analysis and evaluation windows using `prepare_analysis_structure()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build the base results structure\n",
    "results_structure_dict_gbq = {}\n",
    "num_tickers_gbq = len(tickers_to_test_gbq)\n",
    "\n",
    "# Prepare the analysis structure for each ticker\n",
    "for i, ticker in enumerate(tickers_to_test_gbq, start=1):\n",
    "    print(f\"({i}/{num_tickers_gbq}) Preparing analysis structure for {ticker}\")\n",
    "    results_structure_dict_gbq[ticker] = prepare_analysis_structure_gbq(\n",
    "        ticker,\n",
    "        end_of_last_analysis_period_gbq,\n",
    "        analysis_period_gbq,\n",
    "        evaluation_period_gbq,\n",
    "        num_periods_gbq\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results_structure_dict_gbq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_tickers_schwab = len(tickers_to_test_schwab)\n",
    "# for i, ticker in enumerate(tickers_to_test_schwab, start=1):\n",
    "#     print(f\"({i}/{num_tickers_schwab}) Preparing analysis structure for {ticker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs the walk-forward optimization and evaluation loop for each GBQ ticker and stores the updated results in `final_results_dict_gbq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "final_results_dict_gbq = {}\n",
    "num_tickers_gbq = len(tickers_to_test_gbq)\n",
    "\n",
    "# Run the analysis and evaluation loop for each ticker\n",
    "for i, ticker in enumerate(tickers_to_test_gbq, start=1):\n",
    "    print(f\"({i}/{num_tickers_gbq}) Running analysis loop for {ticker}\")\n",
    "    final_results_dict_gbq[ticker] = run_analysis_loop(\n",
    "        results_structure_dict_gbq[ticker],\n",
    "        candles_dict_gbq[ticker],\n",
    "        expected_min_analysis_days_gbq\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at GBQ analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dict items to a list to enable index-based access\n",
    "final_results_list = list(final_results_dict_gbq.items())\n",
    "\n",
    "# Example: display the DataFrame at index 0\n",
    "ticker, df = final_results_list[0]\n",
    "print(f\"Ticker: {ticker}\")\n",
    "display(df)\n",
    "display(final_results_dict_gbq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save GBQ results to disk and print performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ticker in enumerate(tickers_to_test_gbq, start=1):\n",
    "    output_file = f\"{output_stub_gbq}{ticker}.txt\"\n",
    "    output_path = WALK_FORWARD_DIR_GBQ / output_file\n",
    "    final_results_dict_gbq[ticker].to_csv(output_path, sep=\"\\t\", index=False)\n",
    "\n",
    "    print(f\"({i}/{num_tickers_gbq}) Completed {ticker}\")\n",
    "    recent_eval = evaluate_recent_performance(final_results_dict_gbq[ticker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if num_periods_gbq == 1 and analysis_ratio_gbq == 1:\n",
    "#     final_results_dict_gbq = add_analysis_metrics(final_results_dict_gbq, candles_dict_gbq)\n",
    "#     display(final_results_dict_gbq)\n",
    "\n",
    "# if num_periods_gbq == 1 and analysis_ratio_gbq == 1:\n",
    "#     final_results_df_gbq = pd.concat(final_results_dict_gbq.values(), ignore_index=True)\n",
    "#     print(\"Concatenated results for single period analysis.\")\n",
    "#     final_results_df_gbq.to_csv(WALK_FORWARD_DIR_GBQ / \"final_results_df_gbq.csv\", sep=\"\\t\", index=False)\n",
    "#     display(final_results_df_gbq)\n",
    "# else: print(\"Multiple periods detected, skipping concatenation.\")\n",
    "\n",
    "#MARKER# CONTINUE WORKING ON THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 style=\"color:purple;\">Visualization</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_swing_trades(result_row, num_tickers_source, local=True, local_folder=\"local-ticker-data-schwab\", data_source=\"schwab\", swing_probability_filter=None):\n",
    "    \"\"\"\n",
    "    Visualizes price action and trade activity for a specific ticker over one analysis + evaluation cycle.\n",
    "\n",
    "    This function generates a time series plot showing:\n",
    "    - Daily high/low ranges as error bars\n",
    "    - Horizontal lines for the buy and sell thresholds\n",
    "    - A vertical line marking the end of the analysis period\n",
    "    - Buy and sell trade markers during both the analysis and evaluation periods\n",
    "    - A caption with summary metrics (number of trades and returns)\n",
    "\n",
    "    Args:\n",
    "        result_row (pd.Series or pd.DataFrame): A single row from the walkforward results,\n",
    "            containing fields such as 'ticker', 'analysis_period_start', 'analysis_buy', etc.\n",
    "            If a DataFrame is passed, only the first row is used.\n",
    "        local (bool): Whether to load price data from a local file rather than BigQuery.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays a matplotlib plot with the trade and performance context.\n",
    "    \"\"\"\n",
    "    if isinstance(result_row, pd.DataFrame):\n",
    "        result_row = result_row.iloc[0]\n",
    "\n",
    "    analysis_period_start = pd.to_datetime(result_row[\"analysis_period_start\"])\n",
    "    evaluation_period_end = pd.to_datetime(result_row[\"evaluation_period_end\"])\n",
    "    analysis_period_end = pd.to_datetime(result_row[\"analysis_period_end\"])\n",
    "    evaluation_period_start = pd.to_datetime(result_row[\"evaluation_period_start\"])\n",
    "\n",
    "    ticker = result_row[\"ticker\"]\n",
    "    buy_level = result_row[\"analysis_buy\"]\n",
    "    sell_level = result_row[\"analysis_sell\"]\n",
    "\n",
    "    # Pull data and filter\n",
    "    ticker_data = get_ticker_data(ticker, analysis_period_start, local=local, local_folder=local_folder)\n",
    "    ticker_data = ticker_data[ticker_data[\"date_time\"] <= evaluation_period_end]\n",
    "\n",
    "    # Get trades from both periods\n",
    "    trades_analysis = get_trades(\n",
    "        ticker_data[\n",
    "            (ticker_data[\"date_time\"] >= result_row[\"analysis_period_start\"]) &\n",
    "            (ticker_data[\"date_time\"] <= result_row[\"analysis_period_end\"])\n",
    "        ],\n",
    "        upper_bound=sell_level,\n",
    "        lower_bound=buy_level,\n",
    "        time_start=result_row[\"analysis_period_start\"]\n",
    "    )\n",
    "\n",
    "    trades_eval = get_trades(\n",
    "        ticker_data[\n",
    "            (ticker_data[\"date_time\"] >= result_row[\"evaluation_period_start\"]) &\n",
    "            (ticker_data[\"date_time\"] <= result_row[\"evaluation_period_end\"])\n",
    "        ],\n",
    "        upper_bound=sell_level,\n",
    "        lower_bound=buy_level,\n",
    "        time_start=result_row[\"evaluation_period_start\"]\n",
    "    )\n",
    "\n",
    "    trades = pd.concat([trades_analysis, trades_eval], ignore_index=True)\n",
    "\n",
    "    # Add trade price as the bound unless out of range, in which case midpoint\n",
    "    trades[\"price\"] = trades.apply(\n",
    "        lambda row: row[\"trade_price\"]\n",
    "        if row[\"daily_low\"] <= row[\"trade_price\"] <= row[\"daily_high\"]\n",
    "        else 0.5 * (row[\"daily_high\"] + row[\"daily_low\"]),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Plot setup\n",
    "    fig, ax = plt.subplots(figsize=(18, 9))\n",
    "    \n",
    "    # Initialize slope so it can be used later in the caption\n",
    "    analysis_trend_slope = None\n",
    "\n",
    "    # Linear regression line over analysis period only\n",
    "    analysis_data = ticker_data[\n",
    "        (ticker_data[\"date_time\"] >= analysis_period_start) &\n",
    "        (ticker_data[\"date_time\"] <= analysis_period_end)\n",
    "    ].copy()\n",
    "\n",
    "    if not analysis_data.empty:\n",
    "        x_vals = mdates.date2num(analysis_data[\"date_time\"])  # Convert datetime to numeric for regression\n",
    "        y_vals = analysis_data[\"close\"].values\n",
    "\n",
    "        if len(x_vals) >= 2:\n",
    "            slope, intercept, _, _, _ = linregress(x_vals, y_vals)\n",
    "            reg_line = intercept + slope * x_vals\n",
    "            ax.plot(analysis_data[\"date_time\"], reg_line, linestyle=\"--\", linewidth=2, label=\"Linear Trend (Analysis)\")\n",
    "            analysis_trend_slope = slope\n",
    "\n",
    "    # Error bars (high/low ranges)\n",
    "    ax.errorbar(ticker_data[\"date_time\"], \n",
    "                y=(ticker_data[\"high\"] + ticker_data[\"low\"]) / 2,\n",
    "                yerr=(ticker_data[\"high\"] - ticker_data[\"low\"]) / 2,\n",
    "                fmt='-', ecolor='gray', alpha=0.4)\n",
    "\n",
    "    # Horizontal lines for buy/sell levels\n",
    "    ax.axhline(buy_level, color=\"palegreen\", linestyle=\"--\", label=\"Buy Level\")\n",
    "    ax.axhline(sell_level, color=\"lightcoral\", linestyle=\"--\", label=\"Sell Level\")\n",
    "\n",
    "    # Get evenly spaced y-positions for the labels\n",
    "    y_min = ticker_data[\"low\"].min()\n",
    "    y_max = ticker_data[\"high\"].max()\n",
    "    y_positions = np.linspace(y_min, y_max, 4)\n",
    "\n",
    "    # Vertical markers for key dates with evenly spaced labels\n",
    "    ax.axvline(analysis_period_start, color=\"black\", linestyle=\":\", label=\"Analysis Period Starts\")\n",
    "    ax.text(analysis_period_start, y_positions[3], f\"Analysis Period Starts\\n{analysis_period_start.strftime('%Y-%m-%d')}\",\n",
    "            rotation=0, va=\"center\", ha=\"right\", color=\"black\")\n",
    "\n",
    "    ax.axvline(analysis_period_end, color=\"black\", linestyle=\":\", label=\"Analysis Period Ends\")\n",
    "    ax.text(analysis_period_end, y_positions[2], f\"Analysis Period Ends\\n{analysis_period_end.strftime('%Y-%m-%d')}\",\n",
    "            rotation=0, va=\"center\", ha=\"right\", color=\"black\")\n",
    "\n",
    "    ax.axvline(evaluation_period_start, color=\"black\", linestyle=\":\", label=\"Evaluation Period Starts\")\n",
    "    ax.text(evaluation_period_start, y_positions[1], f\"Evaluation Period Starts\\n{evaluation_period_start.strftime('%Y-%m-%d')}\",\n",
    "            rotation=0, va=\"center\", ha=\"left\", color=\"black\")\n",
    "\n",
    "    ax.axvline(evaluation_period_end, color=\"black\", linestyle=\":\", label=\"Evaluation Period Ends\")\n",
    "    ax.text(evaluation_period_end, y_positions[0], f\"Evaluation Period Ends\\n{evaluation_period_end.strftime('%Y-%m-%d')}\",\n",
    "            rotation=0, va=\"center\", ha=\"right\", color=\"black\")\n",
    "\n",
    "    # Plot trade points\n",
    "    buy_trades = trades[trades[\"type\"] == \"buy\"]\n",
    "    sell_trades = trades[trades[\"type\"] == \"sell\"]\n",
    "    ax.scatter(buy_trades[\"date\"], buy_trades[\"price\"], color=\"green\", label=\"Buy\", zorder=5)\n",
    "    ax.scatter(sell_trades[\"date\"], sell_trades[\"price\"], color=\"red\", label=\"Sell\", zorder=5)\n",
    "\n",
    "    # Ensure 'type' column exists to avoid KeyError when counting trades\n",
    "    for df_check, label in [(trades_analysis, \"trades_analysis\"), (trades_eval, \"trades_eval\")]:\n",
    "        if \"type\" not in df_check.columns:\n",
    "            print(f\"âš ï¸ Missing 'type' column in {label} â€” likely no trades occurred.\")\n",
    "            df_check[\"type\"] = None  # Add placeholder column to prevent KeyError\n",
    "\n",
    "    try:\n",
    "        caption_lines = [\n",
    "            f\"Full Range: {analysis_period_start.date()} to {evaluation_period_end.date()}\",\n",
    "            f\"Buy Level: {buy_level:.2f}\",\n",
    "            f\"Sell Level: {sell_level:.2f}\",\n",
    "            f\"Analysis Trades: {len(trades_analysis[trades_analysis['type'] == 'sell'])}\",\n",
    "            f\"Analysis Return: {result_row['analysis_return']:.2%}\",\n",
    "            f\"Evaluation Trades: {len(trades_eval[trades_eval['type'] == 'sell'])}\",\n",
    "            f\"Evaluation Return: {result_row['evaluation_return']:.2%}\",\n",
    "        ]\n",
    "\n",
    "        if data_source == \"schwab\":\n",
    "            caption_lines += [\n",
    "                f\"---------------------------------------\",\n",
    "                f\"Current Price Below Buy: {result_row['current_price_below_lb']}\",\n",
    "                f\"Percent Below Buy: {result_row['percent_below_lb']:.2%}\",\n",
    "                f\"Current Price Below Sell: {result_row['current_price_below_ub']}\",\n",
    "                f\"Percent Below Sell: {result_row['percent_below_ub']:.2%}\",\n",
    "                f\"Price Between Bounds: {result_row['current_price_between_bounds']}\",\n",
    "                f\"Num Days Fully Below Buy: {int(result_row['num_days_below_lb'])}\",\n",
    "                f\"Cyclicality: {result_row['cyclicality']:.4f}\",\n",
    "                f\"Plot Calculated Slope: {analysis_trend_slope:.4f}\",\n",
    "                f\"Analysis Trend Slope: {result_row['trend_slope']:.4f}\",\n",
    "                f\"Normalized Analysis Trend Slope: {result_row['norm_trend_slope']:.4f}\",\n",
    "                f\"Profit Score: {result_row['profit_score']:.4f}\",\n",
    "                f\"Profit Rank: {int(result_row['profit_rank'])}/{num_tickers_source}\",\n",
    "                f\"Swing Probability: {result_row['swing_probability']}\",\n",
    "            ]\n",
    "\n",
    "        caption = \"\\n\".join(caption_lines)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error generating caption for {ticker}: {e}\")\n",
    "        caption = f\"Plot generated for {ticker}, but caption failed to render.\"\n",
    "\n",
    "    ax.set_title(f\"Swing Trades for {ticker}\", fontsize=14)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Price\")\n",
    "    ax.legend()\n",
    "    \n",
    "    ax.text(0.01, 0.01, caption, transform=ax.transAxes, fontsize=10,\n",
    "            va='bottom', ha='left', bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "    # Format date axis\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m\"))\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Auto-save the figure\n",
    "    output_dir = Path(\"swing_trade_charts\") / f\"swing_analysis_period_ending_{str(analysis_period_end.date())}\" / f\"{str(swing_probability_filter)}_{str(analysis_period_end.date())}\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    filename = f\"0{int(result_row['profit_rank'])}_{ticker}_{analysis_period_start.date()}_{analysis_period_end.date()}.png\"\n",
    "    plt.savefig(output_dir / filename, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"Saved plot for {ticker} to {output_dir / filename}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_random_ticker_trades(final_results_dict, num_tickers_source, local_folder, data_source,\n",
    "                              num_tickers=2, swing_probability_filter=None, select_top=False):\n",
    "    \"\"\"\n",
    "    Selects tickers from final_results_dict and plots result rows using plot_swing_trades,\n",
    "    either randomly or based on best profit_rank.\n",
    "\n",
    "    Args:\n",
    "        final_results_dict (dict): Dictionary of {ticker: DataFrame} with analysis/evaluation results.\n",
    "        local_folder (str): Folder containing the associated local candle data.\n",
    "        data_source (str): Source of the candle data (e.g., 'schwab').\n",
    "        num_tickers (int): Number of tickers to plot.\n",
    "        swing_probability_filter (str or None): Filter tickers by swing_probability column.\n",
    "        select_top (bool): If True, selects top tickers by best profit_rank instead of random.\n",
    "\n",
    "    Returns:\n",
    "        None. Plots are displayed.\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "\n",
    "    for ticker, df in final_results_dict.items():\n",
    "        if swing_probability_filter is not None:\n",
    "            df = df[df[\"swing_probability\"] == swing_probability_filter]\n",
    "\n",
    "        if not df.empty:\n",
    "            filtered.append((ticker, df))\n",
    "\n",
    "    if not filtered:\n",
    "        print(f\"No tickers found with swing_probability == '{swing_probability_filter}'.\")\n",
    "        return\n",
    "\n",
    "    if select_top:\n",
    "        # Flatten, then sort by best (lowest) profit_rank per ticker\n",
    "        filtered = sorted(\n",
    "            filtered,\n",
    "            key=lambda item: item[1][\"profit_rank\"].min()\n",
    "        )\n",
    "    else:\n",
    "        filtered = random.sample(filtered, k=min(num_tickers, len(filtered)))\n",
    "\n",
    "    selected = filtered[:min(num_tickers, len(filtered))]\n",
    "\n",
    "    for ticker, df in selected:\n",
    "        for i, row in df.iterrows():\n",
    "            print(f\"Plotting row {i} for {ticker}...\")\n",
    "            try:\n",
    "                plot_swing_trades(row, num_tickers_source, local=True, local_folder=local_folder,\n",
    "                  data_source=data_source, swing_probability_filter=swing_probability_filter)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to plot row {i} for {ticker}: {e}\")\n",
    "        print(f\"COMPLETED PLOTTING FOR {ticker} -------------------------------------------------------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot top 25 tickers from `very_high` category of `swing_probability`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_ticker_trades(\n",
    "    final_results_dict_schwab,\n",
    "    num_tickers_schwab,\n",
    "    local_folder=\"local-ticker-data-schwab\",\n",
    "    data_source='schwab',\n",
    "    num_tickers=25,\n",
    "    swing_probability_filter='very_high',  # 'zero', 'very_low', 'low', 'medium', 'high', 'very_high'\n",
    "    select_top=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot top 10 tickers from `high` category of `swing_probability`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_ticker_trades(\n",
    "    final_results_dict_schwab,\n",
    "    num_tickers_schwab,\n",
    "    local_folder=\"local-ticker-data-schwab\",\n",
    "    data_source='schwab',\n",
    "    num_tickers=10,\n",
    "    swing_probability_filter='high',  # 'zero', 'very_low', 'low', 'medium', 'high', 'very_high'\n",
    "    select_top=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot random tickers' analyses from GBQ base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_random_ticker_trades(final_results_dict_gbq, num_tickers_gbq, local_folder=\"local-ticker-data-gbq\", data_source='gbq', num_tickers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert PNG chart files into one sheet of an Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_excel_with_charts(final_results_dict_schwab, image_root=\"swing_trade_charts\"):\n",
    "    \"\"\"\n",
    "    Creates an Excel workbook with one sheet per subfolder in `image_root`,\n",
    "    embedding all PNG images found in each subfolder. The workbook is saved\n",
    "    in the root folder with the name format:\n",
    "    'swing_recommendations_YYYY-MM-DD.xlsx', where the date corresponds to\n",
    "    the analysis_period_end of the ticker with profit_rank == 1.\n",
    "\n",
    "    Additional behavior:\n",
    "    - Folders are processed in reverse alphabetical order.\n",
    "    - Automatically closes any open Excel processes (Windows only).\n",
    "    - Opens the resulting Excel file after saving.\n",
    "\n",
    "    Args:\n",
    "        final_results_dict_schwab (dict): Dictionary containing result DataFrames for each ticker.\n",
    "        image_root (str): Root directory containing subfolders named after swing probabilities.\n",
    "\n",
    "    Returns:\n",
    "        None. Saves and opens an Excel file after embedding all chart images.\n",
    "    \"\"\"\n",
    "    # Determine correct analysis_period_end from ticker with profit_rank == 1\n",
    "    best_date = None\n",
    "    for df in final_results_dict_schwab.values():\n",
    "        if not df.empty and (df[\"profit_rank\"] == 1).any():\n",
    "            best_date = pd.to_datetime(df[df[\"profit_rank\"] == 1][\"analysis_period_end\"].iloc[0]).strftime(\"%Y-%m-%d\")\n",
    "            break\n",
    "\n",
    "    if best_date is None:\n",
    "        best_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    image_root = Path(image_root) / f\"swing_analysis_period_ending_{best_date}\"\n",
    "    if not image_root.exists():\n",
    "        print(f\"Image root folder '{image_root}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    output_file = f\"swing_recommendations_{best_date}.xlsx\"\n",
    "    output_path = image_root / output_file\n",
    "\n",
    "    # Try to close Excel if open (Windows only)\n",
    "    try:\n",
    "        subprocess.run([\"taskkill\", \"/f\", \"/im\", \"excel.exe\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        print(\"ðŸ›‘ Closed existing Excel instances.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not close Excel automatically: {e}\")\n",
    "\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)  # Remove default sheet\n",
    "\n",
    "    # Process folders in reverse alphabetical order\n",
    "    for folder in sorted(image_root.iterdir(), reverse=True):\n",
    "        if folder.is_dir():\n",
    "            sheet_name = folder.name[:31]  # Excel sheet name limit\n",
    "            ws = wb.create_sheet(title=sheet_name)\n",
    "\n",
    "            images = [\n",
    "                folder / fname for fname in os.listdir(folder)\n",
    "                if fname.lower().endswith(\".png\")]\n",
    "            print(images)\n",
    "\n",
    "            try:\n",
    "                images.sort(key=lambda p: int(p.name.split(\"_\")[0]))\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Could not sort images in {folder}: {e}\")\n",
    "\n",
    "            if not images:\n",
    "                print(f\"No images found in {folder}\")\n",
    "                continue\n",
    "\n",
    "            row = 1\n",
    "            for img_path in images:\n",
    "                try:\n",
    "                    Image.open(img_path).verify()\n",
    "                    img = XLImage(str(img_path))\n",
    "                    img.width = 1800\n",
    "                    img.height = 900\n",
    "                    ws.add_image(img, f\"A{row}\")\n",
    "                    row += 50\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Failed to add image {img_path}: {e}\")\n",
    "\n",
    "    wb.save(output_path)\n",
    "    print(f\"âœ… Excel workbook saved to: {output_path}\")\n",
    "\n",
    "    try:\n",
    "        os.startfile(output_path)\n",
    "        print(f\"ðŸ“‚ Opened Excel workbook.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not open Excel file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_excel_with_charts(final_results_dict_schwab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Up next... Figure out what this is and integrate it into analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two functions serve related but distinct purposes within a swing trading framework, and they operate at different levels of abstraction:\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  `run_analysis_loop(...)`:\n",
    "\n",
    "**Purpose**: Automates **walk-forward optimization and evaluation** over multiple periods **for a single ticker**.\n",
    "\n",
    "**Key Actions**:\n",
    "\n",
    "* Iterates over predefined time windows (`analysis` and `evaluation`) in `ticker_results`.\n",
    "* For each window:\n",
    "\n",
    "  * Runs optimization on `analysis_data` using `analyze_ticker_data(...)` (not the second function you showed, but likely a simpler equivalent).\n",
    "  * Stores the resulting thresholds (`lb`, `ub`), return metrics, and trade counts.\n",
    "  * Evaluates performance in the `evaluation` window using those thresholds.\n",
    "\n",
    "**Use Case**: This is part of a **systematic backtesting framework** that runs many optimizations for multiple sliding periods.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” `analyze_ticker(...)`:\n",
    "\n",
    "**Purpose**: Performs **a single historical optimization** on a tickerâ€™s full dataset over one period.\n",
    "\n",
    "**Key Actions**:\n",
    "\n",
    "* Pulls historical data for the ticker using a fixed lookback (e.g., `month_offset=9`).\n",
    "* Constructs a grid of `lb` (buy threshold) and `ub` (sell threshold) pairs.\n",
    "* Evaluates return and trade count for each pair.\n",
    "* Selects the best-performing threshold set, optionally shows plots and trade logs.\n",
    "\n",
    "**Use Case**: This is a **standalone optimization tool** for a single ticker over a single period, useful for one-off experimentation.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”„ Summary of Differences:\n",
    "\n",
    "| Feature       | `run_analysis_loop(...)`                          | `analyze_ticker(...)`                 |\n",
    "| ------------- | ------------------------------------------------- | ------------------------------------- |\n",
    "| Scope         | Multiple walk-forward windows                     | Single historical window              |\n",
    "| Input         | Pre-split `ticker_results` + data                 | Ticker symbol only                    |\n",
    "| Uses          | Vectorized or external `analyze_ticker_data(...)` | Internal grid search loop             |\n",
    "| Output        | Enriched DataFrame with walk-forward metrics      | Top threshold combos from grid search |\n",
    "| Visualization | None                                              | Optional matplotlib plot              |\n",
    "| Flexibility   | Meant for automation                              | Meant for exploration                 |\n",
    "\n",
    "They can complement each otherâ€”`run_analysis_loop` could internally call a simplified or modular version of `analyze_ticker` if unified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyze_ticker(\n",
    "#     ticker,\n",
    "#     month_offset=9,\n",
    "#     date_pull_begin=None,\n",
    "#     length_out=15,\n",
    "#     verbose_output=True,\n",
    "#     save_experiment=False,\n",
    "#     date_slug=None\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Run a swing trading parameter optimization on a single ticker.\n",
    "\n",
    "#     This function searches for the optimal lower-bound (buy threshold) and upper-bound\n",
    "#     (sell threshold) levels that maximize annualized return over a historical window.\n",
    "#     It builds a grid of lb/ub combinations, evaluates each using simulated trades,\n",
    "#     and returns the best result(s). Optionally, results can be plotted and saved.\n",
    "\n",
    "#     Args:\n",
    "#         ticker (str): The ticker symbol to analyze.\n",
    "#         month_offset (int): Number of months before the most recent date to define the analysis window.\n",
    "#         date_pull_begin (str or datetime, optional): Override to define the start date for data pulling.\n",
    "#         length_out (int): Number of grid points (per axis) to test in the lb/ub threshold search.\n",
    "#         verbose_output (bool): Whether to print trade tables and show a plot of best results.\n",
    "#         save_experiment (bool): If True, saves the full grid of results to disk as a tab-delimited file.\n",
    "#         date_slug (str, optional): Prefix for the experiment file name if saving.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: A single-row or small subset DataFrame containing the best performing\n",
    "#                       threshold combination(s), with fields:\n",
    "#             - lb (float): lower bound (buy threshold)\n",
    "#             - ub (float): upper bound (sell threshold)\n",
    "#             - return (float): annualized return\n",
    "#             - trades (int): number of trades triggered\n",
    "#             - time_start (datetime): start date of the analysis window\n",
    "\n",
    "#     Raises:\n",
    "#         ValueError: If neither `month_offset` nor `date_pull_begin` is supplied.\n",
    "#     \"\"\"\n",
    "#     if not month_offset and not date_pull_begin:\n",
    "#         raise ValueError(\"You must provide either month_offset or date_pull_begin\")\n",
    "\n",
    "#     if date_pull_begin is None:\n",
    "#         date_pull_begin = pd.Timestamp.today() - pd.DateOffset(months=int(1.25 * month_offset))\n",
    "#     else:\n",
    "#         date_pull_begin = pd.to_datetime(date_pull_begin)\n",
    "\n",
    "#     # Pull data\n",
    "#     data = get_ticker_data(\n",
    "#         ticker,\n",
    "#         overall_start_gbq,\n",
    "#         local=use_local_data_gbq,\n",
    "#         credentials_path=private_key,\n",
    "#         project_id=gbq_proj_id\n",
    "#     )\n",
    "\n",
    "#     lb_start = data[\"low\"].quantile(0.01)\n",
    "#     lb_end = data[\"low\"].quantile(0.75)\n",
    "#     ub_start = data[\"high\"].quantile(0.10)\n",
    "#     ub_end = data[\"high\"].quantile(0.99)\n",
    "\n",
    "#     last_date = data[\"date_time\"].max()\n",
    "#     time_start = last_date - pd.DateOffset(months=month_offset)\n",
    "\n",
    "#     # Build grid\n",
    "#     lb_vals = np.linspace(lb_start, lb_end, length_out)\n",
    "#     ub_vals = np.linspace(ub_start, ub_end, length_out)\n",
    "#     grid = []\n",
    "\n",
    "#     for lb in lb_vals:\n",
    "#         for ub in ub_vals:\n",
    "#             if lb >= ub:\n",
    "#                 continue\n",
    "#             result = get_returns(data, upper_bound=ub, lower_bound=lb, time_start=time_start)\n",
    "#             grid.append({\n",
    "#                 \"lb\": lb,\n",
    "#                 \"ub\": ub,\n",
    "#                 \"time_start\": time_start,\n",
    "#                 \"return\": result[\"annualized_return\"],\n",
    "#                 \"trades\": result[\"num_trades\"]\n",
    "#             })\n",
    "\n",
    "#     grid_df = pd.DataFrame(grid).dropna(subset=[\"return\"])\n",
    "\n",
    "#     if save_experiment:\n",
    "#         if not date_slug:\n",
    "#             date_slug = pd.Timestamp.today().strftime(\"%Y%m%d\")\n",
    "#         output_file = Path(\"swing-trading/experiments\") / f\"{date_slug}_{ticker}_swing_experiments.txt\"\n",
    "#         grid_df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "\n",
    "#     if grid_df.empty:\n",
    "#         print(\"No valid trade setups found.\")\n",
    "#         return grid_df\n",
    "\n",
    "#     max_trades = grid_df[\"trades\"].max()\n",
    "#     returns_80pct = grid_df[\"return\"].quantile(0.80)\n",
    "\n",
    "#     if max_trades <= 6:\n",
    "#         top = grid_df.sort_values([\"trades\", \"return\"], ascending=[True, True]).tail(5 if verbose_output else 1)\n",
    "#     else:\n",
    "#         top = grid_df[(grid_df[\"return\"] > returns_80pct)].sort_values(\"trades\").tail(5 if verbose_output else 1)\n",
    "\n",
    "#     if verbose_output and not top.empty:\n",
    "#         best = top.iloc[-1]\n",
    "#         best_trades = get_trades(data, best[\"ub\"], best[\"lb\"], best[\"time_start\"])\n",
    "#         print(best_trades[[\"date\", \"type\", \"trade_price\"]].to_string(index=False))\n",
    "#         print(\"---------------------------\")\n",
    "\n",
    "#         # Plot\n",
    "#         plot_data = data[data[\"date_time\"] >= best[\"time_start\"]]\n",
    "#         fig, ax = plt.subplots(figsize=(14, 6))\n",
    "#         ax.errorbar(\n",
    "#             plot_data[\"date_time\"],\n",
    "#             y=(plot_data[\"high\"] + plot_data[\"low\"]) / 2,\n",
    "#             yerr=(plot_data[\"high\"] - plot_data[\"low\"]) / 2,\n",
    "#             fmt='-', ecolor='gray', alpha=0.5\n",
    "#         )\n",
    "#         ax.axhline(best[\"lb\"], color=\"lightblue\", linestyle=\"--\")\n",
    "#         ax.axhline(best[\"ub\"], color=\"lightblue\", linestyle=\"--\")\n",
    "#         ax.set_title(f\"Swings for {ticker}\")\n",
    "#         ax.set_ylabel(\"Price\")\n",
    "#         ax.set_xlabel(\"\")\n",
    "#         ax.text(0.01, 0.01, f\"Maximum return: {best['return']:.1%}\", transform=ax.transAxes,\n",
    "#                 bbox=dict(facecolor='white', alpha=0.5))\n",
    "#         plt.xticks(rotation=45)\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "#     return top.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = analyze_ticker(\n",
    "#     ticker=\"AAPL\",                                # The stock symbol you want to analyze\n",
    "#     month_offset=9,                               # Go back this many months from the most recent candle to define analysis window\n",
    "#     date_pull_begin=None,                         # Optional override for start date; leave as None to use month_offset\n",
    "#     length_out=15,                                # Number of grid steps for lb/ub threshold testing\n",
    "#     verbose_output=True,                          # Whether to print trade details and plot the result\n",
    "#     save_experiment=False,                        # Whether to save full grid to file\n",
    "#     date_slug=None                                # Optional label for the saved file; None defaults to today's date\n",
    "# )\n",
    "\n",
    "# display(result)                                   # Display the resulting DataFrame with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
